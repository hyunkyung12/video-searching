1
00:00:09,784 --> 00:00:11,867
- Okay let's get started.

2
00:00:13,038 --> 00:00:15,888
Alright, so welcome to lecture 14,

3
00:00:15,888 --> 00:00:20,884
and today we'll be talking
about reinforcement learning.

4
00:00:20,884 --> 00:00:23,222
So some administrative details first,

5
00:00:23,222 --> 00:00:24,436
update on grades.

6
00:00:24,436 --> 00:00:26,594
Midterm grades were released last night,

7
00:00:26,594 --> 00:00:28,332
so see Piazza for more information

8
00:00:28,332 --> 00:00:30,346
and statistics about that.

9
00:00:30,346 --> 00:00:32,902
And we also have A2 and milestone grades

10
00:00:32,902 --> 00:00:35,402
scheduled for later this week.

11
00:00:36,768 --> 00:00:38,943
Also, about your projects, all teams

12
00:00:38,943 --> 00:00:40,682
must register your projects.

13
00:00:40,682 --> 00:00:42,630
So on Piazza we have a form posted,

14
00:00:42,630 --> 00:00:45,546
so you should go there and
this is required, every team

15
00:00:45,546 --> 00:00:47,580
should go and fill out
this form with information

16
00:00:47,580 --> 00:00:50,782
about your project, that
we'll use for final grading

17
00:00:50,782 --> 00:00:53,214
and the poster session.

18
00:00:53,214 --> 00:00:56,168
And the Tiny ImageNet
evaluation servers are also now

19
00:00:56,168 --> 00:00:58,366
online for those of you who are

20
00:00:58,366 --> 00:01:01,779
doing the Tiny ImageNet challenge.

21
00:01:01,779 --> 00:01:04,705
We also have a link to a
course survey on Piazza

22
00:01:04,705 --> 00:01:06,193
that was released a few days ago,

23
00:01:06,193 --> 00:01:09,488
so, please fill it out if
you guys haven't already.

24
00:01:09,488 --> 00:01:11,433
We'd love to have your
feedback and know how

25
00:01:11,433 --> 00:01:13,600
we can improve this class.

26
00:01:16,589 --> 00:01:19,650
Okay, so the topic of today,
reinforcement learning.

27
00:01:19,650 --> 00:01:22,544
Alright, so so far we've talked
about supervised learning,

28
00:01:22,544 --> 00:01:25,472
which is about a type of
problem where we have data x

29
00:01:25,472 --> 00:01:28,009
and then we have labels y
and our goal is to learn

30
00:01:28,009 --> 00:01:30,498
a function that is mapping from x to y.

31
00:01:30,498 --> 00:01:32,757
So, for example, the
classification problem

32
00:01:32,757 --> 00:01:35,067
that we've been working with.

33
00:01:35,067 --> 00:01:37,753
We also talked last lecture
about unsupervised learning,

34
00:01:37,753 --> 00:01:40,982
which is the problem
where we have just data

35
00:01:40,982 --> 00:01:42,664
and no labels, and our goal is to learn

36
00:01:42,664 --> 00:01:45,362
some underlying, hidden
structure of the data.

37
00:01:45,362 --> 00:01:47,695
So, an example of this
is the generative models

38
00:01:47,695 --> 00:01:50,528
that we talked about last lecture.

39
00:01:52,040 --> 00:01:53,892
And so today we're going
to talk about a different

40
00:01:53,892 --> 00:01:57,370
kind of problem set-up, the
reinforcement learning problem.

41
00:01:57,370 --> 00:01:59,592
And so here we have an agent

42
00:01:59,592 --> 00:02:01,824
that can take actions in its environment,

43
00:02:01,824 --> 00:02:04,352
and it can receive rewards
for for its action.

44
00:02:04,352 --> 00:02:07,268
And its goal is going to be
to learn how to take actions

45
00:02:07,268 --> 00:02:09,959
in a way that can maximize its reward.

46
00:02:09,959 --> 00:02:14,101
And so we'll talk about this
in a lot more detail today.

47
00:02:14,101 --> 00:02:16,020
So, the outline for today,
we're going to first

48
00:02:16,020 --> 00:02:18,116
talk about the reinforcement
learning problem,

49
00:02:18,116 --> 00:02:20,927
and then we'll talk about
Markov decision processes,

50
00:02:20,927 --> 00:02:24,747
which is a formalism of the
reinforcement learning problem,

51
00:02:24,747 --> 00:02:26,928
and then we'll talk
about two major classes

52
00:02:26,928 --> 00:02:31,095
of RL algorithms, Q-learning
and policy gradients.

53
00:02:32,876 --> 00:02:35,173
So, in the reinforcement
learning set up, what we have

54
00:02:35,173 --> 00:02:38,936
is we have an agent and
we have an environment.

55
00:02:38,936 --> 00:02:43,268
And so the environment
gives the agent a state.

56
00:02:43,268 --> 00:02:46,877
In turn, the agent is
going to take an action,

57
00:02:46,877 --> 00:02:50,888
and then the environment is
going to give back a reward,

58
00:02:50,888 --> 00:02:52,609
as well as the next state.

59
00:02:52,609 --> 00:02:54,737
And so this is going to
keep going on in this loop,

60
00:02:54,737 --> 00:02:56,256
on and on, until the environment

61
00:02:56,256 --> 00:02:57,967
gives back a terminal state,

62
00:02:57,967 --> 00:03:00,918
which then ends the episode.

63
00:03:00,918 --> 00:03:03,401
So, let's see some examples of this.

64
00:03:03,401 --> 00:03:05,536
First we have here the cart-pole problem,

65
00:03:05,536 --> 00:03:08,113
which is a classic problem
that some of you may have seen,

66
00:03:08,113 --> 00:03:11,142
in, for example, 229 before.

67
00:03:11,142 --> 00:03:13,438
And so this objective
here is that you want to

68
00:03:13,438 --> 00:03:16,252
balance a pole on top of a movable cart.

69
00:03:16,252 --> 00:03:18,020
Alright, so the state
that you have here is

70
00:03:18,020 --> 00:03:20,280
your current description of the system.

71
00:03:20,280 --> 00:03:23,706
So, for example, angular, angular speed

72
00:03:23,706 --> 00:03:25,804
of your pole, your
position, and the horizontal

73
00:03:25,804 --> 00:03:28,206
velocity of your cart.

74
00:03:28,206 --> 00:03:31,712
And the actions you can
take are horizontal forces

75
00:03:31,712 --> 00:03:33,224
that you apply onto the cart, right?

76
00:03:33,224 --> 00:03:35,915
So you're basically trying
to move this cart around

77
00:03:35,915 --> 00:03:38,387
to try and balance this pole on top of it.

78
00:03:38,387 --> 00:03:40,147
And the reward that you're getting

79
00:03:40,147 --> 00:03:42,881
from this environment
is one at each time step

80
00:03:42,881 --> 00:03:43,990
if your pole is upright.

81
00:03:43,990 --> 00:03:45,476
So you basically want to keep this pole

82
00:03:45,476 --> 00:03:48,143
balanced for as long as you can.

83
00:03:49,286 --> 00:03:52,192
Okay, so here's another example
of a classic RL problem.

84
00:03:52,192 --> 00:03:53,998
Here is robot locomotion.

85
00:03:53,998 --> 00:03:57,106
So we have here an example
of a humanoid robot,

86
00:03:57,106 --> 00:03:59,670
as well as an ant robot model.

87
00:03:59,670 --> 00:04:03,128
And our objective here is to
make the robot move forward.

88
00:04:03,128 --> 00:04:05,285
And so the state that we have

89
00:04:05,285 --> 00:04:08,119
describing our system is
the angle and the positions

90
00:04:08,119 --> 00:04:10,807
of all the joints of our robots.

91
00:04:10,807 --> 00:04:13,467
And then the actions that we can take are

92
00:04:13,467 --> 00:04:15,887
the torques applied onto these joints,

93
00:04:15,887 --> 00:04:17,170
right, and so these are

94
00:04:17,170 --> 00:04:18,586
trying to make the robot

95
00:04:18,586 --> 00:04:21,228
move forward and then
the reward that we get is

96
00:04:21,228 --> 00:04:23,775
our forward movement as well
as, I think, in the time of,

97
00:04:23,775 --> 00:04:26,501
in the case of the humanoid,
also, you can have something

98
00:04:26,501 --> 00:04:30,284
like a reward of one for
each time step that this

99
00:04:30,284 --> 00:04:31,701
robot is upright.

100
00:04:33,521 --> 00:04:35,219
So, games are also

101
00:04:35,219 --> 00:04:38,384
a big class of problems that
can be formulated with RL.

102
00:04:38,384 --> 00:04:40,700
So, for example, here we have Atari games

103
00:04:40,700 --> 00:04:44,280
which are a classic success
of deep reinforcement learning

104
00:04:44,280 --> 00:04:47,056
and so here the objective
is to complete these games

105
00:04:47,056 --> 00:04:48,574
with the highest possible score, right.

106
00:04:48,574 --> 00:04:50,628
So, your agent is basically a player

107
00:04:50,628 --> 00:04:52,753
that's trying to play these games.

108
00:04:52,753 --> 00:04:55,196
And the state that you have is going to be

109
00:04:55,196 --> 00:04:57,506
the raw pixels of the game state.

110
00:04:57,506 --> 00:04:59,226
Right, so these are just the

111
00:04:59,226 --> 00:05:01,292
pixels on the screen that you would see

112
00:05:01,292 --> 00:05:02,882
as you're playing the game.

113
00:05:02,882 --> 00:05:04,354
And then the actions that you have

114
00:05:04,354 --> 00:05:06,558
are your game controls, so for example,

115
00:05:06,558 --> 00:05:09,912
in some games maybe moving
left to right, up or down.

116
00:05:09,912 --> 00:05:12,534
And then the score that you
have is your score increase

117
00:05:12,534 --> 00:05:15,667
or decrease at each time step,
and your goal is going to be

118
00:05:15,667 --> 00:05:19,834
to maximize your total score
over the course of the game.

119
00:05:21,312 --> 00:05:24,179
And, finally, here we have
another example of a game here.

120
00:05:24,179 --> 00:05:25,587
It's

121
00:05:25,587 --> 00:05:26,587
Go, which is

122
00:05:27,573 --> 00:05:28,893
something that was a

123
00:05:28,893 --> 00:05:31,697
huge achievement of deep
reinforcement learning last year,

124
00:05:31,697 --> 00:05:34,721
when Deep Minds AlphaGo beats Lee Sedol,

125
00:05:34,721 --> 00:05:36,867
which is one of the

126
00:05:36,867 --> 00:05:38,589
best Go players of the last few years,

127
00:05:38,589 --> 00:05:41,685
and this is actually in the news again

128
00:05:41,685 --> 00:05:45,667
for, as some of you may have
seen, there's another Go

129
00:05:45,667 --> 00:05:47,529
competition going on now with

130
00:05:47,529 --> 00:05:50,919
AlphaGo versus a top-ranked Go player.

131
00:05:50,919 --> 00:05:53,495
And so the objective here is to

132
00:05:53,495 --> 00:05:56,295
win the game, and our
state is the position

133
00:05:56,295 --> 00:05:58,349
of all the pieces, the action
is where to put the next

134
00:05:58,349 --> 00:06:02,062
piece down, and the reward
is, one, if you win at the end

135
00:06:02,062 --> 00:06:03,912
of the game, and zero otherwise.

136
00:06:03,912 --> 00:06:05,032
And we'll also talk about this one

137
00:06:05,032 --> 00:06:08,411
in a little bit more detail, later.

138
00:06:08,411 --> 00:06:09,891
Okay, so

139
00:06:09,891 --> 00:06:12,046
how can we mathematically formalize

140
00:06:12,046 --> 00:06:13,330
the RL problem, right?

141
00:06:13,330 --> 00:06:15,817
This loop that we talked about earlier,

142
00:06:15,817 --> 00:06:18,051
of environments giving agents states,

143
00:06:18,051 --> 00:06:20,634
and then agents taking actions.

144
00:06:22,394 --> 00:06:24,884
So, a Markov decision process is

145
00:06:24,884 --> 00:06:28,512
the mathematical formulation
of the RL problem,

146
00:06:28,512 --> 00:06:31,447
and an MDP satisfies the Markov property,

147
00:06:31,447 --> 00:06:33,054
which is that the current state completely

148
00:06:33,054 --> 00:06:36,107
characterizes the state of the world.

149
00:06:36,107 --> 00:06:40,164
And an MDP here is defined
by tuple of objects,

150
00:06:40,164 --> 00:06:43,170
consisting of S, which is
the set of possible states.

151
00:06:43,170 --> 00:06:45,762
We have A, our set of possible actions,

152
00:06:45,762 --> 00:06:50,018
we also have R, our
distribution of our reward,

153
00:06:50,018 --> 00:06:51,694
given a state, action pair,

154
00:06:51,694 --> 00:06:53,824
so it's a function
mapping from state action

155
00:06:53,824 --> 00:06:55,323
to your reward.

156
00:06:55,323 --> 00:06:57,430
You also have P, which is
a transition probability

157
00:06:57,430 --> 00:07:00,079
distribution over your
next state, that you're

158
00:07:00,079 --> 00:07:02,940
going to transition to given
your state, action pair.

159
00:07:02,940 --> 00:07:05,718
And then finally we have a
Gamma, a discount factor,

160
00:07:05,718 --> 00:07:09,720
which is basically
saying how much we value

161
00:07:09,720 --> 00:07:12,970
rewards coming up soon versus later on.

162
00:07:14,203 --> 00:07:17,395
So, the way the Markov
Decision Process works is that

163
00:07:17,395 --> 00:07:20,053
at our initial time step t equals zero,

164
00:07:20,053 --> 00:07:21,523
the environment is going to sample some

165
00:07:21,523 --> 00:07:24,615
initial state as zero, from
the initial state distribution,

166
00:07:24,615 --> 00:07:26,363
p of s zero.

167
00:07:26,363 --> 00:07:29,271
And then, once it has that,
then from time t equals zero

168
00:07:29,271 --> 00:07:32,253
until it's done, we're going
to iterate through this loop

169
00:07:32,253 --> 00:07:35,797
where the agent is going to
select an action, a sub t.

170
00:07:35,797 --> 00:07:38,885
The environment is going to
sample a reward from here,

171
00:07:38,885 --> 00:07:41,907
so reward given your state and the

172
00:07:41,907 --> 00:07:44,032
action that you just took.

173
00:07:44,032 --> 00:07:47,640
It's also going to sample the next state,

174
00:07:47,640 --> 00:07:51,534
at time t plus one, given
your probability distribution

175
00:07:51,534 --> 00:07:54,467
and then the agent is going to receive

176
00:07:54,467 --> 00:07:56,790
the reward, as well as the
next state, and then we're

177
00:07:56,790 --> 00:07:58,707
going to through this process again,

178
00:07:58,707 --> 00:08:01,769
and keep looping; agent
will select the next action,

179
00:08:01,769 --> 00:08:05,542
and so on until the episode is over.

180
00:08:05,542 --> 00:08:06,989
Okay, so

181
00:08:06,989 --> 00:08:10,724
now based on this, we
can define a policy pi,

182
00:08:10,724 --> 00:08:13,593
which is a function from
your states to your actions

183
00:08:13,593 --> 00:08:16,651
that specifies what action
to take in each state.

184
00:08:16,651 --> 00:08:19,748
And this can be either
deterministic or stochastic.

185
00:08:19,748 --> 00:08:22,447
And our objective now is
to going to be to find

186
00:08:22,447 --> 00:08:24,727
your optimal policy pi
star, that maximizes your

187
00:08:24,727 --> 00:08:27,205
cumulative discounted reward.

188
00:08:27,205 --> 00:08:29,059
So we can see here we have our

189
00:08:29,059 --> 00:08:31,813
some of our future
rewards, which can be also

190
00:08:31,813 --> 00:08:35,509
discounted by your discount factor.

191
00:08:35,509 --> 00:08:39,327
So, let's look at an
example of a simple MDP.

192
00:08:39,327 --> 00:08:42,034
And here we have Grid World, which is this

193
00:08:42,034 --> 00:08:44,533
task where we have this grid of states.

194
00:08:44,533 --> 00:08:46,950
So you can be in any of these

195
00:08:48,112 --> 00:08:50,295
cells of your grid, which are your states.

196
00:08:50,295 --> 00:08:52,613
And you can take actions from your states,

197
00:08:52,613 --> 00:08:54,713
and so these actions are going to be

198
00:08:54,713 --> 00:08:56,527
simple movements, moving to your right,

199
00:08:56,527 --> 00:08:59,299
to your left, up or down.

200
00:08:59,299 --> 00:09:02,683
And you're going to get a
negative reward for each

201
00:09:02,683 --> 00:09:07,163
transition or each time step,
basically, that happens.

202
00:09:07,163 --> 00:09:08,859
Each movement that you take,

203
00:09:08,859 --> 00:09:11,989
and this can be something
like R equals negative one.

204
00:09:11,989 --> 00:09:13,871
And so your objective is going to be

205
00:09:13,871 --> 00:09:15,588
to reach one of the terminal states,

206
00:09:15,588 --> 00:09:17,793
which are the gray states shown here,

207
00:09:17,793 --> 00:09:20,055
in the least number of actions.

208
00:09:20,055 --> 00:09:22,249
Right, so the longer
that you take to reach

209
00:09:22,249 --> 00:09:23,522
your terminal state, you're going to keep

210
00:09:23,522 --> 00:09:26,522
accumulating these negative rewards.

211
00:09:27,625 --> 00:09:30,540
Okay, so if you look at
a random policy here,

212
00:09:30,540 --> 00:09:33,141
a random policy would
consist of, basically,

213
00:09:33,141 --> 00:09:35,305
at any given state or cell that you're in

214
00:09:35,305 --> 00:09:37,770
just sampling randomly which direction

215
00:09:37,770 --> 00:09:39,090
that you're going to move in next.

216
00:09:39,090 --> 00:09:41,843
Right, so all of these
have equal probability.

217
00:09:41,843 --> 00:09:44,115
On the other hand, an optimal policy that

218
00:09:44,115 --> 00:09:46,518
we would like to have is

219
00:09:46,518 --> 00:09:48,672
basically taking the action, the direction

220
00:09:48,672 --> 00:09:51,866
that will move us closest
to a terminal state.

221
00:09:51,866 --> 00:09:53,164
So you can see here,

222
00:09:53,164 --> 00:09:54,808
if we're right next to one of the

223
00:09:54,808 --> 00:09:56,156
terminal states we should

224
00:09:56,156 --> 00:09:57,506
always move in the direction

225
00:09:57,506 --> 00:09:59,171
that gets us to this terminal state.

226
00:09:59,171 --> 00:10:01,385
And otherwise, if you're in
one of these other states,

227
00:10:01,385 --> 00:10:03,822
you want to take the
direction that will take you

228
00:10:03,822 --> 00:10:06,405
closest to one of these states.

229
00:10:09,119 --> 00:10:11,644
Okay, so now given this

230
00:10:11,644 --> 00:10:13,745
description of our MDP, what we want to do

231
00:10:13,745 --> 00:10:17,155
is we want to find our
optimal policy pi star.

232
00:10:17,155 --> 00:10:20,755
Right, our policy that's
maximizing the sum of the rewards.

233
00:10:20,755 --> 00:10:22,955
And so this optimal policy
is going to tell us,

234
00:10:22,955 --> 00:10:25,655
given any state that we're
in, what is the action that

235
00:10:25,655 --> 00:10:27,851
we should take in order
to maximize the sum

236
00:10:27,851 --> 00:10:29,731
of the rewards that we'll get.

237
00:10:29,731 --> 00:10:32,011
And so one question is how do we

238
00:10:32,011 --> 00:10:34,091
handle the randomness in the MDP, right?

239
00:10:34,091 --> 00:10:36,459
We have randomness in

240
00:10:36,459 --> 00:10:39,073
terms of our initial
state that we're sampling,

241
00:10:39,073 --> 00:10:40,727
in therms of this transition probability

242
00:10:40,727 --> 00:10:42,303
distribution that will give us

243
00:10:42,303 --> 00:10:46,341
distribution of our
next states, and so on.

244
00:10:46,341 --> 00:10:49,292
Also what we'll do is we'll
work, then, with maximizing

245
00:10:49,292 --> 00:10:51,947
our expected sum of the rewards.

246
00:10:51,947 --> 00:10:55,451
So, formally, we can write
our optimal policy pi star

247
00:10:55,451 --> 00:10:59,129
as maximizing this expected
sum of future rewards

248
00:10:59,129 --> 00:11:02,957
over policy's pi, where
we have our initial state

249
00:11:02,957 --> 00:11:05,103
sampled from our state distribution.

250
00:11:05,103 --> 00:11:07,388
We have our actions,

251
00:11:07,388 --> 00:11:09,127
sampled from our policy, given the state.

252
00:11:09,127 --> 00:11:11,929
And then we have our next states sampled

253
00:11:11,929 --> 00:11:16,423
from our transition
probability distributions.

254
00:11:16,423 --> 00:11:17,256
Okay, so

255
00:11:18,351 --> 00:11:19,668
before we talk about

256
00:11:19,668 --> 00:11:22,143
exactly how we're going
to find this policy,

257
00:11:22,143 --> 00:11:23,909
let's first talk about a few definitions

258
00:11:23,909 --> 00:11:26,787
that's going to be helpful
for us in doing so.

259
00:11:26,787 --> 00:11:29,115
So, specifically, the value function

260
00:11:29,115 --> 00:11:31,405
and the Q-value function.

261
00:11:31,405 --> 00:11:33,647
So, as we follow the policy,

262
00:11:33,647 --> 00:11:35,489
we're going to sample trajectories

263
00:11:35,489 --> 00:11:37,426
or paths, right, for every episode.

264
00:11:37,426 --> 00:11:40,287
And we're going to have
our initial state as zero,

265
00:11:40,287 --> 00:11:43,611
a-zero, r-zero, s-one,
a-one, r-one, and so on.

266
00:11:43,611 --> 00:11:44,905
We're going to have this trajectory

267
00:11:44,905 --> 00:11:49,331
of states, actions, and
rewards that we get.

268
00:11:49,331 --> 00:11:52,613
And so, how good is a state
that we're currently in?

269
00:11:52,613 --> 00:11:55,985
Well, the value function at any state s,

270
00:11:55,985 --> 00:11:58,513
is the expected cumulative reward

271
00:11:58,513 --> 00:12:01,770
following the policy from
state s, from here on out.

272
00:12:01,770 --> 00:12:05,258
Right, so it's going to be expected value

273
00:12:05,258 --> 00:12:07,635
of our expected cumulative reward,

274
00:12:07,635 --> 00:12:10,800
starting from our current state.

275
00:12:10,800 --> 00:12:13,286
And then how good is a state, action pair?

276
00:12:13,286 --> 00:12:17,370
So how good is taking action a in state s?

277
00:12:17,370 --> 00:12:20,468
And we define this using
a Q-value function,

278
00:12:20,468 --> 00:12:23,574
which is, the expected
cumulative reward from taking

279
00:12:23,574 --> 00:12:27,741
action a in state s and
then following the policy.

280
00:12:29,708 --> 00:12:32,708
Right, so then, the
optimal Q-value function

281
00:12:32,708 --> 00:12:36,404
that we can get is going to be
Q star, which is the maximum

282
00:12:36,404 --> 00:12:39,216
expected cumulative reward that we can get

283
00:12:39,216 --> 00:12:43,383
from a given state action
pair, defined here.

284
00:12:45,099 --> 00:12:48,592
So now we're going to
see one important thing

285
00:12:48,592 --> 00:12:50,018
in reinforcement learning,

286
00:12:50,018 --> 00:12:52,018
which is called the Bellman equation.

287
00:12:52,018 --> 00:12:54,485
So let's consider this a Q-value function

288
00:12:54,485 --> 00:12:57,697
from the optimal policy Q star,

289
00:12:57,697 --> 00:13:00,911
which is then going to
satisfy this Bellman equation,

290
00:13:00,911 --> 00:13:03,533
which is this identity shown here,

291
00:13:03,533 --> 00:13:05,194
and what this means is that

292
00:13:05,194 --> 00:13:08,873
given any state, action pair, s and a,

293
00:13:08,873 --> 00:13:11,748
the value of this pair
is going to be the reward

294
00:13:11,748 --> 00:13:15,092
that you're going to get, r,
plus the value of whatever

295
00:13:15,092 --> 00:13:16,517
state that you end up in.

296
00:13:16,517 --> 00:13:18,868
So, let's say, s prime.

297
00:13:18,868 --> 00:13:22,319
And since we know that we
have the optimal policy,

298
00:13:22,319 --> 00:13:24,150
then we also know that we're going to

299
00:13:24,150 --> 00:13:26,202
play the best action that we can,

300
00:13:26,202 --> 00:13:28,746
right, at our state s prime.

301
00:13:28,746 --> 00:13:31,413
And so then, the value at state s prime

302
00:13:31,413 --> 00:13:34,432
is just going to be the
maximum over our actions,

303
00:13:34,432 --> 00:13:38,626
a prime, of Q star at s prime, a prime.

304
00:13:38,626 --> 00:13:41,325
And so then we get this

305
00:13:41,325 --> 00:13:44,119
identity here, for optimal Q-value.

306
00:13:44,119 --> 00:13:46,753
Right, and then also, as always, we have

307
00:13:46,753 --> 00:13:48,075
this expectation here,

308
00:13:48,075 --> 00:13:49,880
because we have randomness over what state

309
00:13:49,880 --> 00:13:52,380
that we're going to end up in.

310
00:13:54,252 --> 00:13:56,782
And then we can also
infer, from here, that our

311
00:13:56,782 --> 00:13:58,928
optimal policy, right, is going to consist

312
00:13:58,928 --> 00:14:00,860
of taking the best action in any state,

313
00:14:00,860 --> 00:14:02,488
as specified by Q star.

314
00:14:02,488 --> 00:14:04,295
Q star is going to tell us

315
00:14:04,295 --> 00:14:05,462
of the maximum

316
00:14:06,540 --> 00:14:08,437
future reward that we can
get from any of our actions,

317
00:14:08,437 --> 00:14:09,456
so we should just

318
00:14:09,456 --> 00:14:11,356
take a policy that's following this

319
00:14:11,356 --> 00:14:13,615
and just taking the action that's

320
00:14:13,615 --> 00:14:16,863
going to lead to best reward.

321
00:14:16,863 --> 00:14:21,025
Okay, so how can we solve
for this optimal policy?

322
00:14:21,025 --> 00:14:23,381
So, one way we can solve for this is

323
00:14:23,381 --> 00:14:25,692
something called a value
iteration algorithm,

324
00:14:25,692 --> 00:14:28,046
where we're going to use
this Bellman equation

325
00:14:28,046 --> 00:14:29,527
as an iterative update.

326
00:14:29,527 --> 00:14:33,830
So at each step, we're going
to refine our approximation

327
00:14:33,830 --> 00:14:37,997
of Q star by trying to
enforce the Bellman equation.

328
00:14:39,347 --> 00:14:42,602
And so, under some
mathematical conditions,

329
00:14:42,602 --> 00:14:45,602
we also know that this sequence Q, i

330
00:14:47,008 --> 00:14:49,569
of our Q-function is going
to converge to our optimal

331
00:14:49,569 --> 00:14:52,236
Q star as i approaches infinity.

332
00:14:54,257 --> 00:14:55,579
And so this, this works well,

333
00:14:55,579 --> 00:14:58,329
but what's the problem with this?

334
00:14:59,184 --> 00:15:01,887
Well, an important problem
is that this is not scalable.

335
00:15:01,887 --> 00:15:02,720
Right?

336
00:15:02,720 --> 00:15:03,553
We have to compute

337
00:15:03,553 --> 00:15:05,793
Q of s, a here for
every state, action pair

338
00:15:05,793 --> 00:15:08,597
in order to make our iterative updates.

339
00:15:08,597 --> 00:15:10,382
Right, but then this is a problem if,

340
00:15:10,382 --> 00:15:13,049
for example, if we look at these

341
00:15:14,021 --> 00:15:15,865
the state of, for example, an Atari game

342
00:15:15,865 --> 00:15:17,519
that we had earlier, it's going to be

343
00:15:17,519 --> 00:15:18,933
your screen of pixels.

344
00:15:18,933 --> 00:15:22,229
And this is a huge state
space, and it's basically

345
00:15:22,229 --> 00:15:23,865
computationally infeasible

346
00:15:23,865 --> 00:15:27,448
to compute this for
the entire state space.

347
00:15:28,725 --> 00:15:31,200
Okay, so what's the solution to this?

348
00:15:31,200 --> 00:15:33,141
Well, we can use a function approximator

349
00:15:33,141 --> 00:15:35,908
to estimate Q of s, a

350
00:15:35,908 --> 00:15:37,620
so, for example, a neural network, right.

351
00:15:37,620 --> 00:15:40,400
So, we've seen before that
any time, if we have some

352
00:15:40,400 --> 00:15:42,367
really complex function that
don't know, that we want

353
00:15:42,367 --> 00:15:44,360
to estimate, a neural network is

354
00:15:44,360 --> 00:15:46,693
a good way to estimate this.

355
00:15:48,472 --> 00:15:51,458
Okay, so this is going to take us to our

356
00:15:51,458 --> 00:15:54,242
formulation of Q-learning
that we're going to look at.

357
00:15:54,242 --> 00:15:56,646
And so, what we're going
to do is we're going

358
00:15:56,646 --> 00:15:58,906
to use a function approximator

359
00:15:58,906 --> 00:16:02,118
in order to estimate our
action value function.

360
00:16:02,118 --> 00:16:02,951
Right?

361
00:16:02,951 --> 00:16:04,502
And if this function approximator

362
00:16:04,502 --> 00:16:06,013
is a deep neural network, which is

363
00:16:06,013 --> 00:16:08,142
what's been used recently,

364
00:16:08,142 --> 00:16:10,782
then this is going to be
called deep Q-learning.

365
00:16:10,782 --> 00:16:12,322
And so this is something that

366
00:16:12,322 --> 00:16:15,742
you'll hear around as one
of the common approaches

367
00:16:15,742 --> 00:16:20,150
to deep reinforcement
learning that's in use.

368
00:16:20,150 --> 00:16:21,259
Right, and so in this case,

369
00:16:21,259 --> 00:16:23,474
we also have our function parameters

370
00:16:23,474 --> 00:16:26,134
theta here, so our Q-value function

371
00:16:26,134 --> 00:16:28,348
is determined by these weights,

372
00:16:28,348 --> 00:16:30,765
theta, of our neural network.

373
00:16:33,050 --> 00:16:35,425
Okay, so given this
function approximation,

374
00:16:35,425 --> 00:16:37,970
how do we solve for our optimal policy?

375
00:16:37,970 --> 00:16:39,814
So remember that we want to find

376
00:16:39,814 --> 00:16:44,744
a Q-function that's satisfying
the Bellman equation.

377
00:16:44,744 --> 00:16:47,017
Right, and so we want to
enforce this Bellman equation

378
00:16:47,017 --> 00:16:50,452
to happen, so what we
can do when we have this

379
00:16:50,452 --> 00:16:54,713
neural network approximating
our Q-function is that

380
00:16:54,713 --> 00:16:56,811
we can train this where our loss function

381
00:16:56,811 --> 00:16:58,169
is going to try and minimize

382
00:16:58,169 --> 00:17:00,240
the error of our Bellman equation, right?

383
00:17:00,240 --> 00:17:03,689
Or how far q of s, a is from its target,

384
00:17:03,689 --> 00:17:06,454
which is the Y_i here,
the right hand side

385
00:17:06,454 --> 00:17:09,853
of the Bellman equation
that we saw earlier.

386
00:17:09,853 --> 00:17:12,103
So, we're basically going to take these

387
00:17:12,103 --> 00:17:13,994
forward passes of our

388
00:17:13,994 --> 00:17:16,928
loss function, trying
to minimize this error

389
00:17:16,929 --> 00:17:19,332
and then our backward
pass, our gradient update,

390
00:17:19,332 --> 00:17:20,863
is just going to be

391
00:17:20,863 --> 00:17:23,243
you just take the gradient of this

392
00:17:23,243 --> 00:17:28,182
loss, with respect to our
network parameter's theta.

393
00:17:28,183 --> 00:17:31,568
Right, and so our goal is again to

394
00:17:31,568 --> 00:17:33,752
have this effect as we're
taking gradient steps

395
00:17:33,752 --> 00:17:36,107
of iteratively trying
to make our Q-function

396
00:17:36,107 --> 00:17:38,436
closer to our target value.

397
00:17:38,436 --> 00:17:40,853
So, any questions about this?

398
00:17:42,691 --> 00:17:43,524
Okay.

399
00:17:44,537 --> 00:17:48,719
So let's look at a case
study of an example where

400
00:17:48,719 --> 00:17:50,824
one of the classic examples
of deep reinforcement learning

401
00:17:50,824 --> 00:17:53,370
where this approach was applied.

402
00:17:53,370 --> 00:17:56,174
And so we're going to look at
this problem that we saw earlier

403
00:17:56,174 --> 00:17:59,744
of playing Atari games,
where our objective was

404
00:17:59,744 --> 00:18:01,746
to complete the game
with the highest score

405
00:18:01,746 --> 00:18:04,150
and remember our state is
going to be the raw pixel

406
00:18:04,150 --> 00:18:05,460
inputs of the game state,

407
00:18:05,460 --> 00:18:07,064
and we can take these actions

408
00:18:07,064 --> 00:18:09,308
of moving left, right, up, down,

409
00:18:09,308 --> 00:18:12,964
or whatever actions of
the particular game.

410
00:18:12,964 --> 00:18:15,210
And our reward at each time
step, we're going to get

411
00:18:15,210 --> 00:18:18,509
a reward of our score
increase or decrease that we

412
00:18:18,509 --> 00:18:21,183
got at this time step, and
so our cumulative total

413
00:18:21,183 --> 00:18:24,435
reward is this total reward
that we'll usually see

414
00:18:24,435 --> 00:18:27,095
at the top of the screen.

415
00:18:27,095 --> 00:18:30,135
Okay, so the network that
we're going to use for our

416
00:18:30,135 --> 00:18:32,955
Q-function is going to
look something like this,

417
00:18:32,955 --> 00:18:37,355
right, where we have our
Q-network, with weight's theta.

418
00:18:37,355 --> 00:18:41,272
And then our input, our
state s, is going to be

419
00:18:42,259 --> 00:18:43,791
our current game screen.

420
00:18:43,791 --> 00:18:45,377
And in practice we're going to take

421
00:18:45,377 --> 00:18:49,509
a stack of the last four
frames, so we have some history.

422
00:18:49,509 --> 00:18:52,340
And so we'll take these raw pixel values,

423
00:18:52,340 --> 00:18:55,609
we'll do some, you know, RGB
to gray-scale conversions,

424
00:18:55,609 --> 00:18:57,053
some down-sampling, some cropping,

425
00:18:57,053 --> 00:18:58,609
so, some pre-processing.

426
00:18:58,609 --> 00:19:02,543
And what we'll get out of
this is this 84 by 84 by four

427
00:19:02,543 --> 00:19:04,631
stack of the last four frames.

428
00:19:04,631 --> 00:19:05,464
Yeah, question.

429
00:19:05,464 --> 00:19:09,631
[inaudible question from audience]

430
00:19:12,792 --> 00:19:14,768
Okay, so the question
is, are we saying here

431
00:19:14,768 --> 00:19:18,067
that our network is
going to approximate our

432
00:19:18,067 --> 00:19:20,809
Q-value function for
different state, action pairs,

433
00:19:20,809 --> 00:19:22,491
for example, four of these?

434
00:19:22,491 --> 00:19:24,765
Yeah, that's correct.

435
00:19:24,765 --> 00:19:25,598
We'll see,

436
00:19:25,598 --> 00:19:27,551
we'll talk about that in a few slides.

437
00:19:27,551 --> 00:19:29,935
[inaudible question from audience]

438
00:19:29,935 --> 00:19:30,768
So, no.

439
00:19:30,768 --> 00:19:32,883
So, we don't have a Softmax
layer after the connected,

440
00:19:32,883 --> 00:19:35,535
because here our goal
is to directly predict

441
00:19:35,535 --> 00:19:36,816
our Q-value functions.

442
00:19:36,816 --> 00:19:37,712
[inaudible question from audience]

443
00:19:37,712 --> 00:19:38,545
Q-values.

444
00:19:38,545 --> 00:19:40,583
[inaudible question from audience]

445
00:19:40,583 --> 00:19:44,014
Yes, so it's more doing
regression to our Q-values.

446
00:19:44,014 --> 00:19:47,549
Okay, so we have our input to this network

447
00:19:47,549 --> 00:19:51,007
and then on top of this,
we're going to have

448
00:19:51,007 --> 00:19:52,847
a couple of familiar convolutional layers,

449
00:19:52,847 --> 00:19:54,084
and a fully-connected layer,

450
00:19:54,084 --> 00:19:55,334
so here we have

451
00:19:56,191 --> 00:19:58,036
an eight-by-eight
convolutions and we have some

452
00:19:58,036 --> 00:19:59,611
four-by-four convolutions.

453
00:19:59,611 --> 00:20:01,861
Then we have a FC 256 layer,

454
00:20:01,861 --> 00:20:03,458
so this is just a standard kind of networK

455
00:20:03,458 --> 00:20:05,674
that you've seen before.

456
00:20:05,674 --> 00:20:10,382
And then, finally, our last
fully-connected layer has

457
00:20:10,382 --> 00:20:13,470
a vector of outputs, which
is corresponding to your

458
00:20:13,470 --> 00:20:16,074
Q-value for each action, right, given

459
00:20:16,074 --> 00:20:17,415
the state that you've input.

460
00:20:17,415 --> 00:20:19,565
And so, for example, if
you have four actions,

461
00:20:19,565 --> 00:20:21,770
then here we have this
four-dimensional output

462
00:20:21,770 --> 00:20:25,570
corresponding to Q of
current s, as well as a-one,

463
00:20:25,570 --> 00:20:28,685
and then a-two, a-three, and a-four.

464
00:20:28,685 --> 00:20:30,857
Right so this is going
to be one scalar value

465
00:20:30,857 --> 00:20:33,179
for each of our actions.

466
00:20:33,179 --> 00:20:35,610
And then the number of
actions that we have

467
00:20:35,610 --> 00:20:36,955
can vary between,

468
00:20:36,955 --> 00:20:41,122
for example, 4 to 18,
depending on the Atari game.

469
00:20:43,073 --> 00:20:44,839
And one nice thing here is that

470
00:20:44,839 --> 00:20:46,709
using this network structure,

471
00:20:46,709 --> 00:20:49,931
a single feedforward
pass is able to compute

472
00:20:49,931 --> 00:20:52,810
the Q-values for all functions

473
00:20:52,810 --> 00:20:54,651
from the current state.

474
00:20:54,651 --> 00:20:56,117
And so this is really efficient.

475
00:20:56,117 --> 00:20:59,158
Right, so basically we
take our current state

476
00:20:59,158 --> 00:21:03,121
in and then because we have
this output of an action

477
00:21:03,121 --> 00:21:05,946
for each, or Q-value for each
action, as our output layer,

478
00:21:05,946 --> 00:21:10,259
we're able to do one pass and
get all of these values out.

479
00:21:10,259 --> 00:21:12,235
And then in order to train this,

480
00:21:12,235 --> 00:21:15,078
we're just going to use our
loss function from before.

481
00:21:15,078 --> 00:21:17,661
Remember, we're trying to
enforce this Bellman equation

482
00:21:17,661 --> 00:21:21,329
and so, on our forward
pass, our loss function

483
00:21:21,329 --> 00:21:25,193
we're going to try and
iteratively make our Q-value

484
00:21:25,193 --> 00:21:27,987
close to our target value,

485
00:21:27,987 --> 00:21:29,315
that it should have.

486
00:21:29,315 --> 00:21:31,281
And then our backward pass is just

487
00:21:31,281 --> 00:21:34,235
directly taking the gradient of this

488
00:21:34,235 --> 00:21:37,277
loss function that we have and then taking

489
00:21:37,277 --> 00:21:39,777
a gradient step based on that.

490
00:21:40,706 --> 00:21:42,948
So one other thing that's used
here that I want to mention

491
00:21:42,948 --> 00:21:45,639
is something called experience replay.

492
00:21:45,639 --> 00:21:49,556
And so this addresses a
problem with just using

493
00:21:50,579 --> 00:21:53,440
the plain two network
that I just described,

494
00:21:53,440 --> 00:21:55,416
which is that learning from batches

495
00:21:55,416 --> 00:21:58,134
of consecutive samples is bad.

496
00:21:58,134 --> 00:21:58,967
And so the reason

497
00:21:58,967 --> 00:22:01,268
because of this, right, is so for just

498
00:22:01,268 --> 00:22:03,578
playing the game, taking samples

499
00:22:03,578 --> 00:22:06,074
of state action rewards that we have

500
00:22:06,074 --> 00:22:08,222
and just taking consecutive
samples of these

501
00:22:08,222 --> 00:22:09,410
and training with these,

502
00:22:09,410 --> 00:22:11,814
well all of these samples are correlated

503
00:22:11,814 --> 00:22:14,218
and so this leads to

504
00:22:14,218 --> 00:22:16,118
inefficient learning, first of all,

505
00:22:16,118 --> 00:22:19,014
and also, because of this,
our current Q-network

506
00:22:19,014 --> 00:22:21,456
parameters, right, this
determines the policy

507
00:22:21,456 --> 00:22:24,842
that we're going to follow,
it determines our next

508
00:22:24,842 --> 00:22:25,798
samples that we're going to get that

509
00:22:25,798 --> 00:22:27,394
we're going to use for training.

510
00:22:27,394 --> 00:22:29,578
And so this leads to problems where

511
00:22:29,578 --> 00:22:30,832
you can have bad feedback loops.

512
00:22:30,832 --> 00:22:33,920
So, for example, if
currently the maximizing

513
00:22:33,920 --> 00:22:35,468
action that's going to take left,

514
00:22:35,468 --> 00:22:37,588
well this is going to bias all of my

515
00:22:37,588 --> 00:22:39,380
upcoming training examples to be dominated

516
00:22:39,380 --> 00:22:42,297
by samples from the left-hand side.

517
00:22:43,306 --> 00:22:45,406
And so this is a problem, right?

518
00:22:45,406 --> 00:22:47,875
And so the way that we
are going to address these

519
00:22:47,875 --> 00:22:49,808
problems is by using something called

520
00:22:49,808 --> 00:22:53,098
experience replay, where
we're going to keep this

521
00:22:53,098 --> 00:22:56,469
replay memory table of
transitions of state,

522
00:22:56,469 --> 00:22:59,345
as state, action, reward, next state,

523
00:22:59,345 --> 00:23:01,353
transitions that we have, and we're going

524
00:23:01,353 --> 00:23:04,279
to continuously update this
table with new transitions

525
00:23:04,279 --> 00:23:07,185
that we're getting as
game episodes are played,

526
00:23:07,185 --> 00:23:08,773
as we're getting more experience.

527
00:23:08,773 --> 00:23:10,653
Right, and so now what we can do

528
00:23:10,653 --> 00:23:13,207
is that we can now train
our Q-network on random,

529
00:23:13,207 --> 00:23:16,335
mini-batches of transitions
from the replay memory.

530
00:23:16,335 --> 00:23:19,261
Right, so instead of
using consecutive samples,

531
00:23:19,261 --> 00:23:21,815
we're now going to sample across these

532
00:23:21,815 --> 00:23:24,827
transitions that we've accumulated
random samples of these,

533
00:23:24,827 --> 00:23:27,573
and this breaks all of the,

534
00:23:27,573 --> 00:23:31,007
these correlation problems
that we had earlier.

535
00:23:31,007 --> 00:23:33,425
And then also, as another

536
00:23:33,425 --> 00:23:36,370
side benefit is that
each of these transitions

537
00:23:36,370 --> 00:23:39,207
can also contribute to potentially
multiple weight updates.

538
00:23:39,207 --> 00:23:41,440
We're just sampling from this table and so

539
00:23:41,440 --> 00:23:43,652
we could sample one multiple times.

540
00:23:43,652 --> 00:23:44,918
And so, this is going to lead

541
00:23:44,918 --> 00:23:47,585
also to greater data efficiency.

542
00:23:50,580 --> 00:23:52,442
Okay, so let's put this all together

543
00:23:52,442 --> 00:23:54,000
and let's look at the full algorithm

544
00:23:54,000 --> 00:23:57,583
for deep Q-learning
with experience replay.

545
00:23:59,166 --> 00:24:03,940
So we're going to start off with
initializing our replay memory

546
00:24:03,940 --> 00:24:07,383
to some capacity that we
choose, N, and then we're also

547
00:24:07,383 --> 00:24:09,703
going to initialize our

548
00:24:09,703 --> 00:24:13,075
Q-network, just with our random weights

549
00:24:13,075 --> 00:24:14,830
or initial weights.

550
00:24:14,830 --> 00:24:18,688
And then we're going to play
M episodes, or full games.

551
00:24:18,688 --> 00:24:21,832
This is going to be our training episodes.

552
00:24:21,832 --> 00:24:22,998
And then what we're going to do

553
00:24:22,998 --> 00:24:26,574
is we're going to initialize our state,

554
00:24:26,574 --> 00:24:29,526
using the starting game screen pixels

555
00:24:29,526 --> 00:24:31,265
at the beginning of each episode.

556
00:24:31,265 --> 00:24:33,555
And remember, we go through
the pre-processing step

557
00:24:33,555 --> 00:24:37,814
to get to our actual input state.

558
00:24:37,814 --> 00:24:39,313
And then for each time step

559
00:24:39,313 --> 00:24:41,584
of a game that we're currently playing,

560
00:24:41,584 --> 00:24:44,236
we're going to, with a small probability,

561
00:24:44,236 --> 00:24:46,268
select a random action,

562
00:24:46,268 --> 00:24:49,819
so one thing that's
important in these algorithms

563
00:24:49,819 --> 00:24:53,141
is to have sufficient exploration,

564
00:24:53,141 --> 00:24:54,957
so we want to make sure that

565
00:24:54,957 --> 00:24:58,559
we are sampling different
parts of the state space.

566
00:24:58,559 --> 00:25:00,353
And then otherwise, we're going

567
00:25:00,353 --> 00:25:02,405
to select from the greedy action

568
00:25:02,405 --> 00:25:03,614
from the current policy.

569
00:25:03,614 --> 00:25:05,564
Right, so most of the time
we'll take the greedy action

570
00:25:05,564 --> 00:25:07,443
that we think is

571
00:25:07,443 --> 00:25:11,083
a good policy of the type of
actions that we want to take

572
00:25:11,083 --> 00:25:13,580
and states that we want to see,
and with a small probability

573
00:25:13,580 --> 00:25:16,300
we'll sample something random.

574
00:25:16,300 --> 00:25:18,429
Okay, so then we'll take this action,

575
00:25:18,429 --> 00:25:23,076
a, t, and we'll observe the
next reward and the next state.

576
00:25:23,076 --> 00:25:26,070
So r, t and s, t plus one.

577
00:25:26,070 --> 00:25:28,385
And then we'll take this and
we'll store this transition

578
00:25:28,385 --> 00:25:32,771
in our replay memory
that we're building up.

579
00:25:32,771 --> 00:25:34,354
And then we're going to take,

580
00:25:34,354 --> 00:25:35,577
we're going to train a
network a little bit.

581
00:25:35,577 --> 00:25:37,550
So we're going to do experience replay

582
00:25:37,550 --> 00:25:40,429
and we'll take a sample
of a random mini-batches

583
00:25:40,429 --> 00:25:41,901
of transitions that we have

584
00:25:41,901 --> 00:25:44,543
from the replay memory,
and then we'll perform

585
00:25:44,543 --> 00:25:47,214
a gradient descent step on this.

586
00:25:47,214 --> 00:25:49,635
Right, so this is going to
be our full training loop.

587
00:25:49,635 --> 00:25:52,561
We're going to be
continuously playing this game

588
00:25:52,561 --> 00:25:55,774
and then also sampling

589
00:25:55,774 --> 00:25:58,431
minibatches, using
experienced replay to update

590
00:25:58,431 --> 00:26:00,100
our weights of our Q-network and then

591
00:26:00,100 --> 00:26:02,350
continuing in this fashion.

592
00:26:03,887 --> 00:26:05,912
Okay, so let's see.

593
00:26:05,912 --> 00:26:07,524
Let's see if I can,

594
00:26:07,524 --> 00:26:09,030
is this playing?

595
00:26:09,030 --> 00:26:11,852
Okay, so let's take a look

596
00:26:11,852 --> 00:26:13,532
at this deep Q-learning algorithm

597
00:26:13,532 --> 00:26:17,699
from Google DeepMind, trained
on an Atari game of Breakout.

598
00:26:20,911 --> 00:26:22,316
Alright, so it's saying
here that our input

599
00:26:22,316 --> 00:26:26,185
is just going to be our
state are raw game pixels.

600
00:26:26,185 --> 00:26:28,385
And so here we're looking
at what's happening

601
00:26:28,385 --> 00:26:29,520
at the beginning of training.

602
00:26:29,520 --> 00:26:31,505
So we've just started training a bit.

603
00:26:31,505 --> 00:26:33,159
And

604
00:26:33,159 --> 00:26:34,650
right, so it's going to look to

605
00:26:34,650 --> 00:26:36,824
it's learned to kind of hit the ball,

606
00:26:36,824 --> 00:26:40,303
but it's not doing a very
good job of sustaining it.

607
00:26:40,303 --> 00:26:42,886
But it is looking for the ball.

608
00:26:50,969 --> 00:26:53,320
Okay, so now after some more training,

609
00:26:53,320 --> 00:26:55,737
it looks like a couple hours.

610
00:27:00,946 --> 00:27:05,113
Okay, so now it's learning
to do a pretty good job here.

611
00:27:06,190 --> 00:27:08,677
So it's able to continuously follow

612
00:27:08,677 --> 00:27:10,677
this ball and be able to

613
00:27:13,882 --> 00:27:16,593
to remove most of the blocks.

614
00:27:16,593 --> 00:27:18,926
Right, so after 240 minutes.

615
00:27:33,248 --> 00:27:36,203
Okay, so here it's found
the pro strategy, right?

616
00:27:36,203 --> 00:27:38,225
You want to get all the
way to the top and then

617
00:27:38,225 --> 00:27:39,975
have it go by itself.

618
00:27:41,197 --> 00:27:42,796
Okay, so

619
00:27:42,796 --> 00:27:44,450
this is an example of using

620
00:27:44,450 --> 00:27:46,815
deep Q-learning in order to

621
00:27:46,815 --> 00:27:49,501
train an agent to be
able to play Atari games.

622
00:27:49,501 --> 00:27:51,485
It's able to do this on many Atari games

623
00:27:51,485 --> 00:27:52,998
and so you can check out

624
00:27:52,998 --> 00:27:55,081
some more of this online.

625
00:27:56,419 --> 00:27:58,168
Okay, so we've talked about Q-learning.

626
00:27:58,168 --> 00:28:01,149
But there is a problem
with Q-learning, right?

627
00:28:01,149 --> 00:28:03,754
It can be challenging
and what's the problem?

628
00:28:03,754 --> 00:28:05,126
Well, the problem can be that

629
00:28:05,126 --> 00:28:07,226
the Q-function is very complicated.

630
00:28:07,226 --> 00:28:09,344
Right, so we have to, we're
saying that we want to learn

631
00:28:09,344 --> 00:28:12,335
the value of every state action pair.

632
00:28:12,335 --> 00:28:14,854
So, if, let's say you have
something, for example,

633
00:28:14,854 --> 00:28:17,275
a robot grasping, wanting
to grasp an object.

634
00:28:17,275 --> 00:28:19,576
Right, you're going to have a
really high dimensional state.

635
00:28:19,576 --> 00:28:23,033
You have, I mean, let's
say you have all of your

636
00:28:23,033 --> 00:28:26,225
even just joint, joint
positions, and angles.

637
00:28:26,225 --> 00:28:29,380
Right, and so learning the
exact value of every state

638
00:28:29,380 --> 00:28:31,421
action pair that you have, right,

639
00:28:31,421 --> 00:28:34,171
can be really, really hard to do.

640
00:28:35,493 --> 00:28:38,724
But on the other hand, your
policy can be much simpler.

641
00:28:38,724 --> 00:28:40,310
Right, like what you want this robot to do

642
00:28:40,310 --> 00:28:42,542
maybe just to have this simple motion

643
00:28:42,542 --> 00:28:44,556
of just closing your hand, right?

644
00:28:44,556 --> 00:28:45,952
Just, move your fingers in this

645
00:28:45,952 --> 00:28:48,252
particular direction and keep going.

646
00:28:48,252 --> 00:28:51,832
And so, that leads to the question of

647
00:28:51,832 --> 00:28:54,142
can we just learn this policy directly?

648
00:28:54,142 --> 00:28:55,872
Right, is it possible,
maybe, to just find the best

649
00:28:55,872 --> 00:28:58,306
policy from a collection of policies,

650
00:28:58,306 --> 00:28:59,988
without trying to go through this process

651
00:28:59,988 --> 00:29:02,078
of estimating your Q-value

652
00:29:02,078 --> 00:29:05,495
and then using that to infer your policy.

653
00:29:06,790 --> 00:29:09,288
So, this is an approach that

654
00:29:09,288 --> 00:29:10,257
oh,

655
00:29:10,257 --> 00:29:13,154
so, okay, this is an approach that

656
00:29:13,154 --> 00:29:15,938
we're going to call policy gradients.

657
00:29:15,938 --> 00:29:18,228
And so, formally, let's define a

658
00:29:18,228 --> 00:29:20,858
class of parametrized policies.

659
00:29:20,858 --> 00:29:24,146
Parametrized by weights theta,

660
00:29:24,146 --> 00:29:25,889
and so for each policy

661
00:29:25,889 --> 00:29:27,791
let's define the value of the policy.

662
00:29:27,791 --> 00:29:30,859
So, J, our value J,
given parameters theta,

663
00:29:30,859 --> 00:29:32,437
is going to be, or expected

664
00:29:32,437 --> 00:29:35,723
some cumulative sum of future
rewards that we care about.

665
00:29:35,723 --> 00:29:38,971
So, the same reward that we've been using.

666
00:29:38,971 --> 00:29:41,879
And so our goal then, under this setup

667
00:29:41,879 --> 00:29:44,719
is that we want to find an optimal policy,

668
00:29:44,719 --> 00:29:48,243
theta star, which is the maximum, right,

669
00:29:48,243 --> 00:29:51,548
arg max over theta of J of theta.

670
00:29:51,548 --> 00:29:53,946
So we want to find the
policy, the policy parameters

671
00:29:53,946 --> 00:29:56,917
that gives our best expected reward.

672
00:29:56,917 --> 00:29:58,834
So, how can we do this?

673
00:30:00,178 --> 00:30:01,011
Any ideas?

674
00:30:04,993 --> 00:30:06,843
Okay, well, what we can do

675
00:30:06,843 --> 00:30:10,155
is just a gradient assent on
our policy parameters, right?

676
00:30:10,155 --> 00:30:12,476
We've learned that given
some objective that we have,

677
00:30:12,476 --> 00:30:15,460
some parameters we can
just use gradient asscent

678
00:30:15,460 --> 00:30:17,512
and gradient assent in order

679
00:30:17,512 --> 00:30:20,762
to continuously improve our parameters.

680
00:30:23,202 --> 00:30:24,950
And so let's talk more
specifically about how

681
00:30:24,950 --> 00:30:27,174
we can do this, which we're going to call

682
00:30:27,174 --> 00:30:29,196
here the reinforce algorithm.

683
00:30:29,196 --> 00:30:31,068
So, mathematically, we can write

684
00:30:31,068 --> 00:30:34,375
out our expected future reward

685
00:30:34,375 --> 00:30:36,781
over trajectories, and
so we're going to sample

686
00:30:36,781 --> 00:30:38,611
these trajectories of experience, right,

687
00:30:38,611 --> 00:30:40,286
like for example episodes of game play

688
00:30:40,286 --> 00:30:41,902
that we talked about earlier.

689
00:30:41,902 --> 00:30:45,673
S-zero, a-zero, r-zero, s-one,

690
00:30:45,673 --> 00:30:47,411
a-one, r-one, and so on.

691
00:30:47,411 --> 00:30:51,723
Using some policy pi of theta.

692
00:30:51,723 --> 00:30:54,139
Right, and then so, for each trajectory

693
00:30:54,139 --> 00:30:57,739
we can compute a reward
for that trajectory.

694
00:30:57,739 --> 00:30:59,135
It's the cumulative reward that we

695
00:30:59,135 --> 00:31:01,245
got from following this trajectory.

696
00:31:01,245 --> 00:31:03,733
And then the value of a policy,

697
00:31:03,733 --> 00:31:05,968
pi sub theta, is going
to be just the expected

698
00:31:05,968 --> 00:31:07,933
reward of these
trajectories that we can get

699
00:31:07,933 --> 00:31:10,570
from the following pi sub theta.

700
00:31:10,570 --> 00:31:12,701
So that's here, this
expectation over trajectories

701
00:31:12,701 --> 00:31:16,868
that we can get, sampling
trajectories from our policy.

702
00:31:18,563 --> 00:31:19,424
Okay.

703
00:31:19,424 --> 00:31:21,288
So, we want to do gradient ascent, right?

704
00:31:21,288 --> 00:31:22,961
So let's differentiate this.

705
00:31:22,961 --> 00:31:25,023
Once we differentiate
this, then we can just take

706
00:31:25,023 --> 00:31:27,356
gradient steps, like normal.

707
00:31:28,535 --> 00:31:30,418
So, the problem is that
now if we try and just

708
00:31:30,418 --> 00:31:32,678
differentiate this exactly,

709
00:31:32,678 --> 00:31:34,300
this is intractable, right?

710
00:31:34,300 --> 00:31:37,388
So, the gradient of an
expectation is problematic

711
00:31:37,388 --> 00:31:41,319
when p is dependent on
theta here, because here

712
00:31:41,319 --> 00:31:43,513
we want to take this gradient

713
00:31:43,513 --> 00:31:47,661
of p of tau, given theta,

714
00:31:47,661 --> 00:31:48,766
but this is going to be,

715
00:31:48,766 --> 00:31:50,591
we want to take this integral over tau.

716
00:31:50,591 --> 00:31:53,033
Right, so this is intractable.

717
00:31:53,033 --> 00:31:57,327
However, we can use a trick
here to get around this.

718
00:31:57,327 --> 00:32:01,855
And this trick is taking this
gradient that we want, of p.

719
00:32:01,855 --> 00:32:03,203
We can rewrite this

720
00:32:03,203 --> 00:32:04,941
by just multiplying this by one,

721
00:32:04,941 --> 00:32:07,081
by multiplying top and bottom,

722
00:32:07,081 --> 00:32:10,286
both by p of tau given theta.

723
00:32:10,286 --> 00:32:12,052
Right, and then if we look at these terms

724
00:32:12,052 --> 00:32:14,248
that we have now here, in the
way that I've written this,

725
00:32:14,248 --> 00:32:15,958
on the left and the right, this is

726
00:32:15,958 --> 00:32:18,815
actually going to be equivalent to

727
00:32:18,815 --> 00:32:23,424
p of tau times our gradient

728
00:32:23,424 --> 00:32:26,170
with respect to theta, of log, of p.

729
00:32:26,170 --> 00:32:29,074
Right, because the gradient
of the log of p is just going

730
00:32:29,074 --> 00:32:32,741
to be one over p times gradient of p.

731
00:32:33,808 --> 00:32:36,934
Okay, so if we then inject this back

732
00:32:36,934 --> 00:32:41,385
into our expression that we
had earlier for this gradient,

733
00:32:41,385 --> 00:32:43,426
we can see that, what this
will actually look like,

734
00:32:43,426 --> 00:32:46,059
right, because now we
have a gradient of log p

735
00:32:46,059 --> 00:32:49,106
times our probabilities of
all of these trajectories

736
00:32:49,106 --> 00:32:52,187
and then taking this
integral here, over tau.

737
00:32:52,187 --> 00:32:54,495
This is now going to be an expectation

738
00:32:54,495 --> 00:32:58,586
over our trajectories tau,
and so what we've done here

739
00:32:58,586 --> 00:33:02,751
is that we've taken a
gradient of an expectation

740
00:33:02,751 --> 00:33:06,823
and we've transformed it into
an expectation of gradients.

741
00:33:06,823 --> 00:33:09,156
Right, and so now we can use

742
00:33:10,051 --> 00:33:12,404
sample trajectories that we can get

743
00:33:12,404 --> 00:33:14,712
in order to estimate our gradient.

744
00:33:14,712 --> 00:33:17,343
And so we do this using
Monte Carlo sampling,

745
00:33:17,343 --> 00:33:21,260
and this is one of the
core ideas of reinforce.

746
00:33:23,624 --> 00:33:25,846
Okay, so looking at this

747
00:33:25,846 --> 00:33:28,180
expression that we want to compute,

748
00:33:28,180 --> 00:33:30,421
can we compute these
quantities that we had here

749
00:33:30,421 --> 00:33:33,071
without knowing the
transition probabilities?

750
00:33:33,071 --> 00:33:36,643
Alright, so we have that
p of tau is going to be

751
00:33:36,643 --> 00:33:38,466
the probability of a trajectory.

752
00:33:38,466 --> 00:33:40,387
It's going to be the product of

753
00:33:40,387 --> 00:33:43,379
all of our transition
probabilities of the next state

754
00:33:43,379 --> 00:33:45,821
that we get, given our
current state and action

755
00:33:45,821 --> 00:33:49,051
as well as our probability
of the actions that

756
00:33:49,051 --> 00:33:52,232
we've taken under our policy pi.

757
00:33:52,232 --> 00:33:54,743
Right, so we're going to
multiply all of these together,

758
00:33:54,743 --> 00:33:58,441
and get our probability of our trajectory.

759
00:33:58,441 --> 00:34:03,059
So this log of p of tau
that we want to compute

760
00:34:03,059 --> 00:34:06,334
is going to be we just
take this log and this will

761
00:34:06,334 --> 00:34:08,326
separate this out into a sum

762
00:34:08,326 --> 00:34:10,389
of pushing the logs inside.

763
00:34:10,389 --> 00:34:12,383
And then here, when we differentiate this,

764
00:34:12,384 --> 00:34:14,237
we can see we want to
differentiate with respect

765
00:34:14,237 --> 00:34:18,162
to theta, but this first
term that we have here,

766
00:34:18,163 --> 00:34:20,911
log p of the state
transition probabilities

767
00:34:20,911 --> 00:34:22,850
there's no theta term here, and so

768
00:34:22,850 --> 00:34:25,292
the only place where we have
theta is the second term

769
00:34:25,292 --> 00:34:28,709
that we have, of log of pi sub theta,

770
00:34:29,675 --> 00:34:32,914
of our action, given our
state, and so this is the only

771
00:34:32,914 --> 00:34:34,139
term that we keep

772
00:34:34,139 --> 00:34:37,368
in our gradient estimate,
and so we can see here that

773
00:34:37,369 --> 00:34:39,670
this doesn't depend on the
transition probabilities,

774
00:34:39,670 --> 00:34:41,293
right, so we actually don't need to know

775
00:34:41,293 --> 00:34:44,588
our transition probabilities
in order to computer

776
00:34:44,589 --> 00:34:46,422
our gradient estimate.

777
00:34:47,257 --> 00:34:50,879
And then, so, therefore
when we're sampling these,

778
00:34:50,880 --> 00:34:55,047
for any given trajectory tau,
we can estimate J of theta

779
00:34:56,306 --> 00:34:58,524
using this gradient estimate.

780
00:34:58,524 --> 00:35:00,472
This is here shown for a single trajectory

781
00:35:00,472 --> 00:35:02,220
from what we had earlier,

782
00:35:02,220 --> 00:35:05,271
and then we can also sample
over multiple trajectories

783
00:35:05,271 --> 00:35:07,188
to get the expectation.

784
00:35:09,248 --> 00:35:12,974
Okay, so given this gradient
estimator that we've derived,

785
00:35:12,974 --> 00:35:17,141
the interpretation that we can
make from this here, is that

786
00:35:18,217 --> 00:35:21,931
if our reward for a trajectory
is high, if the reward that

787
00:35:21,931 --> 00:35:25,226
we got from taking the
sequence of actions was good,

788
00:35:25,226 --> 00:35:27,517
then let's push up the
probabilities of all

789
00:35:27,517 --> 00:35:29,434
the actions that we've seen.

790
00:35:29,434 --> 00:35:31,458
Right, we're just going to say that

791
00:35:31,458 --> 00:35:33,141
these were good actions that we took.

792
00:35:33,141 --> 00:35:35,287
And then if the reward is low,

793
00:35:35,287 --> 00:35:37,186
we want to push down these probabilities.

794
00:35:37,186 --> 00:35:38,629
We want to say these were bad actions,

795
00:35:38,629 --> 00:35:40,747
let's try and not sample these so much.

796
00:35:40,747 --> 00:35:43,568
Right and so we can see
that's what's happening here,

797
00:35:43,568 --> 00:35:47,392
where we have pi of a, given s.

798
00:35:47,392 --> 00:35:50,980
This is the likelihood of
the actions that we've taken

799
00:35:50,980 --> 00:35:53,163
and then we're going to scale
this, we're going to take the

800
00:35:53,163 --> 00:35:56,621
gradient and the gradient
is going to tell us how much

801
00:35:56,621 --> 00:36:00,555
should we change the
parameters in order to increase

802
00:36:00,555 --> 00:36:03,575
our likelihood of our action, a, right?

803
00:36:03,575 --> 00:36:06,501
And then we're going to
take this and scale it by

804
00:36:06,501 --> 00:36:09,019
how much reward we actually got from it,

805
00:36:09,019 --> 00:36:12,602
so how good were these
actions, in reality.

806
00:36:14,561 --> 00:36:16,209
Okay, so

807
00:36:16,209 --> 00:36:18,454
this might seem simplistic to say that,

808
00:36:18,454 --> 00:36:21,124
you know, if a trajectory
is good, then we're saying

809
00:36:21,124 --> 00:36:22,965
here that all of its actions were good.

810
00:36:22,965 --> 00:36:23,798
Right?

811
00:36:23,798 --> 00:36:26,356
But, in expectation, this
actually averages out.

812
00:36:26,356 --> 00:36:30,125
So we have an unbiased estimator here,

813
00:36:30,125 --> 00:36:32,580
and so if you have many samples of this,

814
00:36:32,580 --> 00:36:35,622
then we will get an accurate
estimate of our gradient.

815
00:36:35,622 --> 00:36:38,510
And this is nice because we can just take

816
00:36:38,510 --> 00:36:40,666
gradient steps and we know
that we're going to be

817
00:36:40,666 --> 00:36:42,994
improving our loss
function and getting closer

818
00:36:42,994 --> 00:36:45,976
to, at least some local optimum of our

819
00:36:45,976 --> 00:36:48,602
policy parameters theta.

820
00:36:48,602 --> 00:36:50,690
Alright, but there is a problem with this,

821
00:36:50,690 --> 00:36:52,789
and the problem is that this also suffers

822
00:36:52,789 --> 00:36:54,884
from high variance.

823
00:36:54,884 --> 00:36:57,201
Because this credit
assignment is really hard.

824
00:36:57,201 --> 00:36:58,902
Right, we're saying that

825
00:36:58,902 --> 00:37:02,283
given a reward that we
got, we're going to say

826
00:37:02,283 --> 00:37:04,412
all of the actions were good,
we're just going to hope

827
00:37:04,412 --> 00:37:06,537
that this assignment of
which actions were actually

828
00:37:06,537 --> 00:37:08,395
the best actions, that mattered,

829
00:37:08,395 --> 00:37:11,080
are going to average out over time.

830
00:37:11,080 --> 00:37:14,598
And so this is really hard
and we need a lot of samples

831
00:37:14,598 --> 00:37:17,190
in order to have a good estimate.

832
00:37:17,190 --> 00:37:19,406
Alright, so this leads to the
question of, is there anything

833
00:37:19,406 --> 00:37:21,684
that we can do to reduce the variance

834
00:37:21,684 --> 00:37:23,851
and improve the estimator?

835
00:37:26,540 --> 00:37:29,123
And so variance reduction is

836
00:37:30,164 --> 00:37:33,323
an important area of research
in policy gradients,

837
00:37:33,323 --> 00:37:36,467
and in coming up with
ways in order to improve

838
00:37:36,467 --> 00:37:39,756
the estimator and require fewer samples.

839
00:37:39,756 --> 00:37:41,445
Alright, so let's look
at a couple of ideas

840
00:37:41,445 --> 00:37:43,278
of how we can do this.

841
00:37:44,202 --> 00:37:46,764
So given our gradient estimator,

842
00:37:46,764 --> 00:37:49,017
so the first idea is that we can

843
00:37:49,017 --> 00:37:52,610
push up the probabilities of an action

844
00:37:52,610 --> 00:37:56,258
only by it's affect on future rewards

845
00:37:56,258 --> 00:37:57,091
from that state, right?

846
00:37:57,091 --> 00:37:59,312
So, now with instead of scaling

847
00:37:59,312 --> 00:38:02,066
this likelihood, or
pushing up this likelihood

848
00:38:02,066 --> 00:38:04,736
of this action by the total
reward of its trajectory,

849
00:38:04,736 --> 00:38:07,320
let's look more
specifically at just the sum

850
00:38:07,320 --> 00:38:09,876
of rewards coming from this time step

851
00:38:09,876 --> 00:38:12,108
on to the end, right?

852
00:38:12,108 --> 00:38:14,224
And so, this is basically saying that

853
00:38:14,224 --> 00:38:17,441
how good an action is, is
only specified by how much

854
00:38:17,441 --> 00:38:18,999
future reward it generates.

855
00:38:18,999 --> 00:38:20,499
Which makes sense.

856
00:38:21,811 --> 00:38:24,251
Okay, so a second idea
that we can also use

857
00:38:24,251 --> 00:38:26,931
is using a discount factor in order

858
00:38:26,931 --> 00:38:29,448
to ignore delayed effects.

859
00:38:29,448 --> 00:38:33,133
Alright so here we've added
back in this discount factor,

860
00:38:33,133 --> 00:38:36,774
that we've seen before,
which is saying that

861
00:38:36,774 --> 00:38:39,991
we are, you know, our discount
factor's going to tell us

862
00:38:39,991 --> 00:38:41,841
how much we care about just the

863
00:38:41,841 --> 00:38:44,510
rewards that are coming up soon,

864
00:38:44,510 --> 00:38:47,276
versus rewards that came much later on.

865
00:38:47,276 --> 00:38:49,462
Right, so we were going to now

866
00:38:49,462 --> 00:38:51,438
say how good or bad an action is,

867
00:38:51,438 --> 00:38:54,071
looking more at the local neighborhood

868
00:38:54,071 --> 00:38:57,489
of action set it generates
in the immediate near future

869
00:38:57,489 --> 00:39:00,880
and down weighting the the
ones that come later on.

870
00:39:00,880 --> 00:39:02,471
Okay so

871
00:39:02,471 --> 00:39:05,194
these are some straightforward ideas

872
00:39:05,194 --> 00:39:07,730
that are generally used in practice.

873
00:39:07,730 --> 00:39:11,529
So, a third idea is this idea of using

874
00:39:11,529 --> 00:39:14,597
a baseline in order to
reduce your variance.

875
00:39:14,597 --> 00:39:18,273
And so, a problem with
just using the raw value

876
00:39:18,273 --> 00:39:20,690
of your trajectories, is that

877
00:39:21,675 --> 00:39:23,869
this isn't necessarily meaningful, right?

878
00:39:23,869 --> 00:39:26,653
So, for example, if your
rewards are all positive,

879
00:39:26,653 --> 00:39:27,973
then you're just going to keep pushing

880
00:39:27,973 --> 00:39:29,835
up the probabilities of all your actions.

881
00:39:29,835 --> 00:39:32,039
And of course, you'll push
them up to various degrees,

882
00:39:32,039 --> 00:39:35,448
but what's really important
is whether a reward is better

883
00:39:35,448 --> 00:39:39,753
or worse than what you're
expecting to be getting.

884
00:39:39,753 --> 00:39:42,993
Alright, so in order to
address this, we can introduce

885
00:39:42,993 --> 00:39:46,071
a baseline function that's
dependent on the state.

886
00:39:46,071 --> 00:39:47,598
Right, so this baseline function tell us

887
00:39:47,598 --> 00:39:51,219
what's, how much we, what's
our guess and what we expect

888
00:39:51,219 --> 00:39:53,886
to get from this state, and then

889
00:39:55,515 --> 00:39:58,031
our reward or our scaling
factor that we're going to use

890
00:39:58,031 --> 00:39:59,837
to be pushing up or
down our probabilities,

891
00:39:59,837 --> 00:40:02,592
can now just be our expected
sum of future rewards,

892
00:40:02,592 --> 00:40:05,508
minus this baseline, so now
it's the relative of how

893
00:40:05,508 --> 00:40:08,939
much better or worse is
the reward that we got

894
00:40:08,939 --> 00:40:10,772
from what we expected.

895
00:40:11,870 --> 00:40:14,971
And so how can we choose this baseline?

896
00:40:14,971 --> 00:40:16,099
Well,

897
00:40:16,099 --> 00:40:19,168
a very simple baseline, the
most simple you can use,

898
00:40:19,168 --> 00:40:21,065
is just taking a moving average

899
00:40:21,065 --> 00:40:23,013
of rewards that you've experienced so far.

900
00:40:23,013 --> 00:40:25,027
So you can even do this
overall trajectories,

901
00:40:25,027 --> 00:40:28,863
and this is just an
average of what rewards

902
00:40:28,863 --> 00:40:31,431
have I been seeing as I've been training,

903
00:40:31,431 --> 00:40:34,765
and as I've been playing these episodes?

904
00:40:34,765 --> 00:40:37,549
Right, and so this gives
some idea of whether the

905
00:40:37,549 --> 00:40:41,716
reward that I currently get
was relatively better or worse.

906
00:40:42,821 --> 00:40:45,737
And so there's some variance
on this that you can use

907
00:40:45,737 --> 00:40:49,215
but so far the variance
reductions that we've seen so far

908
00:40:49,215 --> 00:40:51,588
are all used in what's typically

909
00:40:51,588 --> 00:40:54,452
called "vanilla REINFORCE" algorithm.

910
00:40:54,452 --> 00:40:56,787
Right, so looking at the
cumulative future reward,

911
00:40:56,787 --> 00:41:00,954
having a discount factor,
and some simple baselines.

912
00:41:02,601 --> 00:41:05,081
Now let's talk about how we can

913
00:41:05,081 --> 00:41:06,547
think about this idea of baseline

914
00:41:06,547 --> 00:41:08,769
and potentially choose better baselines.

915
00:41:08,769 --> 00:41:12,084
Right, so if we're going to
think about what's a better

916
00:41:12,084 --> 00:41:13,567
baseline that we can choose,

917
00:41:13,567 --> 00:41:16,569
what we want to do is we want
to push up the probability

918
00:41:16,569 --> 00:41:19,931
of an action from a state,
if the action was better than

919
00:41:19,931 --> 00:41:24,255
the expected value of what we
should get from that state.

920
00:41:24,255 --> 00:41:27,655
So, thinking about the value
of what we're going to expect

921
00:41:27,655 --> 00:41:30,163
from the state, what
does this remind you of?

922
00:41:30,163 --> 00:41:31,189
Does this remind you of anything

923
00:41:31,189 --> 00:41:34,939
that we talked about
earlier in this lecture?

924
00:41:37,023 --> 00:41:37,856
Yes.

925
00:41:37,856 --> 00:41:39,266
[inaudible from audience]

926
00:41:39,266 --> 00:41:41,297
Yeah, so the value functions, right?

927
00:41:41,297 --> 00:41:45,201
The value functions that we
talked about with Q-learning.

928
00:41:45,201 --> 00:41:46,034
So, exactly.

929
00:41:46,034 --> 00:41:47,871
So Q-functions and value functions

930
00:41:47,871 --> 00:41:50,895
and so, the intuition is that

931
00:41:50,895 --> 00:41:52,347
well,

932
00:41:52,347 --> 00:41:54,704
we're happy with an action,

933
00:41:54,704 --> 00:41:58,173
taking an action in a state s, if

934
00:41:58,173 --> 00:42:00,248
our Q-value of taking

935
00:42:00,248 --> 00:42:04,752
a specific action from
this state is larger than

936
00:42:04,752 --> 00:42:06,999
the value function or expected value

937
00:42:06,999 --> 00:42:08,406
of the cumulative future reward

938
00:42:08,406 --> 00:42:09,698
that we can get from this state.

939
00:42:09,698 --> 00:42:11,842
Right, so this means that
this action was better than

940
00:42:11,842 --> 00:42:14,416
other actions that we could've taken.

941
00:42:14,416 --> 00:42:17,896
And on the contrary, we're
unhappy if this action,

942
00:42:17,896 --> 00:42:22,063
if this value or this
difference is negative or small.

943
00:42:23,917 --> 00:42:27,299
Right, so now if we plug
this in, in order to,

944
00:42:27,299 --> 00:42:29,269
as our scaling factor of how much we want

945
00:42:29,269 --> 00:42:32,692
to push up or down, our
probabilities of our actions,

946
00:42:32,692 --> 00:42:34,868
then we can get this estimator here.

947
00:42:34,868 --> 00:42:37,452
Right, so, it's going to be

948
00:42:37,452 --> 00:42:40,168
exactly the same as before, but now where

949
00:42:40,168 --> 00:42:43,993
we've had before our
cumulative expected reward,

950
00:42:43,993 --> 00:42:46,708
with our various reduction,
variance reduction

951
00:42:46,708 --> 00:42:50,514
techniques and baselines in,
here we can just plug in now

952
00:42:50,514 --> 00:42:53,297
this difference of how much better our

953
00:42:53,297 --> 00:42:57,113
current action was,
based on our Q-function

954
00:42:57,113 --> 00:43:00,530
minus our value function from that state.

955
00:43:01,771 --> 00:43:04,148
Right, but what we talked
about so far with our

956
00:43:04,148 --> 00:43:06,993
REINFORCE algorithm, we don't know

957
00:43:06,993 --> 00:43:09,413
what Q and V actually are.

958
00:43:09,413 --> 00:43:11,313
So can we learn these?

959
00:43:11,313 --> 00:43:14,479
And the answer is yes, using Q-learning.

960
00:43:14,479 --> 00:43:16,465
What we've already talked about before.

961
00:43:16,465 --> 00:43:19,828
So we can combine policy gradients

962
00:43:19,828 --> 00:43:22,210
while we've just been talking
about, with Q-learning,

963
00:43:22,210 --> 00:43:25,982
by training both an actor,
which is the policy,

964
00:43:25,982 --> 00:43:28,784
as well as a critic, right, a Q-function,

965
00:43:28,784 --> 00:43:32,366
which is going to tell us
how good we think a state is,

966
00:43:32,366 --> 00:43:34,380
and an action in a state.

967
00:43:34,380 --> 00:43:36,964
Right, so using this in approach,

968
00:43:36,964 --> 00:43:40,633
an actor is going to
decide which action to take

969
00:43:40,633 --> 00:43:43,716
and then the critic, or
Q-function, is going to tell

970
00:43:43,716 --> 00:43:47,708
the actor how good its action
was and how it should adjust.

971
00:43:47,708 --> 00:43:51,072
And so, and this also alleviates
a little bit of the task

972
00:43:51,072 --> 00:43:53,636
of this critic compared
to the Q-learning problems

973
00:43:53,636 --> 00:43:56,694
that we talked about earlier
of having to have this

974
00:43:56,694 --> 00:43:59,958
learning a Q-value for
every state, action pair,

975
00:43:59,958 --> 00:44:01,784
because here it only has to learn this

976
00:44:01,784 --> 00:44:04,762
for the state-action pairs that
are generated by the policy.

977
00:44:04,762 --> 00:44:06,103
It only needs to know this

978
00:44:06,103 --> 00:44:10,512
where it matters for
computing this scaling factor.

979
00:44:10,512 --> 00:44:12,830
Right, and then we can also,
as we're learning this,

980
00:44:12,830 --> 00:44:15,196
incorporate all of the
Q-learning tricks that we saw

981
00:44:15,196 --> 00:44:18,188
earlier, such as experience replay.

982
00:44:18,188 --> 00:44:20,972
And so, now I'm also going to just

983
00:44:20,972 --> 00:44:24,610
define this term that we saw earlier,

984
00:44:24,610 --> 00:44:28,248
Q of s of a, how much,
how good was an action

985
00:44:28,248 --> 00:44:30,831
in a given state, minus V of s?

986
00:44:32,199 --> 00:44:35,533
Our expected value of
how good the state is

987
00:44:35,533 --> 00:44:38,172
by this term advantage function.

988
00:44:38,172 --> 00:44:41,498
Right, so the advantage
function is how much advantage

989
00:44:41,498 --> 00:44:43,568
did we get from playing this action?

990
00:44:43,568 --> 00:44:48,100
How much better the
action was than expected.

991
00:44:48,100 --> 00:44:51,709
So, using this, we can
put together our full

992
00:44:51,709 --> 00:44:53,457
actor-critic algorithm.

993
00:44:53,457 --> 00:44:56,279
And so what this looks like,
is that we're going to start

994
00:44:56,279 --> 00:45:00,326
off with by initializing
our policy parameters theta,

995
00:45:00,326 --> 00:45:03,689
and our critic parameters
that we'll call phi.

996
00:45:03,689 --> 00:45:07,522
And then for each, for
iterations of training,

997
00:45:08,401 --> 00:45:11,149
we're going to sample M trajectories,

998
00:45:11,149 --> 00:45:12,185
under the current policy.

999
00:45:12,185 --> 00:45:13,734
Right, we're going to play
our policy and get these

1000
00:45:13,734 --> 00:45:18,725
trajectories as s-zero, a-zero,
r-zero, s-one and so on.

1001
00:45:18,725 --> 00:45:20,359
Okay, and then we're going to compute

1002
00:45:20,359 --> 00:45:21,671
the gradients that we want.

1003
00:45:21,671 --> 00:45:24,977
Right, so for each of these trajectories

1004
00:45:24,977 --> 00:45:26,017
and in each time step, we're going

1005
00:45:26,017 --> 00:45:28,901
to compute this advantage function,

1006
00:45:28,901 --> 00:45:30,818
and then we're going to

1007
00:45:31,701 --> 00:45:33,465
use this advantage function, right?

1008
00:45:33,465 --> 00:45:37,131
And then we're going to use
that in our gradient estimator

1009
00:45:37,131 --> 00:45:40,533
that we showed earlier, and accumulate our

1010
00:45:40,533 --> 00:45:42,894
gradient estimate that we have for here.

1011
00:45:42,894 --> 00:45:46,017
And then we're also going to train our

1012
00:45:46,017 --> 00:45:50,837
critic parameters phi
by exactly the same way,

1013
00:45:50,837 --> 00:45:54,193
so as we saw earlier,
basically trying to enforce

1014
00:45:54,193 --> 00:45:57,557
this value function, right,
to learn our value function,

1015
00:45:57,557 --> 00:46:01,640
which is going to be pulled
into, just minimizing

1016
00:46:02,638 --> 00:46:05,467
this advantage function and this will

1017
00:46:05,467 --> 00:46:08,324
encourage it to be closer
to this Bellman equation

1018
00:46:08,324 --> 00:46:10,347
that we saw earlier, right?

1019
00:46:10,347 --> 00:46:14,347
And so, this is basically
just iterating between

1020
00:46:15,197 --> 00:46:17,733
learning and optimizing
our policy function,

1021
00:46:17,733 --> 00:46:20,211
as well as our critic function.

1022
00:46:20,211 --> 00:46:22,311
And so then we're going to update the

1023
00:46:22,311 --> 00:46:23,977
gradients and then we're
going to go through and just

1024
00:46:23,977 --> 00:46:26,727
continuously repeat this process.

1025
00:46:29,271 --> 00:46:31,827
Okay, so now let's look at
some examples of REINFORCE

1026
00:46:31,827 --> 00:46:36,027
in action, and let's look
first here at something called

1027
00:46:36,027 --> 00:46:39,464
the Recurrent Attention Model,
which is something that,

1028
00:46:39,464 --> 00:46:42,805
which is a model also
referred to as hard attention,

1029
00:46:42,805 --> 00:46:46,876
but you'll see a lot in,
recently, in computer vision

1030
00:46:46,876 --> 00:46:49,146
tasks for various purposes.

1031
00:46:49,146 --> 00:46:51,806
Right, and so the idea behind this is

1032
00:46:51,806 --> 00:46:55,122
here, I've talked about the
original work on hard attention,

1033
00:46:55,122 --> 00:46:59,167
which is on image
classification, and your goal is

1034
00:46:59,167 --> 00:47:02,504
to still predict the image class,

1035
00:47:02,504 --> 00:47:04,822
but now you're going to do
this by taking a sequence

1036
00:47:04,822 --> 00:47:06,494
of glimpses around the image.

1037
00:47:06,494 --> 00:47:10,300
You're going to look at local
regions around the image

1038
00:47:10,300 --> 00:47:12,754
and you're basically going
to selectively focus on these

1039
00:47:12,754 --> 00:47:17,141
parts and build up information
as you're looking around.

1040
00:47:17,141 --> 00:47:19,382
Right, and so the reason
that we want to do this

1041
00:47:19,382 --> 00:47:21,638
is, well, first of all it
has some nice inspiration

1042
00:47:21,638 --> 00:47:24,551
from human perception in eye movement.

1043
00:47:24,551 --> 00:47:26,869
Let's say we're looking at a complex image

1044
00:47:26,869 --> 00:47:29,225
and we want to determine
what's in the image.

1045
00:47:29,225 --> 00:47:31,594
Well, you know, we might,
maybe look at a low-resolution

1046
00:47:31,594 --> 00:47:34,013
of it first, and then
look specifically at parts

1047
00:47:34,013 --> 00:47:36,913
of the image that will give us clues about

1048
00:47:36,913 --> 00:47:39,168
what's in this image.

1049
00:47:39,168 --> 00:47:40,001
And then,

1050
00:47:41,160 --> 00:47:45,703
this approach of just looking
at, looking around at an image

1051
00:47:45,703 --> 00:47:48,533
at local regions, is also
going to help you save

1052
00:47:48,533 --> 00:47:50,435
computational resources, right?

1053
00:47:50,435 --> 00:47:53,293
You don't need to process the full image.

1054
00:47:53,293 --> 00:47:55,366
In practice, what usually
happens is you look at a

1055
00:47:55,366 --> 00:47:57,511
low-resolution image
first, of a full image,

1056
00:47:57,511 --> 00:48:01,678
to decide how to get started,
and then you look at high-res

1057
00:48:02,773 --> 00:48:04,671
portions of the image after that.

1058
00:48:04,671 --> 00:48:06,979
And so this saves a lot
of computational resources

1059
00:48:06,979 --> 00:48:09,725
and you can think about,
then, benefits of this

1060
00:48:09,725 --> 00:48:11,927
to scalability, right,
being able to, let's say

1061
00:48:11,927 --> 00:48:15,177
process larger images more efficiently.

1062
00:48:16,164 --> 00:48:17,780
And then, finally, this
could also actually help

1063
00:48:17,780 --> 00:48:20,099
with actual classification performance,

1064
00:48:20,099 --> 00:48:21,855
because now you're able to

1065
00:48:21,855 --> 00:48:24,760
ignore clutter and irrelevant
parts of the image.

1066
00:48:24,760 --> 00:48:25,593
Right?

1067
00:48:25,593 --> 00:48:27,678
Like, you know, instead
of always putting through

1068
00:48:27,678 --> 00:48:29,931
your ConvNet, all the parts of your image,

1069
00:48:29,931 --> 00:48:32,846
you can use this to, maybe,
first prune out what are the

1070
00:48:32,846 --> 00:48:34,936
relevant parts that I
actually want to process,

1071
00:48:34,936 --> 00:48:36,353
using my ConvNet.

1072
00:48:37,237 --> 00:48:39,849
Okay, so what's the reinforcement learning

1073
00:48:39,849 --> 00:48:41,531
formulation of this problem?

1074
00:48:41,531 --> 00:48:44,711
Well, our state is going to be

1075
00:48:44,711 --> 00:48:46,889
the glimpses that we've
seen so far, right?

1076
00:48:46,889 --> 00:48:47,722
Our

1077
00:48:48,881 --> 00:48:51,117
what's the information that we've seen?

1078
00:48:51,117 --> 00:48:53,643
Our action is then going to be where

1079
00:48:53,643 --> 00:48:55,228
to look next in the image.

1080
00:48:55,228 --> 00:48:57,090
Right, so in practice,
this can be something like

1081
00:48:57,090 --> 00:48:59,113
the x, y-coordinates,
maybe centered around some

1082
00:48:59,113 --> 00:49:02,842
fixed-sized glimpse that
you want to look at next.

1083
00:49:02,842 --> 00:49:05,664
And then the reward for
the classification problem

1084
00:49:05,664 --> 00:49:08,256
is going to be one, at
the final time step,

1085
00:49:08,256 --> 00:49:12,423
if our image is correctly
classified, and zero otherwise.

1086
00:49:14,495 --> 00:49:16,162
And so, because this

1087
00:49:17,373 --> 00:49:20,016
glimpsing, taking these
glimpses around the image

1088
00:49:20,016 --> 00:49:21,932
is a non-differentiable operation,

1089
00:49:21,932 --> 00:49:24,006
this is why we need to use

1090
00:49:24,006 --> 00:49:25,761
reinforcement learning formulation,

1091
00:49:25,761 --> 00:49:29,088
and learn policies for how
to take these glimpse actions

1092
00:49:29,088 --> 00:49:31,792
and we can train this using REINFORCE.

1093
00:49:31,792 --> 00:49:35,105
So, given the state of glimpses so far,

1094
00:49:35,105 --> 00:49:37,537
the core of our model is going to be

1095
00:49:37,537 --> 00:49:40,891
this RNN that we're going
to use to model the state,

1096
00:49:40,891 --> 00:49:44,501
and then we're going to
use our policy parameters

1097
00:49:44,501 --> 00:49:47,418
in order to output the next action.

1098
00:49:49,354 --> 00:49:53,248
Okay, so what this model looks
like is we're going to take

1099
00:49:53,248 --> 00:49:54,571
an input image.

1100
00:49:54,571 --> 00:49:57,655
Right, and then we're going to
take a glimpse at this image.

1101
00:49:57,655 --> 00:50:00,068
So here, this glimpse is the red box here,

1102
00:50:00,068 --> 00:50:03,184
and this is all blank, zeroes.

1103
00:50:03,184 --> 00:50:06,966
And so we'll pass what
we see so far into some

1104
00:50:06,966 --> 00:50:09,388
neural network, and this can be any

1105
00:50:09,388 --> 00:50:12,193
kind of network depending on your task.

1106
00:50:12,193 --> 00:50:14,276
In the original experiments
that I'm showing here,

1107
00:50:14,276 --> 00:50:16,138
on MNIST, this is very
simple, so you can just

1108
00:50:16,138 --> 00:50:18,758
use a couple of small,
fully-connected layers,

1109
00:50:18,758 --> 00:50:21,724
but you can imagine
for more complex images

1110
00:50:21,724 --> 00:50:26,105
and other tasks you may want
to use fancier ConvNets.

1111
00:50:26,105 --> 00:50:28,775
Right, so you've passed this
into some neural network,

1112
00:50:28,775 --> 00:50:31,065
and then, remember I said
we're also going to be

1113
00:50:31,065 --> 00:50:34,102
integrating our state of,
glimpses that we've seen

1114
00:50:34,102 --> 00:50:36,115
so far, using a recurrent network.

1115
00:50:36,115 --> 00:50:38,057
So, I'm just going to

1116
00:50:38,057 --> 00:50:40,265
we'll see that later on, but
this is going to go through that,

1117
00:50:40,265 --> 00:50:42,646
and then it's going to output my

1118
00:50:42,646 --> 00:50:46,094
x, y-coordinates, of where
I'm going to see next.

1119
00:50:46,094 --> 00:50:48,435
And in practice, this is going to be

1120
00:50:48,435 --> 00:50:50,766
We want to output a
distribution over actions,

1121
00:50:50,766 --> 00:50:53,385
right, and so, what this is
going to be it's going to be

1122
00:50:53,385 --> 00:50:57,282
a gaussian distribution and
we're going to output the mean.

1123
00:50:57,282 --> 00:50:59,084
You can also output a mean and variance

1124
00:50:59,084 --> 00:51:00,545
of this distribution in practice.

1125
00:51:00,545 --> 00:51:03,944
The variance can also be fixed.

1126
00:51:03,944 --> 00:51:07,172
Okay, so we're going to take this

1127
00:51:07,172 --> 00:51:08,496
action that we're now going to sample

1128
00:51:08,496 --> 00:51:11,854
a specific x, y location
from our action distribution

1129
00:51:11,854 --> 00:51:15,457
and then we're going to put
this in to get the next,

1130
00:51:15,457 --> 00:51:17,777
extract the next glimpse from our image.

1131
00:51:17,777 --> 00:51:19,297
Right, so here we've moved

1132
00:51:19,297 --> 00:51:23,385
to the end of the two,
this tail part of the two.

1133
00:51:23,385 --> 00:51:25,465
And so now we're actually
starting to get some signal

1134
00:51:25,465 --> 00:51:26,745
of what we want to see, right?

1135
00:51:26,745 --> 00:51:29,065
Like, what we want to do is we
want to look at the relevant

1136
00:51:29,065 --> 00:51:32,724
parts of the image that are
useful for classification.

1137
00:51:32,724 --> 00:51:35,354
So we pass this through, again,
our neural network layers,

1138
00:51:35,354 --> 00:51:37,104
and then also through

1139
00:51:38,153 --> 00:51:40,362
our recurrent network, right,
that's taking this input

1140
00:51:40,362 --> 00:51:43,642
as well as this previous hidden
state, and we're going to

1141
00:51:43,642 --> 00:51:45,524
use this to get a,

1142
00:51:45,524 --> 00:51:47,343
so this is representing our policy,

1143
00:51:47,343 --> 00:51:49,565
and then we're going to use this to output

1144
00:51:49,565 --> 00:51:51,354
our distribution for the next

1145
00:51:51,354 --> 00:51:54,095
location that we want to glimpse at.

1146
00:51:54,095 --> 00:51:55,874
So we can continue doing this,

1147
00:51:55,874 --> 00:51:57,303
you can see in this next glimpse here,

1148
00:51:57,303 --> 00:51:59,903
we've moved a little bit more
toward the center of the two.

1149
00:51:59,903 --> 00:52:01,723
Alright, so it's probably learning that,

1150
00:52:01,723 --> 00:52:05,005
you know, once I've seen
this tail part of the two,

1151
00:52:05,005 --> 00:52:08,093
that looks like this,
maybe moving in this upper

1152
00:52:08,093 --> 00:52:10,794
left-hand direction will
get you more towards

1153
00:52:10,794 --> 00:52:12,631
a center, which will also have a value,

1154
00:52:12,631 --> 00:52:14,543
valuable information.

1155
00:52:14,543 --> 00:52:17,473
And then we can keep doing this.

1156
00:52:17,473 --> 00:52:20,612
And then finally, at the
end, at our last time step,

1157
00:52:20,612 --> 00:52:23,412
so we can have a fixed
number of time steps here,

1158
00:52:23,412 --> 00:52:26,795
in practice something like six or eight.

1159
00:52:26,795 --> 00:52:29,359
And then at the final time
step, since we want to do

1160
00:52:29,359 --> 00:52:33,350
classification, we'll have our standard

1161
00:52:33,350 --> 00:52:36,100
Softmax layer that will produce a

1162
00:52:37,376 --> 00:52:39,363
distribution of
probabilities for each class.

1163
00:52:39,363 --> 00:52:42,111
And then here the max class was a two,

1164
00:52:42,111 --> 00:52:44,108
so we can predict that this was a two.

1165
00:52:44,108 --> 00:52:46,558
Right, and so this is going
to be the set up of our

1166
00:52:46,558 --> 00:52:50,428
model and our policy, and then we have our

1167
00:52:50,428 --> 00:52:53,079
estimate for the gradient
of this policy that we've

1168
00:52:53,079 --> 00:52:54,420
said earlier we could compute by taking

1169
00:52:54,420 --> 00:52:56,695
trajectories from here

1170
00:52:56,695 --> 00:52:59,569
and using those to do back prop.

1171
00:52:59,569 --> 00:53:02,819
And so we can just do this
in order to train this model

1172
00:53:02,819 --> 00:53:05,281
and learn the parameters
of our policy, right?

1173
00:53:05,281 --> 00:53:08,698
All of the weights that you can see here.

1174
00:53:09,953 --> 00:53:10,786
Okay, so

1175
00:53:12,239 --> 00:53:14,270
here's an example of a

1176
00:53:14,270 --> 00:53:16,710
policies trained on MNIST,

1177
00:53:16,710 --> 00:53:19,016
and so you can see that, in general,

1178
00:53:19,016 --> 00:53:20,808
from wherever it's
starting, usually learns

1179
00:53:20,808 --> 00:53:22,942
to go closer to where the digit is,

1180
00:53:22,942 --> 00:53:25,260
and then looking at the relevant
parts of the digit, right?

1181
00:53:25,260 --> 00:53:27,685
So this is pretty cool and

1182
00:53:27,685 --> 00:53:28,744
this

1183
00:53:28,744 --> 00:53:30,460
you know, follows kind of
what you would expect, right,

1184
00:53:30,460 --> 00:53:31,627
if you were to

1185
00:53:33,335 --> 00:53:34,967
choose places to look next

1186
00:53:34,967 --> 00:53:38,186
in order to most efficiently determine

1187
00:53:38,186 --> 00:53:40,108
what digit this is.

1188
00:53:40,108 --> 00:53:43,491
Right, and so this idea of hard attention,

1189
00:53:43,491 --> 00:53:45,862
of recurrent attention
models, has also been used

1190
00:53:45,862 --> 00:53:49,758
in a lot of tasks in
computer vision in the last

1191
00:53:49,758 --> 00:53:52,687
couple of years, so you'll
see this, used, for example,

1192
00:53:52,687 --> 00:53:54,790
fine-grained image recognition.

1193
00:53:54,790 --> 00:53:57,869
So, I mentioned earlier that

1194
00:53:57,869 --> 00:54:00,596
one of the useful benefits of this

1195
00:54:00,596 --> 00:54:01,763
can be also to

1196
00:54:02,975 --> 00:54:05,198
both save on computational efficiency

1197
00:54:05,198 --> 00:54:08,009
as well as to ignore
clutter and irrelevant

1198
00:54:08,009 --> 00:54:10,180
parts of the image, and
when you have fine-grained

1199
00:54:10,180 --> 00:54:11,750
image classification problems,

1200
00:54:11,750 --> 00:54:13,092
you usually want both of these.

1201
00:54:13,092 --> 00:54:17,307
You want to keep high-resolution,
so that you can look

1202
00:54:17,307 --> 00:54:19,447
at, you know, important differences.

1203
00:54:19,447 --> 00:54:23,327
And then you also want to
focus on these differences

1204
00:54:23,327 --> 00:54:25,777
and ignore irrelevant parts.

1205
00:54:25,777 --> 00:54:27,359
Yeah, question.

1206
00:54:27,359 --> 00:54:31,526
[inaudible question from audience]

1207
00:54:35,061 --> 00:54:36,789
Okay, so yeah, so the question is

1208
00:54:36,789 --> 00:54:39,061
how is there is
computational efficiency,

1209
00:54:39,061 --> 00:54:41,482
because we also have this
recurrent neural network in place.

1210
00:54:41,482 --> 00:54:45,842
So that's true, it depends
on exactly what's your,

1211
00:54:45,842 --> 00:54:47,761
what is your problem, what
is your network, and so on,

1212
00:54:47,761 --> 00:54:50,151
but you can imagine that
if you had some really

1213
00:54:50,151 --> 00:54:52,512
hi- resolution image

1214
00:54:52,512 --> 00:54:54,773
and you don't want to process
the entire parts of this

1215
00:54:54,773 --> 00:54:58,477
image with some huge ConvNet
or some huge, you know,

1216
00:54:58,477 --> 00:55:01,900
network, now you can
get some savings by just

1217
00:55:01,900 --> 00:55:04,669
focusing on specific
smaller parts of the image.

1218
00:55:04,669 --> 00:55:06,589
You only process those parts of the image.

1219
00:55:06,589 --> 00:55:08,507
But, you're right, that
it depends on exactly

1220
00:55:08,507 --> 00:55:10,924
what problem set-up you have.

1221
00:55:12,210 --> 00:55:14,530
This has also been used
in image captioning,

1222
00:55:14,530 --> 00:55:17,138
so if we're going to produce
an caption for an image,

1223
00:55:17,138 --> 00:55:20,421
we can choose, you know,
we can have the image

1224
00:55:20,421 --> 00:55:23,197
use this attention model
to generate this caption

1225
00:55:23,197 --> 00:55:26,120
and what it usually ends up
learning is these policies

1226
00:55:26,120 --> 00:55:28,999
where it'll focus on
specific parts of the image,

1227
00:55:28,999 --> 00:55:31,850
in sequence, and as it
focuses on each part,

1228
00:55:31,850 --> 00:55:34,629
it'll generate some words
or the part of the caption

1229
00:55:34,629 --> 00:55:38,341
referring to that part of the image.

1230
00:55:38,341 --> 00:55:40,170
And then it's also been used,

1231
00:55:40,170 --> 00:55:42,948
also tasks such as visual
question answering,

1232
00:55:42,948 --> 00:55:45,509
where we ask a question about the image

1233
00:55:45,509 --> 00:55:48,981
and you want the model
to output some answer

1234
00:55:48,981 --> 00:55:51,786
to your question, for
example, I don't know,

1235
00:55:51,786 --> 00:55:53,840
how many chairs are around the table?

1236
00:55:53,840 --> 00:55:58,229
And so you can see how
this attention mechanism

1237
00:55:58,229 --> 00:55:59,457
might be a good type of model

1238
00:55:59,457 --> 00:56:03,040
for learning how to
answer these questions.

1239
00:56:05,707 --> 00:56:08,475
Okay, so that was an
example of policy gradients

1240
00:56:08,475 --> 00:56:10,564
in these hard attention models.

1241
00:56:10,564 --> 00:56:13,524
And so, now I'm going to
talk about one more example,

1242
00:56:13,524 --> 00:56:16,430
that also uses policy gradients,

1243
00:56:16,430 --> 00:56:18,465
which is learning how to play Go.

1244
00:56:18,465 --> 00:56:22,006
Right, so DeepMind had this agent

1245
00:56:22,006 --> 00:56:24,497
for playing Go, called AlphGo,

1246
00:56:24,497 --> 00:56:27,297
that's been in the news a lot

1247
00:56:27,297 --> 00:56:30,708
in the past, last year and this year.

1248
00:56:30,708 --> 00:56:31,541
So, sorry?

1249
00:56:31,541 --> 00:56:32,374
[inaudible comment from audience]

1250
00:56:32,374 --> 00:56:35,291
And yesterday, yes, that's correct.

1251
00:56:36,172 --> 00:56:39,258
So this is very exciting,
recent news as well.

1252
00:56:39,258 --> 00:56:40,987
So last year,

1253
00:56:40,987 --> 00:56:43,234
a first version of AlphaGo

1254
00:56:43,234 --> 00:56:44,817
was put into a

1255
00:56:46,678 --> 00:56:49,539
competition against one
of the best Go players

1256
00:56:49,539 --> 00:56:52,609
of recent years, Lee Sedol, and the agent

1257
00:56:52,609 --> 00:56:54,927
was able to beat him

1258
00:56:54,927 --> 00:56:57,886
four to one, in a game of five matches.

1259
00:56:57,886 --> 00:57:00,541
And actually, right now, just

1260
00:57:00,541 --> 00:57:03,788
there's another match with
Ke Jie, which is current

1261
00:57:03,788 --> 00:57:07,855
world number one, and
so it's best of three

1262
00:57:07,855 --> 00:57:09,348
in China right now.

1263
00:57:09,348 --> 00:57:12,236
And so the first game was yesterday.

1264
00:57:12,236 --> 00:57:13,436
AlphaGo won.

1265
00:57:13,436 --> 00:57:16,596
I think it was by just
half a point, and so,

1266
00:57:16,596 --> 00:57:18,606
so there's two more games to watch.

1267
00:57:18,606 --> 00:57:20,657
These are all live-stream, so

1268
00:57:20,657 --> 00:57:24,276
you guys, should also go
online and watch these games.

1269
00:57:24,276 --> 00:57:28,193
It's pretty interesting
to hear the commentary.

1270
00:57:29,225 --> 00:57:32,577
But, so what is this AlphaGo
agent, right, from DeepMind?

1271
00:57:32,577 --> 00:57:34,868
And it's based on a lot
of what we've talked

1272
00:57:34,868 --> 00:57:36,466
about so far in this lecture.

1273
00:57:36,466 --> 00:57:39,687
And what it is it's a mixed
of supervised learning

1274
00:57:39,687 --> 00:57:42,045
and reinforcement learning,

1275
00:57:42,045 --> 00:57:44,657
as well as a mix of some older

1276
00:57:44,657 --> 00:57:48,573
methods for Go, Monte Carlo Tree Search,

1277
00:57:48,573 --> 00:57:51,656
as well as recent deep RL approaches.

1278
00:57:52,579 --> 00:57:56,986
So, okay, so how does AlphaGo
beat the Go world champion?

1279
00:57:56,986 --> 00:57:59,363
Well, what it first does is

1280
00:57:59,363 --> 00:58:02,449
to train AlphaGo, what it
takes as input is going to be

1281
00:58:02,449 --> 00:58:04,089
a few featurization of the board.

1282
00:58:04,089 --> 00:58:06,470
So it's basically, right,
your board and the positions

1283
00:58:06,470 --> 00:58:08,739
of the pieces on the board.

1284
00:58:08,739 --> 00:58:10,739
That's your natural state representation.

1285
00:58:10,739 --> 00:58:13,819
And what they do in order
to improve performance

1286
00:58:13,819 --> 00:58:16,638
a little bit is that
they featurize this into

1287
00:58:16,638 --> 00:58:17,958
some

1288
00:58:17,958 --> 00:58:20,510
more channels of one is all
the different stone colors,

1289
00:58:20,510 --> 00:58:21,956
so this is kind of like your

1290
00:58:21,956 --> 00:58:23,790
configuration of your board.

1291
00:58:23,790 --> 00:58:27,270
Also some channels, for
example, where, which moves

1292
00:58:27,270 --> 00:58:31,138
are legal, some bias
channels, some various things

1293
00:58:31,138 --> 00:58:33,125
and then, given this state, right,

1294
00:58:33,125 --> 00:58:35,117
it's going to first

1295
00:58:35,117 --> 00:58:36,518
train a network

1296
00:58:36,518 --> 00:58:38,867
that's initialized with
supervised training

1297
00:58:38,867 --> 00:58:40,897
from professional Go games.

1298
00:58:40,897 --> 00:58:43,147
So, given the current board configuration

1299
00:58:43,147 --> 00:58:45,627
or features, featurization of this,

1300
00:58:45,627 --> 00:58:48,495
what's the correct next action to take?

1301
00:58:48,495 --> 00:58:50,678
Alright, so given

1302
00:58:50,678 --> 00:58:52,787
examples of professional games played,

1303
00:58:52,787 --> 00:58:55,608
you know, just collected over time,

1304
00:58:55,608 --> 00:58:57,635
we can just take all of
these professional Go moves,

1305
00:58:57,635 --> 00:58:59,815
train a standard, supervised mapping,

1306
00:58:59,815 --> 00:59:02,605
from board state to action to take.

1307
00:59:02,605 --> 00:59:05,365
Alright, so they take this,
which is a pretty good start,

1308
00:59:05,365 --> 00:59:07,637
and then they're going
to use this to initialize

1309
00:59:07,637 --> 00:59:09,227
a policy network.

1310
00:59:09,227 --> 00:59:10,844
Right, so policy network,
it's just going to take

1311
00:59:10,844 --> 00:59:14,985
the exact same structure of input is your

1312
00:59:14,985 --> 00:59:16,389
board state and your output is the

1313
00:59:16,389 --> 00:59:17,778
actions that you're going to take.

1314
00:59:17,778 --> 00:59:20,887
And this was the set-up
for the policy gradients

1315
00:59:20,887 --> 00:59:21,978
that we just saw, right?

1316
00:59:21,978 --> 00:59:25,156
So now we're going to just
continue training this

1317
00:59:25,156 --> 00:59:27,130
using policy gradients.

1318
00:59:27,130 --> 00:59:30,831
And it's going to do this
reinforcement learning training

1319
00:59:30,831 --> 00:59:35,123
by playing against itself for
random, previous iterations.

1320
00:59:35,123 --> 00:59:37,384
So self play, and the
reward it's going to get

1321
00:59:37,384 --> 00:59:42,243
is one, if it wins, and a
negative one if it loses.

1322
00:59:42,243 --> 00:59:44,624
And what we're also going to
do is we're also going to learn

1323
00:59:44,624 --> 00:59:47,573
a value network, so,
something like a critic.

1324
00:59:47,573 --> 00:59:51,235
And then, the final AlphaGo
is going to be combining

1325
00:59:51,235 --> 00:59:53,982
all of these together, so
policy and value networks

1326
00:59:53,982 --> 00:59:56,075
as well as with

1327
00:59:56,075 --> 00:59:59,043
a Monte Carlo Tree Search
algorithm, in order to select

1328
00:59:59,043 --> 01:00:01,475
actions by look ahead search.

1329
01:00:01,475 --> 01:00:04,743
Right, so after putting all this together,

1330
01:00:04,743 --> 01:00:08,853
a value of a node, of
where you are in play,

1331
01:00:08,853 --> 01:00:11,590
and what you do next, is
going to be a combination

1332
01:00:11,590 --> 01:00:13,811
of your value function, as well as

1333
01:00:13,811 --> 01:00:16,552
roll at outcome that you're
computing from standard

1334
01:00:16,552 --> 01:00:19,891
Monte Carlo Tree Search roll outs.

1335
01:00:19,891 --> 01:00:22,891
Okay, so, yeah, so this is basically

1336
01:00:24,203 --> 01:00:27,453
the various, the components of AlphaGo.

1337
01:00:28,397 --> 01:00:30,314
If you're interested in
reading more about this,

1338
01:00:30,314 --> 01:00:33,814
there's a nature paper about this in 2016,

1339
01:00:34,664 --> 01:00:37,656
and they trained this, I think, over,

1340
01:00:37,656 --> 01:00:40,765
the version of AlphaGo
that's being used in these

1341
01:00:40,765 --> 01:00:45,016
matches is, like, I think
a couple thousand CPUs

1342
01:00:45,016 --> 01:00:47,953
plus a couple hundred GPUs,
putting all of this together,

1343
01:00:47,953 --> 01:00:52,120
so it's a huge amount of
training that's going on, right.

1344
01:00:55,659 --> 01:00:57,514
And yeah, so you guys should,

1345
01:00:57,514 --> 01:00:59,681
follow the game this week.

1346
01:01:01,643 --> 01:01:03,491
It's pretty exciting.

1347
01:01:03,491 --> 01:01:07,524
Okay, so in summary,
today we've talked about

1348
01:01:07,524 --> 01:01:10,858
policy gradients, right,
which are general.

1349
01:01:10,858 --> 01:01:13,025
They, you're just directly

1350
01:01:14,456 --> 01:01:18,855
taking gradient descent or
ascent on your policy parameters,

1351
01:01:18,855 --> 01:01:21,942
so this works well for a
large class of problems,

1352
01:01:21,942 --> 01:01:23,947
but it also suffers from high variance,

1353
01:01:23,947 --> 01:01:25,938
so it requires a lot of samples,

1354
01:01:25,938 --> 01:01:28,669
and your challenge here
is sample efficiency.

1355
01:01:28,669 --> 01:01:32,608
We also talked about
Q-learning, which doesn't always

1356
01:01:32,608 --> 01:01:35,349
work, it's harder to
sometimes get it to work

1357
01:01:35,349 --> 01:01:37,536
because of this problem
that we talked earlier where

1358
01:01:37,536 --> 01:01:39,702
you are trying to compute this

1359
01:01:39,702 --> 01:01:42,285
exact state, action value

1360
01:01:43,324 --> 01:01:47,769
for many, for very high
dimensions, but when it does work,

1361
01:01:47,769 --> 01:01:51,342
for problems, for example,
the Atari we saw earlier,

1362
01:01:51,342 --> 01:01:53,092
then it's usually more sample efficient

1363
01:01:53,092 --> 01:01:54,322
than policy gradients.

1364
01:01:54,322 --> 01:01:56,540
Right, and one of the
challenges in Q-learning is that

1365
01:01:56,540 --> 01:01:57,484
you want to make sure that you're

1366
01:01:57,484 --> 01:01:59,902
doing sufficient exploration.

1367
01:01:59,902 --> 01:02:00,735
Yeah?

1368
01:02:00,735 --> 01:02:04,902
[inaudible question from audience]

1369
01:02:14,313 --> 01:02:17,764
Oh, so for Q-learning can
you do this process where

1370
01:02:17,764 --> 01:02:20,684
you're, okay, where you're
trying to start this off by

1371
01:02:20,684 --> 01:02:21,753
some supervised training?

1372
01:02:21,753 --> 01:02:24,924
So, I guess the direct
approach for Q-learning doesn't

1373
01:02:24,924 --> 01:02:27,532
do that because you're
trying to regress to these

1374
01:02:27,532 --> 01:02:29,972
Q-values, right, instead of
policy gradients over this

1375
01:02:29,972 --> 01:02:32,732
distribution, but I think there
are ways in which you can,

1376
01:02:32,732 --> 01:02:34,232
like, massage this

1377
01:02:35,938 --> 01:02:37,985
type of thing to also bootstrap.

1378
01:02:37,985 --> 01:02:40,393
Because I think bootstrapping
in general or like

1379
01:02:40,393 --> 01:02:43,854
behavior cloning is a good way to

1380
01:02:43,854 --> 01:02:46,021
warm start these policies.

1381
01:02:47,454 --> 01:02:50,284
Okay, so, right, so we've
talked about policy gradients

1382
01:02:50,284 --> 01:02:54,213
and Q-learning, and just
another look at some of these,

1383
01:02:54,213 --> 01:02:55,413
some of the guarantees that you have,

1384
01:02:55,413 --> 01:02:56,752
right, with policy gradients.

1385
01:02:56,752 --> 01:02:58,622
One thing we do know
that's really nice is that

1386
01:02:58,622 --> 01:03:02,789
this will always converge to
a local minimum of J of theta,

1387
01:03:04,339 --> 01:03:06,592
because we're just directly
doing gradient ascent,

1388
01:03:06,592 --> 01:03:09,043
and so this is often,

1389
01:03:09,043 --> 01:03:12,131
and this local minimum is
often just pretty good, right.

1390
01:03:12,131 --> 01:03:14,931
And in Q-learning, on the
other hand, we don't have any

1391
01:03:14,931 --> 01:03:17,041
guarantees because here
we're trying to approximate

1392
01:03:17,041 --> 01:03:20,277
this Bellman equation with
a complicated function

1393
01:03:20,277 --> 01:03:23,358
approximator and so, in this
case, this is the problem

1394
01:03:23,358 --> 01:03:25,787
with Q-learning being a
little bit trickier to train

1395
01:03:25,787 --> 01:03:29,954
in terms of applicability
to a wide range of problems.

1396
01:03:31,849 --> 01:03:34,737
Alright, so today you got basically very,

1397
01:03:34,737 --> 01:03:37,907
brief, kind of high-level
overview of reinforcement learning

1398
01:03:37,907 --> 01:03:41,546
and some major classes
of algorithms in RL.

1399
01:03:41,546 --> 01:03:44,419
And next time we're going to have a

1400
01:03:44,419 --> 01:03:47,577
guest lecturer from, Song
Han, who's done a lot

1401
01:03:47,577 --> 01:03:51,276
of pioneering work in model compression

1402
01:03:51,276 --> 01:03:52,569
and energy efficient deep learning,

1403
01:03:52,569 --> 01:03:56,459
and so he's going to talk some
of this, about some of this.

1404
01:03:56,459 --> 00:00:00,000
Thank you.

