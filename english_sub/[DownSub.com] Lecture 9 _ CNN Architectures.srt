1
00:00:15,562 --> 00:00:18,141
- All right welcome to lecture nine.

2
00:00:18,141 --> 00:00:22,506
So today we will be talking
about CNN Architectures.

3
00:00:22,506 --> 00:00:23,899
And just a few administrative points

4
00:00:23,899 --> 00:00:28,516
before we get started,
assignment two is due Thursday.

5
00:00:28,516 --> 00:00:31,782
The mid term will be in
class on Tuesday May ninth,

6
00:00:31,782 --> 00:00:35,091
so next week and it will
cover material through Tuesday

7
00:00:35,091 --> 00:00:37,665
through this coming Thursday May fourth.

8
00:00:37,665 --> 00:00:40,077
So everything up to
recurrent neural networks

9
00:00:40,077 --> 00:00:42,160
are going to be fair game.

10
00:00:42,160 --> 00:00:44,613
The poster session
we've decided on a time,

11
00:00:44,613 --> 00:00:47,601
it's going to be Tuesday June sixth

12
00:00:47,601 --> 00:00:48,704
from twelve to three p.m.

13
00:00:48,704 --> 00:00:49,931
So this is the last week of classes.

14
00:00:49,931 --> 00:00:52,431
So we have our our poster
session a little bit early

15
00:00:52,431 --> 00:00:54,638
during the last week so that after that,

16
00:00:54,638 --> 00:00:56,775
once you guys get feedback
you still have some time

17
00:00:56,775 --> 00:01:00,942
to work for your final report
which will be due finals week.

18
00:01:04,135 --> 00:01:06,622
Okay, so just a quick review of last time.

19
00:01:06,622 --> 00:01:08,091
Last time we talked
about different kinds of

20
00:01:08,091 --> 00:01:10,134
deep learning frameworks.

21
00:01:10,134 --> 00:01:12,667
We talked about you know
PyTorch, TensorFlow,

22
00:01:12,667 --> 00:01:13,500
Caffe2

23
00:01:15,324 --> 00:01:16,917
and we saw that using
these kinds of frameworks

24
00:01:16,917 --> 00:01:19,572
we were able to easily build
big computational graphs,

25
00:01:19,572 --> 00:01:23,410
for example very large neural
networks and comm nets,

26
00:01:23,410 --> 00:01:25,649
and be able to really
easily compute gradients

27
00:01:25,649 --> 00:01:26,594
in these graphs.

28
00:01:26,594 --> 00:01:29,458
So to compute all of the
gradients for all the intermediate

29
00:01:29,458 --> 00:01:33,225
variables weights inputs and
use that to train our models

30
00:01:33,225 --> 00:01:36,475
and to run all this efficiently on GPUs

31
00:01:38,468 --> 00:01:39,904
And we saw that for a
lot of these frameworks

32
00:01:39,904 --> 00:01:43,338
the way this works is by
working with these modularized

33
00:01:43,338 --> 00:01:45,788
layers that you guys have
been working writing with,

34
00:01:45,788 --> 00:01:48,281
in your home works as well
where we have a forward pass,

35
00:01:48,281 --> 00:01:50,738
we have a backward pass,

36
00:01:50,738 --> 00:01:54,382
and then in our final model architecture,

37
00:01:54,382 --> 00:01:56,427
all we need to do then is to just define

38
00:01:56,427 --> 00:01:59,214
all of these sequence of layers together.

39
00:01:59,214 --> 00:02:02,080
So using that we're able
to very easily be able to

40
00:02:02,080 --> 00:02:05,747
build up very complex
network architectures.

41
00:02:07,436 --> 00:02:09,971
So today we're going to talk
about some specific kinds

42
00:02:09,971 --> 00:02:13,282
of CNN Architectures that are
used today in cutting edge

43
00:02:13,282 --> 00:02:15,330
applications and research.

44
00:02:15,330 --> 00:02:17,867
And so we'll go into depth
in some of the most commonly

45
00:02:17,867 --> 00:02:20,441
used architectures for
these that are winners

46
00:02:20,441 --> 00:02:22,935
of ImageNet classification benchmarks.

47
00:02:22,935 --> 00:02:26,407
So in chronological
order AlexNet, VGG net,

48
00:02:26,407 --> 00:02:28,895
GoogLeNet, and ResNet.

49
00:02:28,895 --> 00:02:31,616
And so these will go into a lot of depth.

50
00:02:31,616 --> 00:02:33,451
And then I'll also after
that, briefly go through

51
00:02:33,451 --> 00:02:36,598
some other architectures that are not

52
00:02:36,598 --> 00:02:40,148
as prominently used these
days, but are interesting

53
00:02:40,148 --> 00:02:41,998
either from a historical perspective,

54
00:02:41,998 --> 00:02:44,581
or as recent areas of research.

55
00:02:47,632 --> 00:02:49,272
Okay, so just a quick review.

56
00:02:49,272 --> 00:02:51,649
We talked a long time ago about LeNet,

57
00:02:51,649 --> 00:02:54,202
which was one of the first
instantiations of a comNet

58
00:02:54,202 --> 00:02:56,413
that was successfully used in practice.

59
00:02:56,413 --> 00:03:00,094
And so this was the comNet
that took an input image,

60
00:03:00,094 --> 00:03:03,646
used com filters five by five filters

61
00:03:03,646 --> 00:03:06,588
applied at stride one and
had a couple of conv layers,

62
00:03:06,588 --> 00:03:09,247
a few pooling layers and then
some fully connected layers

63
00:03:09,247 --> 00:03:10,145
at the end.

64
00:03:10,145 --> 00:03:13,380
And this fairly simple comNet
was very successfully applied

65
00:03:13,380 --> 00:03:15,130
to digit recognition.

66
00:03:17,840 --> 00:03:21,521
So AlexNet from 2012 which
you guys have also heard

67
00:03:21,521 --> 00:03:23,685
already before in previous classes,

68
00:03:23,685 --> 00:03:28,219
was the first large scale
convolutional neural network

69
00:03:28,219 --> 00:03:31,989
that was able to do well on
the ImageNet classification

70
00:03:31,989 --> 00:03:36,889
task so in 2012 AlexNet was
entered in the competition,

71
00:03:36,889 --> 00:03:39,380
and was able to outperform
all previous non deep

72
00:03:39,380 --> 00:03:41,421
learning based models
by a significant margin,

73
00:03:41,421 --> 00:03:44,974
and so this was the comNet
that started the spree

74
00:03:44,974 --> 00:03:48,822
of comNet research and usage afterwards.

75
00:03:48,822 --> 00:03:52,051
And so the basic comNet
AlexNet architecture

76
00:03:52,051 --> 00:03:54,820
is a conv layer followed by pooling layer,

77
00:03:54,820 --> 00:03:57,237
normalization, com pool norm,

78
00:03:59,231 --> 00:04:01,816
and then a few more conv
layers, a pooling layer,

79
00:04:01,816 --> 00:04:04,232
and then several fully
connected layers afterwards.

80
00:04:04,232 --> 00:04:06,937
So this actually looks very
similar to the LeNet network

81
00:04:06,937 --> 00:04:08,285
that we just saw.

82
00:04:08,285 --> 00:04:10,576
There's just more layers in total.

83
00:04:10,576 --> 00:04:12,421
There is five of these conv layers,

84
00:04:12,421 --> 00:04:15,504
and two fully connected layers before

85
00:04:16,397 --> 00:04:19,257
the final fully connected
layer going to the output

86
00:04:19,257 --> 00:04:20,090
classes.

87
00:04:22,699 --> 00:04:24,904
So let's first get a sense
of the sizes involved

88
00:04:24,904 --> 00:04:26,740
in the AlexNet.

89
00:04:26,740 --> 00:04:28,417
So if we look at the input to the AlexNet

90
00:04:28,417 --> 00:04:30,465
this was trained on ImageNet, with inputs

91
00:04:30,465 --> 00:04:33,938
at a size 227 by 227 by 3 images.

92
00:04:33,938 --> 00:04:37,825
And if we look at this first
layer which is a conv layer

93
00:04:37,825 --> 00:04:41,266
for the AlexNet, it's 11 by 11 filters,

94
00:04:41,266 --> 00:04:44,003
96 of these applied at stride 4.

95
00:04:44,003 --> 00:04:45,966
So let's just think
about this for a moment.

96
00:04:45,966 --> 00:04:50,133
What's the output volume
size of this first layer?

97
00:04:52,598 --> 00:04:54,181
And there's a hint.

98
00:04:58,579 --> 00:05:01,204
So remember we have our input size,

99
00:05:01,204 --> 00:05:04,351
we have our convolutional filters, ray.

100
00:05:04,351 --> 00:05:07,006
And we have this formula,
which is the hint over here

101
00:05:07,006 --> 00:05:09,871
that gives you the size
of the output dimensions

102
00:05:09,871 --> 00:05:12,251
after applying com right?

103
00:05:12,251 --> 00:05:15,569
So remember it was the full
image, minus the filter size,

104
00:05:15,569 --> 00:05:18,442
divided by the stride, plus one.

105
00:05:18,442 --> 00:05:21,875
So given that that's
written up here for you 55,

106
00:05:21,875 --> 00:05:25,274
does anyone have a guess at
what's the final output size

107
00:05:25,274 --> 00:05:27,729
after this conv layer?

108
00:05:27,729 --> 00:05:30,633
[student speaks off mic]

109
00:05:30,633 --> 00:05:32,757
- So I had 55 by 55 by 96, yep.

110
00:05:32,757 --> 00:05:33,776
That's correct.

111
00:05:33,776 --> 00:05:35,898
Right so our spatial
dimensions at the output

112
00:05:35,898 --> 00:05:38,923
are going to be 55 in each
dimension and then we have

113
00:05:38,923 --> 00:05:41,745
96 total filters so the
depth after our conv layer

114
00:05:41,745 --> 00:05:43,245
is going to be 96.

115
00:05:44,114 --> 00:05:46,201
So that's the output volume.

116
00:05:46,201 --> 00:05:50,296
And what's the total number
of parameters in this layer?

117
00:05:50,296 --> 00:05:53,629
So remember we have 96 11 by 11 filters.

118
00:05:55,661 --> 00:05:58,563
[student speaks off mic]

119
00:05:58,563 --> 00:06:01,563
- [Lecturer] 96 by 11 by 11, almost.

120
00:06:02,755 --> 00:06:05,082
So yes, so I had another by three,

121
00:06:05,082 --> 00:06:06,107
yes that's correct.

122
00:06:06,107 --> 00:06:08,961
So each of the filters is going to

123
00:06:08,961 --> 00:06:12,279
see through a local region
of 11 by 11 by three,

124
00:06:12,279 --> 00:06:14,442
right because the input depth was three.

125
00:06:14,442 --> 00:06:18,843
And so, that's each filter
size, times we have 96

126
00:06:18,843 --> 00:06:19,793
of these total.

127
00:06:19,793 --> 00:06:23,960
And so there's 35K parameters
in this first layer.

128
00:06:26,828 --> 00:06:28,589
Okay, so now if we look
at the second layer

129
00:06:28,589 --> 00:06:31,043
this is a pooling layer
right and in this case

130
00:06:31,043 --> 00:06:34,814
we have three three by three
filters applied at stride two.

131
00:06:34,814 --> 00:06:38,981
So what's the output volume
of this layer after pooling?

132
00:06:41,511 --> 00:06:45,678
And again we have a hint, very
similar to the last question.

133
00:06:52,061 --> 00:06:53,811
Okay, 27 by 27 by 96.

134
00:06:55,494 --> 00:06:57,077
Yes that's correct.

135
00:06:58,526 --> 00:07:00,409
Right so the pooling layer
is basically going to use

136
00:07:00,409 --> 00:07:02,338
this formula that we had here.

137
00:07:02,338 --> 00:07:05,894
Again because these are pooling
applied at a stride of two

138
00:07:05,894 --> 00:07:08,506
so we're going to use the
same formula to determine

139
00:07:08,506 --> 00:07:11,739
the spatial dimensions and
so the spatial dimensions

140
00:07:11,739 --> 00:07:15,871
are going to be 27 by
27, and pooling preserves

141
00:07:15,871 --> 00:07:17,465
the depth.

142
00:07:17,465 --> 00:07:20,117
So we had 96 as depth as input,

143
00:07:20,117 --> 00:07:21,504
and it's still going to be 96 depth

144
00:07:21,504 --> 00:07:22,337
at output.

145
00:07:23,635 --> 00:07:25,104
And next question.

146
00:07:25,104 --> 00:07:28,937
What's the number of
parameters in this layer?

147
00:07:32,256 --> 00:07:33,811
I hear some muttering.

148
00:07:33,811 --> 00:07:35,164
[student answers off mic]

149
00:07:35,164 --> 00:07:35,997
- Nothing.

150
00:07:36,882 --> 00:07:37,715
Okay.

151
00:07:37,715 --> 00:07:39,528
Yes, so pooling layer
has no parameters, so,

152
00:07:39,528 --> 00:07:41,611
kind of a trick question.

153
00:07:43,549 --> 00:07:46,082
Okay, so we can basically, yes, question?

154
00:07:46,082 --> 00:07:48,002
[student speaks off mic]

155
00:07:48,002 --> 00:07:51,961
- The question is, why are
there no parameters in the

156
00:07:51,961 --> 00:07:52,990
pooling layer?

157
00:07:52,990 --> 00:07:54,380
The parameters are the weights right,

158
00:07:54,380 --> 00:07:55,361
that we're trying to learn.

159
00:07:55,361 --> 00:07:57,321
And so convolutional layers
have weights that we learn

160
00:07:57,321 --> 00:07:59,702
but pooling all we do is have a rule,

161
00:07:59,702 --> 00:08:01,415
we look at the pooling region,

162
00:08:01,415 --> 00:08:03,046
and we take the max.

163
00:08:03,046 --> 00:08:06,520
So there's no parameters that are learned.

164
00:08:06,520 --> 00:08:08,939
So we can keep on doing
this and you can just repeat

165
00:08:08,939 --> 00:08:11,817
the process and it's kind of
a good exercise to go through

166
00:08:11,817 --> 00:08:13,810
this and figure out the
sizes, the parameters,

167
00:08:13,810 --> 00:08:15,060
at every layer.

168
00:08:17,283 --> 00:08:18,589
And so if you do this all the way,

169
00:08:18,589 --> 00:08:22,517
you can look at this is
the final architecture

170
00:08:22,517 --> 00:08:23,498
that you can work with.

171
00:08:23,498 --> 00:08:25,989
There's 11 by 11 filters at the beginning,

172
00:08:25,989 --> 00:08:28,319
then five by five and some
three by three filters.

173
00:08:28,319 --> 00:08:32,730
And so these are generally
pretty familiar looking sizes

174
00:08:32,730 --> 00:08:34,827
that you've seen before
and then at the end

175
00:08:34,827 --> 00:08:36,996
we have a couple of fully connected layers

176
00:08:36,996 --> 00:08:39,932
of size 4096 and finally the last layer,

177
00:08:39,933 --> 00:08:42,350
is FC8 going to the soft max,

178
00:08:43,499 --> 00:08:47,166
which is going to the
1000 ImageNet classes.

179
00:08:48,849 --> 00:08:50,612
And just a couple of details about this,

180
00:08:50,612 --> 00:08:53,515
it was the first use of
the ReLu non-linearity

181
00:08:53,515 --> 00:08:55,484
that we've talked about
that's the most commonly used

182
00:08:55,484 --> 00:08:57,162
non-linearity.

183
00:08:57,162 --> 00:09:00,602
They used local response
normalization layers

184
00:09:00,602 --> 00:09:03,302
basically trying to
normalize the response across

185
00:09:03,302 --> 00:09:06,772
neighboring channels but this
is something that's not really

186
00:09:06,772 --> 00:09:08,201
used anymore.

187
00:09:08,201 --> 00:09:10,165
It turned out not to, other people showed

188
00:09:10,165 --> 00:09:12,747
that it didn't have so much of an effect.

189
00:09:12,747 --> 00:09:14,709
There's a lot of heavy data augmentation,

190
00:09:14,709 --> 00:09:17,054
and so you can look in the
paper for more details,

191
00:09:17,054 --> 00:09:19,673
but things like flipping,
jittering, cropping,

192
00:09:19,673 --> 00:09:22,579
color normalization all of these things

193
00:09:22,579 --> 00:09:25,191
which you'll probably
find useful for you when

194
00:09:25,191 --> 00:09:27,650
you're working on your
projects for example,

195
00:09:27,650 --> 00:09:29,537
so a lot of data augmentation here.

196
00:09:29,537 --> 00:09:33,229
They also use dropout batch size of 128,

197
00:09:33,229 --> 00:09:37,993
and learned with SGD with
momentum which we talked about

198
00:09:37,993 --> 00:09:40,608
in an earlier lecture,
and basically just started

199
00:09:40,608 --> 00:09:43,105
with a base learning
rate of 1e negative 2.

200
00:09:43,105 --> 00:09:46,212
Every time it plateaus,
reduce by a factor of 10

201
00:09:46,212 --> 00:09:48,216
and then just keep going.

202
00:09:48,216 --> 00:09:50,955
Until they finish training

203
00:09:50,955 --> 00:09:53,571
and a little bit of weight
decay and in the end,

204
00:09:53,571 --> 00:09:57,089
in order to get the best
numbers they also did

205
00:09:57,089 --> 00:09:59,822
an ensembling of models and
so training multiple of these,

206
00:09:59,822 --> 00:10:02,722
averaging them together and
this also gives an improvement

207
00:10:02,722 --> 00:10:03,972
in performance.

208
00:10:05,215 --> 00:10:07,138
And so one other thing I want to point out

209
00:10:07,138 --> 00:10:09,591
is that if you look at this
AlexNet diagram up here,

210
00:10:09,591 --> 00:10:13,390
it looks kind of like the
normal comNet diagrams

211
00:10:13,390 --> 00:10:16,045
that we've been seeing,
except for one difference,

212
00:10:16,045 --> 00:10:18,580
which is that it's, you
can see it's kind of split

213
00:10:18,580 --> 00:10:22,747
in these two different rows
or columns going across.

214
00:10:23,987 --> 00:10:27,431
And so the reason for this
is mostly historical note,

215
00:10:27,431 --> 00:10:31,431
so AlexNet was trained
on GTX580 GPUs older GPUs

216
00:10:32,632 --> 00:10:34,916
that only had three gigs of memory.

217
00:10:34,916 --> 00:10:38,065
So it couldn't actually fit
this entire network on here,

218
00:10:38,065 --> 00:10:39,757
and so what they ended up doing,

219
00:10:39,757 --> 00:10:42,583
was they spread the
network across two GPUs.

220
00:10:42,583 --> 00:10:45,091
So on each GPU you would
have half of the neurons,

221
00:10:45,091 --> 00:10:47,265
or half of the feature maps.

222
00:10:47,265 --> 00:10:50,040
And so for example if you
look at this first conv layer,

223
00:10:50,040 --> 00:10:52,540
we have 55 by 55 by 96 output,

224
00:10:55,199 --> 00:10:57,568
but if you look at this diagram carefully,

225
00:10:57,568 --> 00:11:00,144
you can zoom in later in the actual paper,

226
00:11:00,144 --> 00:11:02,965
you can see that, it's actually only 48

227
00:11:02,965 --> 00:11:04,965
depth-wise, on each GPU,

228
00:11:05,859 --> 00:11:07,986
and so they just spread
it, the feature maps,

229
00:11:07,986 --> 00:11:09,403
directly in half.

230
00:11:11,098 --> 00:11:13,269
And so what happens is that
for most of these layers,

231
00:11:13,269 --> 00:11:16,580
for example com one, two, four and five,

232
00:11:16,580 --> 00:11:18,177
the connections are only with feature maps

233
00:11:18,177 --> 00:11:21,816
on the same GPU, so you
would take as input,

234
00:11:21,816 --> 00:11:26,072
half of the feature maps
that were on the the same GPU

235
00:11:26,072 --> 00:11:29,304
as before and you don't
look at the full 96

236
00:11:29,304 --> 00:11:30,493
feature maps for example.

237
00:11:30,493 --> 00:11:34,660
You just take as input the
48 in that first layer.

238
00:11:35,577 --> 00:11:38,527
And then there's a few
layers so com three,

239
00:11:38,527 --> 00:11:42,006
as well as FC six, seven and eight,

240
00:11:42,006 --> 00:11:45,195
where here are the GPUs
do talk to each other

241
00:11:45,195 --> 00:11:47,673
and so there's connections
with all feature maps

242
00:11:47,673 --> 00:11:48,506
in the preceding layer.

243
00:11:48,506 --> 00:11:50,125
so there's communication across the GPUs,

244
00:11:50,125 --> 00:11:52,015
and each of these neurons
are then connected

245
00:11:52,015 --> 00:11:55,001
to the full depth of the
previous input layer.

246
00:11:55,001 --> 00:11:56,437
Question.

247
00:11:56,437 --> 00:11:59,141
- [Student] It says the
full simplified AlexNetwork

248
00:11:59,141 --> 00:12:00,085
architecture.

249
00:12:00,085 --> 00:12:02,252
[mumbles]

250
00:12:06,393 --> 00:12:07,781
- Oh okay, so the question
is why does it say

251
00:12:07,781 --> 00:12:10,843
full simplified AlexNet architecture here?

252
00:12:10,843 --> 00:12:13,139
It just says that because I
didn't put all the details

253
00:12:13,139 --> 00:12:17,192
on here, so for example this
is the full set of layers

254
00:12:17,192 --> 00:12:19,846
in the architecture, and
the strides and so on,

255
00:12:19,846 --> 00:12:22,911
but for example the normalization
layer, there's other,

256
00:12:22,911 --> 00:12:26,078
these details are not written on here.

257
00:12:31,447 --> 00:12:33,244
And then just one little note,

258
00:12:33,244 --> 00:12:35,576
if you look at the paper
and try and write out

259
00:12:35,576 --> 00:12:38,659
the math and architectures and so on,

260
00:12:39,668 --> 00:12:43,178
there's a little bit of
an issue on the very first

261
00:12:43,178 --> 00:12:45,474
layer they'll say if
you'll look in the figure

262
00:12:45,474 --> 00:12:47,276
they'll say 224 by 224 ,

263
00:12:47,276 --> 00:12:49,439
but there's actually some
kind of funny pattern

264
00:12:49,439 --> 00:12:51,364
going on and so the
numbers actually work out

265
00:12:51,364 --> 00:12:53,531
if you look at it as 227.

266
00:12:55,792 --> 00:12:59,959
AlexNet was the winner of
the ImageNet classification

267
00:13:00,944 --> 00:13:03,396
benchmark in 2012, you can see that

268
00:13:03,396 --> 00:13:06,056
it cut the error rate
by quite a large margin.

269
00:13:06,056 --> 00:13:10,223
It was the first CNN base
winner, and it was widely used

270
00:13:11,750 --> 00:13:13,430
as a base to our architecture

271
00:13:13,430 --> 00:13:16,530
almost ubiquitously from then
until a couple years ago.

272
00:13:16,530 --> 00:13:18,790
It's still used quite a bit.

273
00:13:18,790 --> 00:13:22,066
It's used in transfer learning
for lots of different tasks

274
00:13:22,066 --> 00:13:24,881
and so it was used for
basically a long time,

275
00:13:24,881 --> 00:13:28,487
and it was very famous and
now though there's been

276
00:13:28,487 --> 00:13:31,677
some more recent architectures
that have generally

277
00:13:31,677 --> 00:13:34,012
just had better performance
and so we'll talk about these

278
00:13:34,012 --> 00:13:36,592
next and these are going to be
the more common architectures

279
00:13:36,592 --> 00:13:40,092
that you'll be wanting to use in practice.

280
00:13:41,663 --> 00:13:45,838
So just quickly first in
2013 the ImageNet challenge

281
00:13:45,838 --> 00:13:48,623
was won by something called a ZFNet.

282
00:13:48,623 --> 00:13:49,528
Yes, question.

283
00:13:49,528 --> 00:13:53,539
[student speaks off mic]

284
00:13:53,539 --> 00:13:55,298
- So the question is intuition why AlexNet

285
00:13:55,298 --> 00:13:57,422
was so much better than
the ones that came before,

286
00:13:57,422 --> 00:14:01,600
DefLearning comNets [mumbles] this is just

287
00:14:01,600 --> 00:14:05,596
a very different kind of
approach in architecture.

288
00:14:05,596 --> 00:14:07,564
So this was the first deep
learning based approach

289
00:14:07,564 --> 00:14:09,814
first comNet that was used.

290
00:14:13,255 --> 00:14:16,126
So in 2013 the challenge
was won by something called

291
00:14:16,126 --> 00:14:19,108
a ZFNet [Zeller Fergus Net]
named after the creators.

292
00:14:19,108 --> 00:14:23,537
And so this mostly was
improving hyper parameters

293
00:14:23,537 --> 00:14:24,559
over the AlexNet.

294
00:14:24,559 --> 00:14:26,194
It had the same number of layers,

295
00:14:26,194 --> 00:14:28,566
the same general structure
and they made a few

296
00:14:28,566 --> 00:14:31,518
changes things like
changing the stride size,

297
00:14:31,518 --> 00:14:34,909
different numbers of filters
and after playing around

298
00:14:34,909 --> 00:14:36,545
with these hyper parameters more,

299
00:14:36,545 --> 00:14:40,751
they were able to improve the error rate.

300
00:14:40,751 --> 00:14:42,179
But it's still basically the same idea.

301
00:14:42,179 --> 00:14:44,720
So in 2014 there are a
couple of architectures

302
00:14:44,720 --> 00:14:47,291
that were now more significantly different

303
00:14:47,291 --> 00:14:50,653
and made another jump in performance,

304
00:14:50,653 --> 00:14:54,820
and the main difference with
these networks first of all

305
00:14:56,699 --> 00:14:58,988
was much deeper networks.

306
00:14:58,988 --> 00:15:01,926
So from the eight layer
network that was in 2012

307
00:15:01,926 --> 00:15:06,093
and 2013, now in 2014 we
had two very close winners

308
00:15:07,606 --> 00:15:10,999
that were around 19 layers and 22 layers.

309
00:15:10,999 --> 00:15:13,131
So significantly deeper.

310
00:15:13,131 --> 00:15:17,312
And the winner of this
was GoogleNet, from Google

311
00:15:17,312 --> 00:15:20,986
but very close behind was
something called VGGNet

312
00:15:20,986 --> 00:15:24,750
from Oxford, and on actually
the localization challenge

313
00:15:24,750 --> 00:15:28,231
VGG got first place in
some of the other tracks.

314
00:15:28,231 --> 00:15:32,768
So these were both very,
very strong networks.

315
00:15:32,768 --> 00:15:35,473
So let's first look at VGG
in a little bit more detail.

316
00:15:35,473 --> 00:15:39,458
And so the VGG network is the
idea of much deeper networks

317
00:15:39,458 --> 00:15:41,628
and with much smaller filters.

318
00:15:41,628 --> 00:15:44,795
So they increased the number of layers

319
00:15:45,632 --> 00:15:48,854
from eight layers in AlexNet
right to now they had

320
00:15:48,854 --> 00:15:52,021
models with 16 to 19 layers in VGGNet.

321
00:15:53,100 --> 00:15:55,196
And one key thing that they
did was they kept very small

322
00:15:55,196 --> 00:15:58,307
filter so only three by
three conv all the way,

323
00:15:58,307 --> 00:16:01,459
which is basically the
smallest com filter size

324
00:16:01,459 --> 00:16:04,726
that is looking at a little
bit of the neighboring pixels.

325
00:16:04,726 --> 00:16:07,100
And they just kept this
very simple structure

326
00:16:07,100 --> 00:16:09,553
of three by three convs
with the periodic pooling

327
00:16:09,553 --> 00:16:12,295
all the way through the network.

328
00:16:12,295 --> 00:16:15,486
And it's very simple elegant
network architecture,

329
00:16:15,486 --> 00:16:18,591
was able to get 7.3% top five error

330
00:16:18,591 --> 00:16:20,758
on the ImageNet challenge.

331
00:16:23,461 --> 00:16:28,252
So first the question of
why use smaller filters.

332
00:16:28,252 --> 00:16:31,931
So when we take these
small filters now we have

333
00:16:31,931 --> 00:16:34,181
fewer parameters and we
try and stack more of them

334
00:16:34,181 --> 00:16:35,659
instead of having larger filters,

335
00:16:35,659 --> 00:16:37,869
have smaller filters
with more depth instead,

336
00:16:37,869 --> 00:16:40,154
have more of these filters instead,

337
00:16:40,154 --> 00:16:43,384
what happens is that you end
up having the same effective

338
00:16:43,384 --> 00:16:46,414
receptive field as if you
only have one seven by seven

339
00:16:46,414 --> 00:16:48,012
convolutional layer.

340
00:16:48,012 --> 00:16:52,099
So here's a question, what is
the effective receptive field

341
00:16:52,099 --> 00:16:55,416
of three of these three
by three conv layers

342
00:16:55,416 --> 00:16:56,276
with stride one?

343
00:16:56,276 --> 00:16:58,854
So if you were to stack three
three by three conv layers

344
00:16:58,854 --> 00:17:01,999
with Stride one what's the
effective receptive field,

345
00:17:01,999 --> 00:17:06,166
the total area of the input,
spatial area of the input

346
00:17:07,232 --> 00:17:10,463
that enure at the top
layer of the three layers

347
00:17:10,463 --> 00:17:11,630
is looking at.

348
00:17:13,123 --> 00:17:16,797
So I heard fifteen pixels,
why fifteen pixels?

349
00:17:16,797 --> 00:17:21,419
- [Student] Okay, so the
reason given was because

350
00:17:21,419 --> 00:17:22,276
they overlap--

351
00:17:22,276 --> 00:17:26,662
- Okay, so the reason given
was because they overlap.

352
00:17:26,662 --> 00:17:28,179
So it's on the right track.

353
00:17:28,179 --> 00:17:32,350
What actually is happening
though is you have to see,

354
00:17:32,350 --> 00:17:34,803
at the first layer, the
receptive field is going to be

355
00:17:34,803 --> 00:17:36,478
three by three right?

356
00:17:36,478 --> 00:17:38,558
And then at the second layer,

357
00:17:38,558 --> 00:17:41,379
each of these neurons in the second layer

358
00:17:41,379 --> 00:17:44,003
is going to look at three
by three other first layer

359
00:17:44,003 --> 00:17:47,278
filters, but the corners
of these three by three

360
00:17:47,278 --> 00:17:49,690
have an additional pixel on each side,

361
00:17:49,690 --> 00:17:52,486
that is looking at in
the original input layer.

362
00:17:52,486 --> 00:17:55,476
So the second layer is actually
looking at five by five

363
00:17:55,476 --> 00:17:57,233
receptive field and then
if you do this again,

364
00:17:57,233 --> 00:18:01,119
the third layer is
looking at three by three

365
00:18:01,119 --> 00:18:04,850
in the second layer but this is going to,

366
00:18:04,850 --> 00:18:06,245
if you just draw out this
pyramid is looking at

367
00:18:06,245 --> 00:18:07,717
seven by seven in the input layer.

368
00:18:07,717 --> 00:18:09,565
So the effective receptive field here

369
00:18:09,565 --> 00:18:12,470
is going to be seven by seven.

370
00:18:12,470 --> 00:18:16,836
Which is the same as one
seven by seven conv layer.

371
00:18:16,836 --> 00:18:18,676
So what happens is that
this has the same effective

372
00:18:18,676 --> 00:18:21,128
receptive field as a
seven by seven conv layer

373
00:18:21,128 --> 00:18:22,356
but it's deeper.

374
00:18:22,356 --> 00:18:24,801
It's able to have more
non-linearities in there,

375
00:18:24,801 --> 00:18:27,011
and it's also fewer parameters.

376
00:18:27,011 --> 00:18:28,599
So if you look at the
total number of parameters,

377
00:18:28,599 --> 00:18:33,179
each of these conv filters
for the three by threes

378
00:18:33,179 --> 00:18:37,346
is going to have nine parameters
in each conv [mumbles]

379
00:18:38,975 --> 00:18:42,338
three times three, and
then times the input depth,

380
00:18:42,338 --> 00:18:45,458
so three times three times
C, times this total number

381
00:18:45,458 --> 00:18:48,859
of output feature maps, which is again C

382
00:18:48,859 --> 00:18:50,491
is we're going to preserve the total

383
00:18:50,491 --> 00:18:51,844
number of channels.

384
00:18:51,844 --> 00:18:54,300
So you get three times three,

385
00:18:54,300 --> 00:18:57,128
times C times C for each of these layers,

386
00:18:57,128 --> 00:18:59,089
and we have three layers
so it's going to be

387
00:18:59,089 --> 00:19:00,975
three times this number,

388
00:19:00,975 --> 00:19:04,167
compared to if you had a
single seven by seven layer

389
00:19:04,167 --> 00:19:06,090
then you get, by the same reasoning,

390
00:19:06,090 --> 00:19:08,219
seven squared times C squared.

391
00:19:08,219 --> 00:19:10,675
So you're going to have
fewer parameters total,

392
00:19:10,675 --> 00:19:11,842
which is nice.

393
00:19:16,380 --> 00:19:20,511
So now if we look at
this full network here

394
00:19:20,511 --> 00:19:22,721
there's a lot of numbers up
here that you can go back

395
00:19:22,721 --> 00:19:24,971
and look at more carefully
but if we look at all

396
00:19:24,971 --> 00:19:28,572
of the sizes and number
of parameters the same way

397
00:19:28,572 --> 00:19:31,526
that we calculated the
example for AlexNet,

398
00:19:31,526 --> 00:19:33,327
this is a good exercise to go through,

399
00:19:33,327 --> 00:19:36,563
we can see that you
know going the same way

400
00:19:36,563 --> 00:19:40,656
we have a couple of these conv
layers and a pooling layer

401
00:19:40,656 --> 00:19:43,973
a couple more conv layers,
pooling layer, several more

402
00:19:43,973 --> 00:19:45,120
conv layers and so on.

403
00:19:45,120 --> 00:19:46,644
And so this just keeps going up.

404
00:19:46,644 --> 00:19:49,068
And if you counted the total
number of convolutional

405
00:19:49,068 --> 00:19:51,680
and fully connected layers,
we're going to have 16

406
00:19:51,680 --> 00:19:53,241
in this case for VGG 16,

407
00:19:53,241 --> 00:19:57,121
and then VGG 19, it's just a very similar

408
00:19:57,121 --> 00:20:01,288
architecture, but with a few
more conv layers in there.

409
00:20:03,831 --> 00:20:06,415
And so the total memory
usage of this network,

410
00:20:06,415 --> 00:20:10,467
so just making a forward
pass through counting up

411
00:20:10,467 --> 00:20:13,538
all of these numbers so
in the memory numbers here

412
00:20:13,538 --> 00:20:16,414
written in terms of the total numbers,

413
00:20:16,414 --> 00:20:18,006
like we calculated earlier,

414
00:20:18,006 --> 00:20:20,170
and if you look at four bytes per number,

415
00:20:20,170 --> 00:20:23,935
this is going to be
about 100 megs per image,

416
00:20:23,935 --> 00:20:27,657
and so this is the scale
of the memory usage

417
00:20:27,657 --> 00:20:29,537
that's happening and this is
only for a forward pass right,

418
00:20:29,537 --> 00:20:31,707
when you do a backward pass
you're going to have to store

419
00:20:31,707 --> 00:20:36,280
more and so this is
pretty heavy memory wise.

420
00:20:36,280 --> 00:20:39,947
100 megs per image, if
you have on five gigs

421
00:20:41,200 --> 00:20:44,223
of total memory, then
you're only going to be able

422
00:20:44,223 --> 00:20:46,473
to store about 50 of these.

423
00:20:48,110 --> 00:20:50,766
And so also the total number
of parameters here we have

424
00:20:50,766 --> 00:20:54,324
is 138 million parameters in this network,

425
00:20:54,324 --> 00:20:56,941
and this compares with
60 million for AlexNet.

426
00:20:56,941 --> 00:20:58,291
Question?

427
00:20:58,291 --> 00:21:01,708
[student speaks off mic]

428
00:21:07,014 --> 00:21:09,097
- So the question is what
do we mean by deeper,

429
00:21:09,097 --> 00:21:10,730
is it the number of
filters, number of layers?

430
00:21:10,730 --> 00:21:14,897
So deeper in this case is
always referring to layers.

431
00:21:16,415 --> 00:21:19,082
So there are two usages of the word depth

432
00:21:19,082 --> 00:21:23,249
which is confusing one is
the depth rate per channel,

433
00:21:24,975 --> 00:21:26,892
width by height by depth, you can use

434
00:21:26,892 --> 00:21:27,752
the word depth here,

435
00:21:27,752 --> 00:21:30,078
but in general we talk about
the depth of a network,

436
00:21:30,078 --> 00:21:33,065
this is going to be the
total number of layers

437
00:21:33,065 --> 00:21:35,108
in the network, and usually in particular

438
00:21:35,108 --> 00:21:38,781
we're counting the total
number of weight layers.

439
00:21:38,781 --> 00:21:41,195
So the total number of
layers with trainable weight,

440
00:21:41,195 --> 00:21:44,178
so convolutional layers
and fully connected layers.

441
00:21:44,178 --> 00:21:47,678
[student mumbles off mic]

442
00:22:01,620 --> 00:22:04,118
- Okay, so the question
is, within each layer

443
00:22:04,118 --> 00:22:06,984
what do different filters need?

444
00:22:06,984 --> 00:22:11,238
And so we talked about this
back in the comNet lecture,

445
00:22:11,238 --> 00:22:13,853
so you can also go back and refer to that,

446
00:22:13,853 --> 00:22:18,356
but each filter is a set of
let's say three by three convs,

447
00:22:18,356 --> 00:22:20,900
so each filter is looking at a,

448
00:22:20,900 --> 00:22:24,253
is a set of weight looking at
a three by three value input

449
00:22:24,253 --> 00:22:28,426
input depth, and this
produces one feature map,

450
00:22:28,426 --> 00:22:30,759
one activation map of
all the responses of the

451
00:22:30,759 --> 00:22:32,764
different spatial locations.

452
00:22:32,764 --> 00:22:35,994
And then we have we can have
as many filters as we want

453
00:22:35,994 --> 00:22:38,328
right so for example 96 and each of these

454
00:22:38,328 --> 00:22:40,456
is going to produce a feature map.

455
00:22:40,456 --> 00:22:42,748
And so it's just like
each filter corresponds

456
00:22:42,748 --> 00:22:45,249
to a different pattern
that we're looking for

457
00:22:45,249 --> 00:22:47,620
in the input that we
convolve around and we see

458
00:22:47,620 --> 00:22:49,178
the responses everywhere in the input,

459
00:22:49,178 --> 00:22:52,824
we create a map of these
and then another filter

460
00:22:52,824 --> 00:22:56,991
will we convolve over the
image and create another map.

461
00:22:59,571 --> 00:23:01,036
Question.

462
00:23:01,036 --> 00:23:04,453
[student speaks off mic]

463
00:23:08,275 --> 00:23:10,808
- So question is, is
there intuition behind,

464
00:23:10,808 --> 00:23:13,376
as you go deeper into the network
we have more channel depth

465
00:23:13,376 --> 00:23:17,543
so more number of filters
right and so you can have

466
00:23:18,486 --> 00:23:22,576
any design that you want so
you don't have to do this.

467
00:23:22,576 --> 00:23:25,151
In practice you will see this
happen a lot of the times

468
00:23:25,151 --> 00:23:28,547
and one of the reasons is
people try and maintain

469
00:23:28,547 --> 00:23:31,408
kind of a relatively
constant level of compute,

470
00:23:31,408 --> 00:23:34,718
so as you go higher up or
deeper into your network,

471
00:23:34,718 --> 00:23:38,801
you're usually also using
basically down sampling

472
00:23:40,416 --> 00:23:43,869
and having smaller total
spatial area and then so then

473
00:23:43,869 --> 00:23:46,569
they also increase now you
increase by depth a little bit,

474
00:23:46,569 --> 00:23:48,525
it's not as expensive
now to increase by depth

475
00:23:48,525 --> 00:23:51,321
because it's spatially smaller and so,

476
00:23:51,321 --> 00:23:54,177
yeah that's just a reason.

477
00:23:54,177 --> 00:23:55,526
Question.

478
00:23:55,526 --> 00:23:58,943
[student speaks off mic]

479
00:24:00,682 --> 00:24:02,478
- So performance-wise is
there any reason to use

480
00:24:02,478 --> 00:24:05,463
SBN [mumbles] instead
of SouthMax [mumbles],

481
00:24:05,463 --> 00:24:07,671
so no, for a classifier
you can use either one,

482
00:24:07,671 --> 00:24:10,571
and you did that earlier
in the class as well,

483
00:24:10,571 --> 00:24:13,191
but in general SouthMax losses,

484
00:24:13,191 --> 00:24:16,052
have generally worked
well and been standard use

485
00:24:16,052 --> 00:24:18,052
for classification here.

486
00:24:19,319 --> 00:24:20,833
Okay yeah one more question.

487
00:24:20,833 --> 00:24:24,333
[student mumbles off mic]

488
00:24:38,712 --> 00:24:41,091
- Yes, so the question
is, we don't have to store

489
00:24:41,091 --> 00:24:43,714
all of the memory like we
can throw away the parts

490
00:24:43,714 --> 00:24:46,208
that we don't need and so on?

491
00:24:46,208 --> 00:24:48,230
And yes this is true.

492
00:24:48,230 --> 00:24:50,031
Some of this you don't need to keep,

493
00:24:50,031 --> 00:24:53,095
but you're also going to
be doing a backwards pass

494
00:24:53,095 --> 00:24:55,229
through ware for the most part,

495
00:24:55,229 --> 00:24:56,586
when you were doing the chain

496
00:24:56,586 --> 00:24:58,141
rule and so on you needed
a lot of these activations

497
00:24:58,141 --> 00:25:01,631
as part of it and so in
large part a lot of this

498
00:25:01,631 --> 00:25:03,381
does need to be kept.

499
00:25:04,816 --> 00:25:08,990
So if we look at the distribution
of where memory is used

500
00:25:08,990 --> 00:25:11,612
and where parameters are,
you can see that a lot

501
00:25:11,612 --> 00:25:15,250
of memories in these early
layers right where you still have

502
00:25:15,250 --> 00:25:19,656
spatial dimensions you're
going to have more memory usage

503
00:25:19,656 --> 00:25:21,994
and then a lot of the
parameters are actually in

504
00:25:21,994 --> 00:25:24,864
the last layers, the
fully connected layers

505
00:25:24,864 --> 00:25:26,821
have a huge number of parameters right,

506
00:25:26,821 --> 00:25:29,647
because we have all of
these dense connections.

507
00:25:29,647 --> 00:25:33,995
And so that's something
just to know and then

508
00:25:33,995 --> 00:25:37,809
keep in mind so later on we'll
see some networks actually

509
00:25:37,809 --> 00:25:39,771
get rid of these fully
connected layers and be able

510
00:25:39,771 --> 00:25:43,155
to save a lot on the number of parameters.

511
00:25:43,155 --> 00:25:45,029
And then just one last thing to point out,

512
00:25:45,029 --> 00:25:47,483
you'll also see different ways of calling

513
00:25:47,483 --> 00:25:48,869
all of these layers right.

514
00:25:48,869 --> 00:25:51,831
So here I've written out
exactly what the layers are.

515
00:25:51,831 --> 00:25:55,151
conv3-64 means three by three convs

516
00:25:55,151 --> 00:25:57,000
with 64 total filters.

517
00:25:57,000 --> 00:26:01,499
But for VGGNet on this
diagram on the right here

518
00:26:01,499 --> 00:26:03,868
there's also common ways
that people will look

519
00:26:03,868 --> 00:26:06,000
at each group of filters,

520
00:26:06,000 --> 00:26:08,992
so each orange block here, as in conv1

521
00:26:08,992 --> 00:26:11,734
part one, so conv1-1, conv1-2,

522
00:26:11,734 --> 00:26:12,632
and so on.

523
00:26:12,632 --> 00:26:15,465
So just something to keep in mind.

524
00:26:17,404 --> 00:26:20,515
So VGGNet ended up getting
second place in the

525
00:26:20,515 --> 00:26:22,930
ImageNet 2014 classification challenge,

526
00:26:22,930 --> 00:26:25,593
first in localization.

527
00:26:25,593 --> 00:26:27,106
They followed a very
similar training procedure

528
00:26:27,106 --> 00:26:29,847
as Alex Krizhevsky for the AlexNet.

529
00:26:29,847 --> 00:26:33,895
They didn't use local
response normalization,

530
00:26:33,895 --> 00:26:35,333
so as I mentioned earlier,

531
00:26:35,333 --> 00:26:37,367
they found out this
didn't really help them,

532
00:26:37,367 --> 00:26:39,574
and so they took it out.

533
00:26:39,574 --> 00:26:44,281
You'll see VGG 16 and VGG
19 are common variants

534
00:26:44,281 --> 00:26:46,657
of the cycle here, and this is just

535
00:26:46,657 --> 00:26:50,425
the number of layers, 19
is slightly deeper than 16.

536
00:26:50,425 --> 00:26:54,348
In practice VGG 19 works
very little bit better,

537
00:26:54,348 --> 00:26:57,009
and there's a little
bit more memory usage,

538
00:26:57,009 --> 00:27:01,176
so you can use either but
16 is very commonly used.

539
00:27:02,280 --> 00:27:06,247
For best results, like
AlexNet, they did ensembling

540
00:27:06,247 --> 00:27:09,198
in order to average several models,

541
00:27:09,198 --> 00:27:10,920
and you get better results.

542
00:27:10,920 --> 00:27:14,432
And they also showed in their work that

543
00:27:14,432 --> 00:27:18,112
the FC7 features of the last
fully connected layer before

544
00:27:18,112 --> 00:27:20,968
going to the 1000 ImageNet classes.

545
00:27:20,968 --> 00:27:24,771
The 4096 size layer just before that,

546
00:27:24,771 --> 00:27:27,273
is a good feature representation,

547
00:27:27,273 --> 00:27:29,648
that can even just be used as is,

548
00:27:29,648 --> 00:27:32,714
to extract these features from other data,

549
00:27:32,714 --> 00:27:35,865
and generalized these other tasks as well.

550
00:27:35,865 --> 00:27:38,602
And so FC7 is a good
feature representation.

551
00:27:38,602 --> 00:27:39,952
Yeah question.

552
00:27:39,952 --> 00:27:42,742
[student speaks off mic]

553
00:27:42,742 --> 00:27:45,242
- Sorry what was the question?

554
00:27:46,749 --> 00:27:50,846
Okay, so the question is
what is localization here?

555
00:27:50,846 --> 00:27:53,921
And so this is a task,
and we'll talk about it

556
00:27:53,921 --> 00:27:55,684
a little bit more in a later lecture

557
00:27:55,684 --> 00:27:57,973
on detection and localization
so I don't want to

558
00:27:57,973 --> 00:27:59,568
go into detail here but
it's basically an image,

559
00:27:59,568 --> 00:28:04,015
not just classifying What's
the class of the image,

560
00:28:04,015 --> 00:28:08,518
but also drawing a bounding
box around where that

561
00:28:08,518 --> 00:28:10,243
object is in the image.

562
00:28:10,243 --> 00:28:11,677
And the difference with detection,

563
00:28:11,677 --> 00:28:13,641
which is a very related
task is that detection

564
00:28:13,641 --> 00:28:16,963
there can be multiple instances
of this object in the image

565
00:28:16,963 --> 00:28:18,938
localization we're
assuming there's just one,

566
00:28:18,938 --> 00:28:21,481
this classification but we just how this

567
00:28:21,481 --> 00:28:23,481
additional bounding box.

568
00:28:26,153 --> 00:28:29,390
So we looked at VGG which
was one of the deep networks

569
00:28:29,390 --> 00:28:33,192
from 2014 and then now
we'll talk about GoogleNet

570
00:28:33,192 --> 00:28:34,996
which was the other one that won

571
00:28:34,996 --> 00:28:37,413
the classification challenge.

572
00:28:38,422 --> 00:28:41,288
So GoogleNet again was
a much deeper network

573
00:28:41,288 --> 00:28:44,871
with 22 layers but one
of the main insights

574
00:28:45,853 --> 00:28:48,586
and special things about
GoogleNet is that it really

575
00:28:48,586 --> 00:28:51,653
looked at this problem of
computational efficiency

576
00:28:51,653 --> 00:28:54,959
and it tried to design a
network architecture that was

577
00:28:54,959 --> 00:28:58,676
very efficient in the amount of compute.

578
00:28:58,676 --> 00:29:01,869
And so they did this using
this inception module

579
00:29:01,869 --> 00:29:05,833
which we'll go into more
detail and basically stacking

580
00:29:05,833 --> 00:29:09,146
a lot of these inception
modules on top of each other.

581
00:29:09,146 --> 00:29:12,292
There's also no fully connected
layers in this network,

582
00:29:12,292 --> 00:29:14,515
so they got rid of that
were able to save a lot

583
00:29:14,515 --> 00:29:17,133
of parameters and so in total
there's only five million

584
00:29:17,133 --> 00:29:20,651
parameters which is twelve
times less than AlexNet,

585
00:29:20,651 --> 00:29:25,118
which had 60 million even
though it's much deeper now.

586
00:29:25,118 --> 00:29:27,785
It got 6.7% top five error.

587
00:29:32,202 --> 00:29:34,414
So what's the inception module?

588
00:29:34,414 --> 00:29:36,173
So the idea behind the inception module

589
00:29:36,173 --> 00:29:40,833
is that they wanted to design
a good local network typology

590
00:29:40,833 --> 00:29:43,203
and it has this idea
of this local topology

591
00:29:43,203 --> 00:29:47,177
that's you know you can
think of it as a network

592
00:29:47,177 --> 00:29:48,857
within a network and
then stack a lot of these

593
00:29:48,857 --> 00:29:53,151
local typologies one on top of each other.

594
00:29:53,151 --> 00:29:55,890
And so in this local
network that they're calling

595
00:29:55,890 --> 00:29:59,197
an inception module what they're
doing is they're basically

596
00:29:59,197 --> 00:30:02,673
applying several different
kinds of filter operations

597
00:30:02,673 --> 00:30:05,823
in parallel on top of the
same input coming into

598
00:30:05,823 --> 00:30:07,948
this same layer.

599
00:30:07,948 --> 00:30:09,715
So we have our input coming
in from the previous layer

600
00:30:09,715 --> 00:30:12,706
and then we're going to do
different kinds of convolutions.

601
00:30:12,706 --> 00:30:16,838
So a one by one conv, right
a three by three conv,

602
00:30:16,838 --> 00:30:18,684
five by five conv, and then they also

603
00:30:18,684 --> 00:30:22,208
have a pooling operation
in this case three by three

604
00:30:22,208 --> 00:30:24,290
pooling, and so you get
all of these different

605
00:30:24,290 --> 00:30:26,457
outputs from these different layers,

606
00:30:26,457 --> 00:30:29,565
and then what they do is
they concatenate all these

607
00:30:29,565 --> 00:30:32,309
filter outputs together depth wise, and so

608
00:30:32,309 --> 00:30:36,036
then this creates one
tenser output at the end

609
00:30:36,036 --> 00:30:39,703
that is going tom pass
on to the next layer.

610
00:30:41,830 --> 00:30:44,368
So if we look at just a
naive way of doing this

611
00:30:44,368 --> 00:30:46,616
we just do exactly that we
have all of these different

612
00:30:46,616 --> 00:30:50,825
operations we get the outputs
we concatenate them together.

613
00:30:50,825 --> 00:30:53,196
So what's the problem with this?

614
00:30:53,196 --> 00:30:56,027
And it turns out that
computational complexity

615
00:30:56,027 --> 00:30:58,527
is going to be a problem here.

616
00:30:59,792 --> 00:31:01,514
So if we look more
carefully at an example,

617
00:31:01,514 --> 00:31:05,473
so here just for as an example
I've put one by one conv,

618
00:31:05,473 --> 00:31:09,476
128 filter so three by
three conv 192 filters,

619
00:31:09,476 --> 00:31:11,966
five by five convs and 96 filters.

620
00:31:11,966 --> 00:31:14,582
Assume everything has basically the stride

621
00:31:14,582 --> 00:31:17,041
that's going to maintain
the spatial dimensions,

622
00:31:17,041 --> 00:31:20,208
and that we have this input coming in.

623
00:31:22,151 --> 00:31:24,858
So what is the output size
of the one by one filter

624
00:31:24,858 --> 00:31:28,708
with 128 , one by one
conv with 128 filters?

625
00:31:28,708 --> 00:31:30,041
Who has a guess?

626
00:31:36,720 --> 00:31:40,720
OK so I heard 28 by 28,
by 128 which is correct.

627
00:31:41,798 --> 00:31:44,043
So right by one by one conv
we're going to maintain

628
00:31:44,043 --> 00:31:46,821
spatial dimensions and
then on top of that,

629
00:31:46,821 --> 00:31:50,374
each conv filter is going to look through

630
00:31:50,374 --> 00:31:53,969
the entire 256 depth of the input,

631
00:31:53,969 --> 00:31:55,723
but then the output is going to be,

632
00:31:55,723 --> 00:31:57,684
we have a 28 by 28 feature map

633
00:31:57,684 --> 00:32:00,145
for each of the 128 filters that we have

634
00:32:00,145 --> 00:32:01,004
in this conv layer.

635
00:32:01,004 --> 00:32:03,171
So we get 28 by 28 by 128.

636
00:32:06,279 --> 00:32:08,368
OK and then now if we do the same thing

637
00:32:08,368 --> 00:32:11,771
and we look at the filter
sizes of the output sizes sorry

638
00:32:11,771 --> 00:32:15,749
of all of the different
filters here, after the

639
00:32:15,749 --> 00:32:17,803
three by three conv we're
going to have this volume

640
00:32:17,803 --> 00:32:21,189
of 28 by 28 by 192 right
after five by five conv

641
00:32:21,189 --> 00:32:23,323
we have 96 filters here.

642
00:32:23,323 --> 00:32:25,369
So 28 by 28 by 96,

643
00:32:25,369 --> 00:32:28,851
and then out pooling layer is just going

644
00:32:28,851 --> 00:32:32,821
to keep the same spatial
dimension here, so pooling layer

645
00:32:32,821 --> 00:32:35,522
will preserve it in depth,

646
00:32:35,522 --> 00:32:36,835
and here because of our stride,

647
00:32:36,835 --> 00:32:41,002
we're also going to preserve
our spatial dimensions.

648
00:32:42,035 --> 00:32:44,288
And so now if we look at
the output size after filter

649
00:32:44,288 --> 00:32:48,589
concatenation what we're
going to get is 28 by 28,

650
00:32:48,589 --> 00:32:52,308
these are all 28 by 28, and
we concatenating depth wise.

651
00:32:52,308 --> 00:32:56,681
So we get 28 by 28 times
all of these added together,

652
00:32:56,681 --> 00:32:58,807
and the total output size is going to be

653
00:32:58,807 --> 00:33:00,140
28 by 28 by 672.

654
00:33:01,923 --> 00:33:05,798
So the input to our
inception module was 28 by 28

655
00:33:05,798 --> 00:33:10,185
by 256, then the output
from this module is 28 by 28

656
00:33:10,185 --> 00:33:11,018
by 672.

657
00:33:12,276 --> 00:33:15,148
So we kept the same spatial dimensions,

658
00:33:15,148 --> 00:33:18,064
and we blew up the depth.

659
00:33:18,064 --> 00:33:18,998
Question.

660
00:33:18,998 --> 00:33:22,715
[student speaks off mic]

661
00:33:22,715 --> 00:33:24,393
OK So in this case, yeah, the question is,

662
00:33:24,393 --> 00:33:26,356
how are we getting 28
by 28 for everything?

663
00:33:26,356 --> 00:33:28,436
So here we're doing all the zero padding

664
00:33:28,436 --> 00:33:30,117
in order to maintain
the spatial dimensions,

665
00:33:30,117 --> 00:33:32,130
and that way we can do this filter

666
00:33:32,130 --> 00:33:34,213
concatenation depth-wise.

667
00:33:35,205 --> 00:33:37,043
Question in the back.

668
00:33:37,043 --> 00:33:40,460
[student speaks off mic]

669
00:33:45,634 --> 00:33:48,615
- OK The question is what's
the 256 deep at the input,

670
00:33:48,615 --> 00:33:51,228
and so this is not the
input to the network,

671
00:33:51,228 --> 00:33:53,677
this is the input just
to this local module

672
00:33:53,677 --> 00:33:54,624
that I'm looking at.

673
00:33:54,624 --> 00:33:58,460
So in this case 256 is
the depth of the previous

674
00:33:58,460 --> 00:34:01,316
inception module that
came just before this.

675
00:34:01,316 --> 00:34:05,491
And so now coming out
we have 28 by 28 by 672,

676
00:34:05,491 --> 00:34:07,045
and that's going to be
the input to the next

677
00:34:07,045 --> 00:34:09,248
inception module.

678
00:34:09,248 --> 00:34:10,725
Question.

679
00:34:10,726 --> 00:34:14,143
[student speaks off mic]

680
00:34:17,849 --> 00:34:21,533
- Okay the question is, how
did we get 28 by 28 by 128

681
00:34:21,533 --> 00:34:23,991
for the first one, the first conv,

682
00:34:23,991 --> 00:34:28,614
and this is basically it's a
one by one convolution right,

683
00:34:28,614 --> 00:34:30,701
so we're going to take
this one by one convolution

684
00:34:30,701 --> 00:34:34,868
slide it across our 28 by
28 by 256 input spatially

685
00:34:36,295 --> 00:34:38,547
where it's at each location,
it's going to multiply,

686
00:34:38,547 --> 00:34:41,005
it's going to do a [mumbles]

687
00:34:41,005 --> 00:34:42,766
through the entire 256
depth, and so we do this

688
00:34:42,766 --> 00:34:44,527
one by one conv slide it over spatially

689
00:34:44,527 --> 00:34:47,793
and we get a feature map
out that's 28 by 28 by one.

690
00:34:47,793 --> 00:34:52,459
There's one number at each
spatial location coming out,

691
00:34:52,460 --> 00:34:54,955
and each filter produces
one of these 28 by 28

692
00:34:54,955 --> 00:34:59,122
by one maps, and we have
here a total 128 filters,

693
00:35:01,860 --> 00:35:05,610
and that's going to
produce 28 by 28, by 128.

694
00:35:06,619 --> 00:35:08,461
OK so if you look at
the number of operations

695
00:35:08,461 --> 00:35:11,213
that are happening in
the convolutional layer,

696
00:35:11,213 --> 00:35:15,951
let's look at the first one for
example this one by one conv

697
00:35:15,951 --> 00:35:20,696
as I was just saying at each
each location we're doing

698
00:35:20,696 --> 00:35:23,363
a one by one by 256 dot product.

699
00:35:25,355 --> 00:35:29,168
So there's 256 multiply
operations happening here

700
00:35:29,168 --> 00:35:32,903
and then for each filter
map we have 28 by 28

701
00:35:32,903 --> 00:35:36,673
spatial locations, so
that's the first 28 times 28

702
00:35:36,673 --> 00:35:38,675
first two numbers that
are multiplied here.

703
00:35:38,675 --> 00:35:41,500
These are the spatial
locations for each filter map,

704
00:35:41,500 --> 00:35:45,921
and so we have to do this
to 25 60 multiplication

705
00:35:45,921 --> 00:35:50,415
each one of these then
we have 128 total filters

706
00:35:50,415 --> 00:35:53,276
at this layer, or we're
producing 128 total

707
00:35:53,276 --> 00:35:54,669
feature maps.

708
00:35:54,669 --> 00:35:58,809
And so the total number
of these operations here

709
00:35:58,809 --> 00:36:00,364
is going to be 28 times 28

710
00:36:00,364 --> 00:36:02,031
times 128 times 256.

711
00:36:02,939 --> 00:36:05,267
And so this is going to be the same for,

712
00:36:05,267 --> 00:36:07,805
you can think about this
for the three by three conv,

713
00:36:07,805 --> 00:36:10,053
and the five by five conv,
that's exactly the same

714
00:36:10,053 --> 00:36:11,159
principle.

715
00:36:11,159 --> 00:36:15,500
And in total we're going to
get 854 million operations

716
00:36:15,500 --> 00:36:17,500
that are happening here.

717
00:36:18,778 --> 00:36:22,001
- [Student] And the 128,
192, and 96 are just values

718
00:36:22,001 --> 00:36:22,941
[mumbles]

719
00:36:22,941 --> 00:36:25,841
- Question the 128, 192 and
256 are values that I picked.

720
00:36:25,841 --> 00:36:29,854
Yes, these are not values
that I just came up with.

721
00:36:29,854 --> 00:36:32,553
They are similar to the
ones that you will see

722
00:36:32,553 --> 00:36:36,404
in like a particular
layer of inception net,

723
00:36:36,404 --> 00:36:38,943
so in GoogleNet basically,
each module has a different

724
00:36:38,943 --> 00:36:41,163
set of these kinds of
parameters, and I picked one

725
00:36:41,163 --> 00:36:43,913
that was similar to one of these.

726
00:36:45,899 --> 00:36:48,341
And so this is very expensive
computationally right,

727
00:36:48,341 --> 00:36:49,856
these these operations.

728
00:36:49,856 --> 00:36:51,321
And then the other thing
that I also want to note

729
00:36:51,321 --> 00:36:55,786
is that the pooling layer also
adds to this problem because

730
00:36:55,786 --> 00:36:57,872
it preserves the whole feature depth.

731
00:36:57,872 --> 00:37:02,864
So at every layer your total
depth can only grow right,

732
00:37:02,864 --> 00:37:04,329
you're going to take
the full featured depth

733
00:37:04,329 --> 00:37:07,316
from your pooling layer, as
well as all the additional

734
00:37:07,316 --> 00:37:11,323
feature maps from the conv
layers and add these up together.

735
00:37:11,323 --> 00:37:14,515
So here our input was 256
depth and our output is

736
00:37:14,515 --> 00:37:18,687
672 depth and you're just
going to keep increasing this

737
00:37:18,687 --> 00:37:19,770
as you go up.

738
00:37:22,730 --> 00:37:24,979
So how do we deal with this
and how do we keep this

739
00:37:24,979 --> 00:37:26,251
more manageable?

740
00:37:26,251 --> 00:37:30,161
And so one of the key
insights that GoogleNet used

741
00:37:30,161 --> 00:37:33,140
was that well we can we
can address this by using

742
00:37:33,140 --> 00:37:36,991
bottleneck layers and try and
project these feature maps

743
00:37:36,991 --> 00:37:41,401
to lower dimension before our
our convolutional operations,

744
00:37:41,401 --> 00:37:43,984
so before our expensive layers.

745
00:37:45,817 --> 00:37:47,452
And so what exactly does that mean?

746
00:37:47,452 --> 00:37:51,035
So reminder one by one
convolution, I guess

747
00:37:52,165 --> 00:37:54,529
we were just going through
this but it's taking your input

748
00:37:54,529 --> 00:37:57,630
volume, it's performing a
dot product at each spatial

749
00:37:57,630 --> 00:38:00,951
location and what it does is
it preserves spatial dimension

750
00:38:00,951 --> 00:38:03,970
but it reduces the depth and
it reduces that by projecting

751
00:38:03,970 --> 00:38:06,949
your input depth to a lower dimension.

752
00:38:06,949 --> 00:38:09,075
It just takes it's basically
like a linear combination

753
00:38:09,075 --> 00:38:11,325
of your input feature maps.

754
00:38:13,690 --> 00:38:15,900
And so this main idea is
that it's projecting your

755
00:38:15,900 --> 00:38:19,009
depth down and so the inception module

756
00:38:19,009 --> 00:38:23,553
takes these one by one convs
and adds these at a bunch

757
00:38:23,553 --> 00:38:26,535
of places in these modules
where there's going to be,

758
00:38:26,535 --> 00:38:29,895
in order to alleviate
this expensive compute.

759
00:38:29,895 --> 00:38:33,208
So before the three by three
and five by five conv layers,

760
00:38:33,208 --> 00:38:36,972
it puts in one of these
one by one convolutions.

761
00:38:36,972 --> 00:38:40,042
And then after the
pooling layer it also puts

762
00:38:40,042 --> 00:38:43,125
an additional one by one convolution.

763
00:38:44,094 --> 00:38:46,919
Right so these are the one
by one bottleneck layers

764
00:38:46,919 --> 00:38:48,419
that are added in.

765
00:38:49,372 --> 00:38:51,791
And so how does this change the math

766
00:38:51,791 --> 00:38:53,546
that we were looking at earlier?

767
00:38:53,546 --> 00:38:55,841
So now basically what's
happening is that we still

768
00:38:55,841 --> 00:38:59,399
have the same input here 28 by 28 by 256,

769
00:38:59,399 --> 00:39:03,137
but these one by one convs
are going to reduce the depth

770
00:39:03,137 --> 00:39:07,181
dimension and so you can see
before the three by three

771
00:39:07,181 --> 00:39:09,802
convs, if I put a one by
one conv with 64 filters,

772
00:39:09,802 --> 00:39:12,416
my output from that is going to be,

773
00:39:12,416 --> 00:39:13,666
28 by 28 by 64.

774
00:39:14,994 --> 00:39:18,148
So instead of now going into
the three by three convs

775
00:39:18,148 --> 00:39:21,304
afterwards instead of 28
by 28 by 256 coming in,

776
00:39:21,304 --> 00:39:25,964
we only have a 28 by 28,
by 64 block coming in.

777
00:39:25,964 --> 00:39:30,255
And so this is now
reducing the smaller input

778
00:39:30,255 --> 00:39:32,264
going into these conv
layers, the same thing for

779
00:39:32,264 --> 00:39:35,614
the five by five conv, and
then for the pooling layer,

780
00:39:35,614 --> 00:39:38,976
after the pooling comes
out, we're going to

781
00:39:38,976 --> 00:39:41,309
reduce the depth after this.

782
00:39:42,372 --> 00:39:44,703
And so, if you work out
the math the same way

783
00:39:44,703 --> 00:39:47,202
for all of the convolutional ops here,

784
00:39:47,202 --> 00:39:49,411
adding in now all these one by one convs

785
00:39:49,411 --> 00:39:52,024
on top of the three by
threes and five by fives,

786
00:39:52,024 --> 00:39:57,021
the total number of operations
is 358 million operations,

787
00:39:57,021 --> 00:40:01,060
so it's much less than the
854 million that we had

788
00:40:01,060 --> 00:40:03,309
in the naive version, and
so you can see how you

789
00:40:03,309 --> 00:40:07,440
can use this one by one
conv, and the filter size

790
00:40:07,440 --> 00:40:11,248
for that to control your computation.

791
00:40:11,248 --> 00:40:12,928
Yes, question in the back.

792
00:40:12,928 --> 00:40:16,345
[student speaks off mic]

793
00:40:24,335 --> 00:40:27,317
- Yes, so the question
is, have you looked into

794
00:40:27,317 --> 00:40:30,028
what information might be
lost by doing this one by one

795
00:40:30,028 --> 00:40:31,789
conv at the beginning.

796
00:40:31,789 --> 00:40:35,922
And so there might be
some information loss,

797
00:40:35,922 --> 00:40:38,455
but at the same time if
you're doing these projections

798
00:40:38,455 --> 00:40:41,565
you're taking a linear
combination of these input

799
00:40:41,565 --> 00:40:45,065
feature maps which has redundancy in them,

800
00:40:46,675 --> 00:40:48,433
you're taking combinations of them,

801
00:40:48,433 --> 00:40:50,647
and you're also introducing
an additional non-linearity

802
00:40:50,647 --> 00:40:53,399
after the one by one
conv, so it also actually

803
00:40:53,399 --> 00:40:55,565
helps in that way with
adding a little bit more

804
00:40:55,565 --> 00:41:00,232
depth and so, I don't think
there's a rigorous analysis

805
00:41:00,232 --> 00:41:03,954
of this, but basically in
general this works better

806
00:41:03,954 --> 00:41:08,124
and there's reasons why it helps as well.

807
00:41:08,124 --> 00:41:11,693
OK so here we have, we're
basically using these one by one

808
00:41:11,693 --> 00:41:16,437
convs to help manage our
computational complexity,

809
00:41:16,437 --> 00:41:18,972
and then what GooleNet
does is it takes these

810
00:41:18,972 --> 00:41:20,156
inception modules and it's going to stack

811
00:41:20,156 --> 00:41:21,260
all these together.

812
00:41:21,260 --> 00:41:23,637
So this is a full inception architecture.

813
00:41:23,637 --> 00:41:28,222
And if we look at this a
little bit more detail,

814
00:41:28,222 --> 00:41:29,651
so here I've flipped it,

815
00:41:29,651 --> 00:41:31,701
because it's so big, it's not going to fit

816
00:41:31,701 --> 00:41:33,583
vertically any more on the slide.

817
00:41:33,583 --> 00:41:35,868
So what we start with is
we first have this stem

818
00:41:35,868 --> 00:41:39,717
network, so this is more
the kind of vanilla plain

819
00:41:39,717 --> 00:41:42,012
conv net that we've seen earlier [mumbles]

820
00:41:42,012 --> 00:41:44,066
six sequence of layers.

821
00:41:44,066 --> 00:41:46,888
So conv pool a couple
of convs in another pool

822
00:41:46,888 --> 00:41:49,380
just to get started and then after that

823
00:41:49,380 --> 00:41:52,321
we have all of our different
our multiple inception

824
00:41:52,321 --> 00:41:55,721
modules all stacked on top of each other,

825
00:41:55,721 --> 00:41:59,243
and then on top we have
our classifier output.

826
00:41:59,243 --> 00:42:01,288
And notice here that
they've really removed

827
00:42:01,288 --> 00:42:03,002
the expensive fully connected layers

828
00:42:03,002 --> 00:42:06,887
it turns out that the model
works great without them,

829
00:42:06,887 --> 00:42:09,792
even and you reduce a lot of parameters.

830
00:42:09,792 --> 00:42:12,875
And then what they also have here is,

831
00:42:14,643 --> 00:42:17,057
you can see these couple
of extra stems coming out

832
00:42:17,057 --> 00:42:19,676
and these are auxiliary
classification outputs

833
00:42:19,676 --> 00:42:24,083
and so these are also you know
just a little mini networks

834
00:42:24,083 --> 00:42:27,036
with an average pooling,
a one by one conv,

835
00:42:27,036 --> 00:42:30,027
a couple of fully connected
layers here going to

836
00:42:30,027 --> 00:42:33,360
the soft Max and also a 1000 way SoftMax

837
00:42:35,218 --> 00:42:36,512
with the ImageNet classes.

838
00:42:36,512 --> 00:42:39,376
And so you're actually
using your ImageNet training

839
00:42:39,376 --> 00:42:42,160
classification loss in
three separate places here.

840
00:42:42,160 --> 00:42:46,384
The standard end of the
network, as well as in these

841
00:42:46,384 --> 00:42:49,699
two places earlier on in
the network, and the reason

842
00:42:49,699 --> 00:42:52,562
they do that is just
this is a deep network

843
00:42:52,562 --> 00:42:54,891
and they found that having
these additional auxiliary

844
00:42:54,891 --> 00:42:58,641
classification outputs,
you get more gradient

845
00:43:00,861 --> 00:43:02,950
training injected at the earlier layers,

846
00:43:02,950 --> 00:43:05,771
and so more just helpful signal flowing in

847
00:43:05,771 --> 00:43:08,521
because these intermediate
layers should also be

848
00:43:08,521 --> 00:43:09,354
helpful.

849
00:43:10,323 --> 00:43:11,708
You should be able to do classification

850
00:43:11,708 --> 00:43:14,294
based off some of these as well.

851
00:43:14,294 --> 00:43:17,885
And so this is the full architecture,

852
00:43:17,885 --> 00:43:21,521
there's 22 total layers
with weights and so

853
00:43:21,521 --> 00:43:25,166
within each of these modules
each of those one by one,

854
00:43:25,166 --> 00:43:27,622
three by three, five by
five is a weight layer,

855
00:43:27,622 --> 00:43:30,284
just including all of
these parallel layers,

856
00:43:30,284 --> 00:43:34,451
and in general it's a relatively
more carefully designed

857
00:43:38,470 --> 00:43:41,908
architecture and part of this
is based on some of these

858
00:43:41,908 --> 00:43:44,938
intuitions that we're talking
about and part of them

859
00:43:44,938 --> 00:43:48,387
also is just you know
Google the authors they had

860
00:43:48,387 --> 00:43:51,046
huge clusters and they're
cross validating across

861
00:43:51,046 --> 00:43:53,826
all kinds of design
choices and this is what

862
00:43:53,826 --> 00:43:56,321
ended up working well.

863
00:43:56,321 --> 00:43:57,915
Question?

864
00:43:57,915 --> 00:44:01,332
[student speaks off mic]

865
00:44:25,252 --> 00:44:28,071
- Yeah so the question is,
are the auxiliary outputs

866
00:44:28,071 --> 00:44:31,224
actually useful for the
final classification,

867
00:44:31,224 --> 00:44:33,267
to use these as well?

868
00:44:33,267 --> 00:44:35,026
I think when they're training them

869
00:44:35,026 --> 00:44:38,214
they do average all these
for the losses coming out.

870
00:44:38,214 --> 00:44:39,974
I think they are helpful.

871
00:44:39,974 --> 00:44:42,180
I can't remember if in
the final architecture,

872
00:44:42,180 --> 00:44:44,765
whether they average all
of these or just take one,

873
00:44:44,765 --> 00:44:47,178
it seems very possible that
they would use all of them,

874
00:44:47,178 --> 00:44:50,082
but you'll need to check on that.

875
00:44:50,082 --> 00:44:53,499
[student speaks off mic]

876
00:44:59,162 --> 00:45:01,411
- So the question is for
the bottleneck layers,

877
00:45:01,411 --> 00:45:05,389
is it possible to use some
other types of dimensionality

878
00:45:05,389 --> 00:45:09,518
reduction and yes you can use
other kinds of dimensionality

879
00:45:09,518 --> 00:45:11,029
reduction.

880
00:45:11,029 --> 00:45:13,893
The benefits here of
this one by one conv is,

881
00:45:13,893 --> 00:45:16,232
you're getting this effect,
but it's all, you know

882
00:45:16,232 --> 00:45:17,948
it's a conv layer just like any other.

883
00:45:17,948 --> 00:45:19,664
You have the soul network of these,

884
00:45:19,664 --> 00:45:21,350
you just train it this full network

885
00:45:21,350 --> 00:45:23,646
back [mumbles] through everything,

886
00:45:23,646 --> 00:45:25,157
and it's learning how to combine the

887
00:45:25,157 --> 00:45:26,990
previous feature maps.

888
00:45:29,411 --> 00:45:31,540
Okay yeah, question in the back.

889
00:45:31,540 --> 00:45:34,957
[student speaks off mic]

890
00:45:36,617 --> 00:45:40,284
- Yes so, question is
are any weights shared

891
00:45:41,969 --> 00:45:43,359
or all they all separate and yeah,

892
00:45:43,359 --> 00:45:46,352
all of these layers have separate weights.

893
00:45:46,352 --> 00:45:47,500
Question.

894
00:45:47,500 --> 00:45:50,917
[student speaks off mic]

895
00:45:57,594 --> 00:45:59,273
- Yes so the question is why do we have

896
00:45:59,273 --> 00:46:00,953
to inject gradients at earlier layers?

897
00:46:00,953 --> 00:46:04,703
So our classification
output at the very end,

898
00:46:06,517 --> 00:46:08,896
where we get a gradient on this, it's

899
00:46:08,896 --> 00:46:10,409
passed all the way back
through the chain roll

900
00:46:10,409 --> 00:46:12,824
but the problem is when
you have very deep networks

901
00:46:12,824 --> 00:46:15,282
and you're going all the
way back through these,

902
00:46:15,282 --> 00:46:18,916
some of this gradient
signal can become minimized

903
00:46:18,916 --> 00:46:21,988
and lost closer to the beginning,
and so that's why having

904
00:46:21,988 --> 00:46:25,588
these additional ones in earlier parts

905
00:46:25,588 --> 00:46:29,187
can help provide some additional signal.

906
00:46:29,187 --> 00:46:33,477
[student mumbles off mic]

907
00:46:33,477 --> 00:46:35,486
- So the question is are you
doing back prop all the times

908
00:46:35,486 --> 00:46:36,663
for each output.

909
00:46:36,663 --> 00:46:38,824
No it's just one back
prop all the way through,

910
00:46:38,824 --> 00:46:42,256
and you can think of these three,

911
00:46:42,256 --> 00:46:44,790
you can think of there being kind of like

912
00:46:44,790 --> 00:46:46,754
an addition at the end
of these if you were to

913
00:46:46,754 --> 00:46:48,885
draw up your computational
graph, and so you get your

914
00:46:48,885 --> 00:46:52,364
final signal and you can
just take all of these

915
00:46:52,364 --> 00:46:54,814
gradients and just back plot
them all the way through.

916
00:46:54,814 --> 00:46:57,719
So it's as if they were
added together at the end

917
00:46:57,719 --> 00:46:59,780
in a computational graph.

918
00:46:59,780 --> 00:47:02,066
OK so in the interest of
time because we still have

919
00:47:02,066 --> 00:47:06,233
a lot to get through, can
take other questions offline.

920
00:47:08,163 --> 00:47:11,330
Okay so GoogleNet basically 22 layers.

921
00:47:12,251 --> 00:47:14,627
It has an efficient inception module,

922
00:47:14,627 --> 00:47:16,793
there's no fully connected layers.

923
00:47:16,793 --> 00:47:18,919
12 times fewer parameters than AlexNet,

924
00:47:18,919 --> 00:47:22,836
and it's the ILSVRC 2014
classification winner.

925
00:47:26,038 --> 00:47:28,775
And so now let's look at the 2015 winner,

926
00:47:28,775 --> 00:47:31,679
which is the ResNet network and so here

927
00:47:31,679 --> 00:47:35,964
this idea is really, this
revolution of depth net right.

928
00:47:35,964 --> 00:47:39,149
We were starting to increase
depth in 2014, and here we've

929
00:47:39,149 --> 00:47:43,477
just had this hugely
deeper model at 152 layers

930
00:47:43,477 --> 00:47:46,426
was the ResNet architecture.

931
00:47:46,426 --> 00:47:49,656
And so now let's look at that
in a little bit more detail.

932
00:47:49,656 --> 00:47:52,521
So the ResNet architecture,
is getting extremely

933
00:47:52,521 --> 00:47:55,096
deep networks, much deeper
than any other networks

934
00:47:55,096 --> 00:47:58,534
before and it's doing this using this idea

935
00:47:58,534 --> 00:48:01,289
of residual connections
which we'll talk about.

936
00:48:01,289 --> 00:48:04,968
And so, they had 152
layer model for ImageNet.

937
00:48:04,968 --> 00:48:08,779
They were able to get 3.5
of 7% top 5 error with this

938
00:48:08,779 --> 00:48:11,927
and the really special
thing is that they swept

939
00:48:11,927 --> 00:48:15,004
all classification and
detection contests in the

940
00:48:15,004 --> 00:48:17,862
ImageNet mart benchmark
and this other benchmark

941
00:48:17,862 --> 00:48:18,924
called COCO.

942
00:48:18,924 --> 00:48:20,189
It just basically won everything.

943
00:48:20,189 --> 00:48:24,356
So it was just clearly
better than everything else.

944
00:48:25,865 --> 00:48:29,258
And so now let's go into a
little bit of the motivation

945
00:48:29,258 --> 00:48:31,869
behind ResNet and residual connections

946
00:48:31,869 --> 00:48:33,348
that we'll talk about.

947
00:48:33,348 --> 00:48:36,504
And the question that they
started off by trying to answer

948
00:48:36,504 --> 00:48:39,769
is what happens when we try
and stack deeper and deeper

949
00:48:39,769 --> 00:48:42,749
layers on a plain
convolutional neural network?

950
00:48:42,749 --> 00:48:44,584
So if we take something like VGG

951
00:48:44,584 --> 00:48:47,937
or some normal network that's
just stacks of conv and

952
00:48:47,937 --> 00:48:50,517
pool layers on top of each
other can we just continuously

953
00:48:50,517 --> 00:48:54,684
extend these, get deeper
layers and just do better?

954
00:48:56,411 --> 00:48:59,231
And and the answer is no.

955
00:48:59,231 --> 00:49:00,832
So if you so if you look at what happens

956
00:49:00,832 --> 00:49:03,242
when you get deeper, so here
I'm comparing a 20 layer

957
00:49:03,242 --> 00:49:07,409
network and a 56 layer network
and so this is just a plain

958
00:49:10,308 --> 00:49:12,718
kind of network you'll see
that in the test error here

959
00:49:12,718 --> 00:49:15,864
on the right the 56 layer
network is doing worse

960
00:49:15,864 --> 00:49:17,627
than the 28 layer network.

961
00:49:17,627 --> 00:49:20,581
So the deeper network was
not able to do better.

962
00:49:20,581 --> 00:49:23,748
But then the really weird thing is now

963
00:49:24,639 --> 00:49:27,178
if you look at the training error right

964
00:49:27,178 --> 00:49:28,778
we here have again the 20 layer network

965
00:49:28,778 --> 00:49:30,490
and a 56 layer network.

966
00:49:30,490 --> 00:49:33,927
The 56 layer network, one of
the obvious problems you think,

967
00:49:33,927 --> 00:49:38,094
I have a really deep network,
I have tons of parameters

968
00:49:39,117 --> 00:49:42,104
maybe it's probably starting
to over fit at some point.

969
00:49:42,104 --> 00:49:44,682
But what actually happens is
that when you're over fitting

970
00:49:44,682 --> 00:49:46,276
you would expect to have very good,

971
00:49:46,276 --> 00:49:49,795
very low training error rate,
and just bad test error,

972
00:49:49,795 --> 00:49:51,995
but what's happening here is
that in the training error

973
00:49:51,995 --> 00:49:54,571
the 56 layer network is
also doing worse than

974
00:49:54,571 --> 00:49:56,321
the 20 layer network.

975
00:49:57,643 --> 00:49:59,438
And so even though the
deeper model performs worse,

976
00:49:59,438 --> 00:50:02,355
this is not caused by over-fitting.

977
00:50:04,272 --> 00:50:07,957
And so the hypothesis
of the ResNet creators

978
00:50:07,957 --> 00:50:11,063
is that the problem is actually
an optimization problem.

979
00:50:11,063 --> 00:50:14,171
Deeper models are just harder to optimize,

980
00:50:14,171 --> 00:50:16,421
than more shallow networks.

981
00:50:17,645 --> 00:50:19,317
And the reasoning was that well,

982
00:50:19,317 --> 00:50:21,489
a deeper model should be
able to perform at least

983
00:50:21,489 --> 00:50:24,073
as well as a shallower model.

984
00:50:24,073 --> 00:50:26,238
You can have actually a
solution by construction

985
00:50:26,238 --> 00:50:28,645
where you just take the learned layers

986
00:50:28,645 --> 00:50:30,486
from your shallower model, you just

987
00:50:30,486 --> 00:50:33,140
copy these over and then
for the remaining additional

988
00:50:33,140 --> 00:50:36,002
deeper layers you just
add identity mappings.

989
00:50:36,002 --> 00:50:38,902
So by construction this
should be working just as well

990
00:50:38,902 --> 00:50:40,343
as the shallower layer.

991
00:50:40,343 --> 00:50:43,087
And your model that weren't
able to learn properly,

992
00:50:43,087 --> 00:50:47,105
it should be able to learn at least this.

993
00:50:47,105 --> 00:50:50,688
And so motivated by
this their solution was

994
00:50:52,418 --> 00:50:56,994
well how can we make it
easier for our architecture,

995
00:50:56,994 --> 00:50:59,896
our model to learn these
kinds of solutions,

996
00:50:59,896 --> 00:51:01,404
or at least something like this?

997
00:51:01,404 --> 00:51:06,303
And so their idea is well
instead of just stacking

998
00:51:06,303 --> 00:51:08,675
all these layers on top
of each other and having

999
00:51:08,675 --> 00:51:12,604
every layer try and learn
some underlying mapping

1000
00:51:12,604 --> 00:51:17,564
of a desired function, lets
instead have these blocks,

1001
00:51:17,564 --> 00:51:20,472
where we try and fit a residual mapping,

1002
00:51:20,472 --> 00:51:22,518
instead of a direct mapping.

1003
00:51:22,518 --> 00:51:24,863
And so what this looks
like is here on this right

1004
00:51:24,863 --> 00:51:29,030
where the input to these block
is just the input coming in

1005
00:51:30,628 --> 00:51:34,795
and here we are going to
use our, here on the side,

1006
00:51:38,148 --> 00:51:41,051
we're going to use our
layers to try and fit

1007
00:51:41,051 --> 00:51:44,218
some residual of our desire to H of X,

1008
00:51:45,142 --> 00:51:49,309
minus X instead of the desired
function H of X directly.

1009
00:51:50,260 --> 00:51:53,814
And so basically at the
end of this block we take

1010
00:51:53,814 --> 00:51:56,637
the step connection on
this right here, this loop,

1011
00:51:56,637 --> 00:51:59,787
where we just take our input,
we just use pass it through

1012
00:51:59,787 --> 00:52:03,346
as an identity, and so if
we had no weight layers

1013
00:52:03,346 --> 00:52:05,102
in between it was just
going to be the identity

1014
00:52:05,102 --> 00:52:08,051
it would be the same thing
as the output, but now we use

1015
00:52:08,051 --> 00:52:10,955
our additional weight
layers to learn some delta,

1016
00:52:10,955 --> 00:52:13,372
for some residual from our X.

1017
00:52:14,877 --> 00:52:16,604
And so now the output
of this is going to be

1018
00:52:16,604 --> 00:52:19,937
just our original R X plus some residual

1019
00:52:20,895 --> 00:52:22,003
that we're going to call it.

1020
00:52:22,003 --> 00:52:25,312
It's basically a delta
and so the idea is that

1021
00:52:25,312 --> 00:52:29,238
now the output it should
be easy for example,

1022
00:52:29,238 --> 00:52:32,238
in the case where identity is ideal,

1023
00:52:33,320 --> 00:52:37,078
to just squash all of
these weights of F of X

1024
00:52:37,078 --> 00:52:40,059
from our weight layers
just set it to all zero

1025
00:52:40,059 --> 00:52:42,308
for example, then we're
just going to get identity

1026
00:52:42,308 --> 00:52:44,319
as the output, and we can get something,

1027
00:52:44,319 --> 00:52:47,831
for example, close to this
solution by construction

1028
00:52:47,831 --> 00:52:49,388
that we had earlier.

1029
00:52:49,388 --> 00:52:51,314
Right, so this is just
a network architecture

1030
00:52:51,314 --> 00:52:53,448
that says okay, let's try and fit this,

1031
00:52:53,448 --> 00:52:57,574
learn how our weight layers
residual, and be something

1032
00:52:57,574 --> 00:53:01,772
close, that way it'll more
likely be something close to X,

1033
00:53:01,772 --> 00:53:04,151
it's just modifying X,
than to learn exactly

1034
00:53:04,151 --> 00:53:06,198
this full mapping of what it should be.

1035
00:53:06,198 --> 00:53:09,059
Okay, any questions about this?

1036
00:53:09,059 --> 00:53:09,999
[student speaks off mic]

1037
00:53:09,999 --> 00:53:13,499
- Question is is there the same dimension?

1038
00:53:14,580 --> 00:53:18,413
So yes these two paths
are the same dimension.

1039
00:53:19,562 --> 00:53:22,383
In general either it's the same dimension,

1040
00:53:22,383 --> 00:53:24,666
or what they actually
do is they have these

1041
00:53:24,666 --> 00:53:27,571
projections and shortcuts
and they have different ways

1042
00:53:27,571 --> 00:53:31,790
of padding to make things work
out to be the same dimension.

1043
00:53:31,790 --> 00:53:33,098
Depth wise.

1044
00:53:33,098 --> 00:53:34,205
Yes

1045
00:53:34,205 --> 00:53:35,763
- [Student] When you use the word residual

1046
00:53:35,763 --> 00:53:39,930
you were talking about [mumbles off mic]

1047
00:53:46,667 --> 00:53:49,407
- So the question is what
exactly do we mean by

1048
00:53:49,407 --> 00:53:52,968
residual this output
of this transformation

1049
00:53:52,968 --> 00:53:54,448
is a residual?

1050
00:53:54,448 --> 00:53:58,419
So we can think of our output
here right as this F of X

1051
00:53:58,419 --> 00:54:02,709
plus X, where F of X is the
output of our transformation

1052
00:54:02,709 --> 00:54:06,346
and then X is our input,
just passed through

1053
00:54:06,346 --> 00:54:07,460
by the identity.

1054
00:54:07,460 --> 00:54:10,605
So we'd like to using a plain layer,

1055
00:54:10,605 --> 00:54:12,854
what we're trying to do is learn something

1056
00:54:12,854 --> 00:54:16,287
like H of X, but what we saw
earlier is that it's hard

1057
00:54:16,287 --> 00:54:18,008
to learn H of X.

1058
00:54:18,008 --> 00:54:21,481
It's a good H of X as we
get very deep networks.

1059
00:54:21,481 --> 00:54:23,771
And so here the idea is
let's try and break it down

1060
00:54:23,771 --> 00:54:27,301
instead of as H of X is
equal to F of X plus,

1061
00:54:27,301 --> 00:54:30,248
and let's just try and learn F of X.

1062
00:54:30,248 --> 00:54:33,648
And so instead of learning
directly this H of X

1063
00:54:33,648 --> 00:54:36,341
we just want to learn what
is it that we need to add

1064
00:54:36,341 --> 00:54:40,551
or subtract to our input as
we move on to the next layer.

1065
00:54:40,551 --> 00:54:44,160
So you can think of it as
kind of modifying this input,

1066
00:54:44,160 --> 00:54:45,469
in place in a sense.

1067
00:54:45,469 --> 00:54:46,699
We have--

1068
00:54:46,699 --> 00:54:49,931
[interrupted by student mumbling off mic]

1069
00:54:49,931 --> 00:54:51,654
- The question is, when we're
saying the word residual

1070
00:54:51,654 --> 00:54:53,248
are we talking about F of X?

1071
00:54:53,248 --> 00:54:54,357
Yeah.

1072
00:54:54,357 --> 00:54:56,522
So F of X is what we're
calling the residual.

1073
00:54:56,522 --> 00:54:58,939
And it just has that meaning.

1074
00:55:02,287 --> 00:55:04,751
Yes another question.

1075
00:55:04,751 --> 00:55:08,251
[student mumbles off mic]

1076
00:55:12,129 --> 00:55:14,334
- So the question is in
practice do we just sum

1077
00:55:14,334 --> 00:55:17,480
F of X and X together, or
do we learn some weighted

1078
00:55:17,480 --> 00:55:20,955
combination and you just do a direct sum.

1079
00:55:20,955 --> 00:55:23,599
Because when you do a direct sum,

1080
00:55:23,599 --> 00:55:26,869
this is the idea of let
me just learn what is it

1081
00:55:26,869 --> 00:55:29,619
I have to add or subtract onto X.

1082
00:55:31,462 --> 00:55:35,273
Is this clear to everybody,
the main intuition?

1083
00:55:35,273 --> 00:55:36,171
Question.

1084
00:55:36,171 --> 00:55:39,588
[student speaks off mic]

1085
00:55:41,531 --> 00:55:43,865
- Yeah, so the question
is not clear why is it

1086
00:55:43,865 --> 00:55:45,823
that learning the
residual should be easier

1087
00:55:45,823 --> 00:55:47,909
than learning the direct mapping?

1088
00:55:47,909 --> 00:55:50,609
And so this is just their hypotheses,

1089
00:55:50,609 --> 00:55:55,181
and a hypotheses is that if
we're learning the residual

1090
00:55:55,181 --> 00:55:59,557
you just have to learn
what's the delta to X right?

1091
00:55:59,557 --> 00:56:02,947
And if our hypotheses is that generally

1092
00:56:02,947 --> 00:56:07,169
even something like our
solution by construction,

1093
00:56:07,169 --> 00:56:10,690
where we had some number
of these shallow layers

1094
00:56:10,690 --> 00:56:13,718
that were learned and we had
all these identity mappings

1095
00:56:13,718 --> 00:56:16,911
at the top this was a
solution that should have been

1096
00:56:16,911 --> 00:56:20,219
good, and so that implies that
maybe a lot of these layers,

1097
00:56:20,219 --> 00:56:23,157
actually something just close to identity,

1098
00:56:23,157 --> 00:56:24,795
would be a good layer

1099
00:56:24,795 --> 00:56:27,092
And so because of that,
now we formulate this

1100
00:56:27,092 --> 00:56:30,168
as being able to learn the identity

1101
00:56:30,168 --> 00:56:31,764
plus just a little delta.

1102
00:56:31,764 --> 00:56:35,125
And if really the identity
is best we just make

1103
00:56:35,125 --> 00:56:37,573
F of X squashes transformation
to just be zero,

1104
00:56:37,573 --> 00:56:39,662
which is something that's relatively,

1105
00:56:39,662 --> 00:56:41,173
might seem easier to learn,

1106
00:56:41,173 --> 00:56:43,509
also we're able to get
things that are close

1107
00:56:43,509 --> 00:56:45,594
to identity mappings.

1108
00:56:45,594 --> 00:56:47,927
And so again this is not
something that's necessarily

1109
00:56:47,927 --> 00:56:51,776
proven or anything it's just
the intuition and hypothesis,

1110
00:56:51,776 --> 00:56:54,848
and then we'll also see
later some works where people

1111
00:56:54,848 --> 00:56:56,533
are actually trying to
challenge this and say oh maybe

1112
00:56:56,533 --> 00:56:59,518
it's not actually the residuals
that are so necessary,

1113
00:56:59,518 --> 00:57:03,069
but at least this is the
hypothesis for this paper,

1114
00:57:03,069 --> 00:57:06,105
and in practice using this model,

1115
00:57:06,105 --> 00:57:08,317
it was able to do very well.

1116
00:57:08,317 --> 00:57:09,620
Question.

1117
00:57:09,620 --> 00:57:13,037
[student speaks off mic]

1118
00:57:42,623 --> 00:57:45,394
- Yes so the question is
have people tried other ways

1119
00:57:45,394 --> 00:57:49,938
of combining the inputs
from previous layers and yes

1120
00:57:49,938 --> 00:57:52,556
so this is basically a very
active area of research

1121
00:57:52,556 --> 00:57:55,018
on and how we formulate
all these connections,

1122
00:57:55,018 --> 00:57:57,557
and what's connected to what
in all of these structures.

1123
00:57:57,557 --> 00:58:00,174
So we'll see a few more
examples of different network

1124
00:58:00,174 --> 00:58:04,505
architectures briefly later
but this is an active area

1125
00:58:04,505 --> 00:58:05,505
of research.

1126
00:58:06,468 --> 00:58:10,245
OK so we basically have all
of these residual blocks

1127
00:58:10,245 --> 00:58:12,903
that are stacked on top of each other.

1128
00:58:12,903 --> 00:58:15,598
We can see the full resident architecture.

1129
00:58:15,598 --> 00:58:18,750
Each of these residual blocks
has two three by three conv

1130
00:58:18,750 --> 00:58:23,285
layers as part of this block
and there's also been work

1131
00:58:23,285 --> 00:58:26,479
just saying that this happens
to be a good configuration

1132
00:58:26,479 --> 00:58:28,109
that works well.

1133
00:58:28,109 --> 00:58:30,638
We stack all these blocks
together very deeply.

1134
00:58:30,638 --> 00:58:34,161
Another thing like with
this very deep architecture

1135
00:58:34,161 --> 00:58:37,911
it's basically also
enabling up to 150 layers

1136
00:58:42,231 --> 00:58:45,981
deep of this, and then
what we do is we stack

1137
00:58:47,392 --> 00:58:49,725
all these and periodically we also double

1138
00:58:49,725 --> 00:58:52,042
the number of filters
and down sample spatially

1139
00:58:52,042 --> 00:58:54,792
using stride two when we do that.

1140
00:58:56,666 --> 00:58:59,319
And then we have this additional [mumbles]

1141
00:58:59,319 --> 00:59:02,835
at the very beginning of our network

1142
00:59:02,835 --> 00:59:03,668
and at the end we also hear,

1143
00:59:03,668 --> 00:59:04,677
don't have any fully connected layers

1144
00:59:04,677 --> 00:59:06,844
and we just have a global
average pooling layer

1145
00:59:06,844 --> 00:59:09,451
that's going to average
over everything spatially,

1146
00:59:09,451 --> 00:59:13,618
and then be input into the
last 1000 way classification.

1147
00:59:15,504 --> 00:59:17,801
So this is the full ResNet architecture

1148
00:59:17,801 --> 00:59:20,333
and it's very simple and
elegant just stacking up

1149
00:59:20,333 --> 00:59:22,745
all of these ResNet blocks
on top of each other,

1150
00:59:22,745 --> 00:59:27,032
and they have total depths
of up to 34, 50, 100,

1151
00:59:27,032 --> 00:59:30,199
and they tried up to 152 for ImageNet.

1152
00:59:35,040 --> 00:59:39,369
OK so one additional
thing just to know is that

1153
00:59:39,369 --> 00:59:41,869
for a very deep network,
so the ones that are more

1154
00:59:41,869 --> 00:59:44,774
than 50 layers deep, they
also use bottleneck layers

1155
00:59:44,774 --> 00:59:47,473
similar to what GoogleNet did
in order to improve efficiency

1156
00:59:47,473 --> 00:59:51,865
and so within each block
now you're going to,

1157
00:59:51,865 --> 00:59:54,645
what they did is, have this
one by one conv filter,

1158
00:59:54,645 --> 00:59:58,005
that first projects it
down to a smaller depth.

1159
00:59:58,005 --> 01:00:01,774
So again if we are looking
at let's say 28 by 28

1160
01:00:01,774 --> 01:00:05,087
by 256 implant, we do
this one by one conv,

1161
01:00:05,087 --> 01:00:06,926
it's taking it's
projecting the depth down.

1162
01:00:06,926 --> 01:00:08,759
We get 28 by 28 by 64.

1163
01:00:09,917 --> 01:00:12,220
Now your convolution
your three by three conv,

1164
01:00:12,220 --> 01:00:15,809
in here they only have
one, is operating over this

1165
01:00:15,809 --> 01:00:19,296
reduced step so it's going
to be less expensive,

1166
01:00:19,296 --> 01:00:21,881
and then afterwards they have another

1167
01:00:21,881 --> 01:00:24,621
one by one conv that
projects the depth back up

1168
01:00:24,621 --> 01:00:28,180
to 256, and so, this is
the actual block that

1169
01:00:28,180 --> 01:00:30,680
you'll see in deeper networks.

1170
01:00:33,831 --> 01:00:38,618
So in practice the ResNet
also uses batch normalization

1171
01:00:38,618 --> 01:00:42,092
after every conv layer, they
use Xavier initialization

1172
01:00:42,092 --> 01:00:46,259
with an extra scaling factor
that they helped introduce

1173
01:00:47,221 --> 01:00:51,388
to improve the initialization
trained with SGD + momentum.

1174
01:00:52,414 --> 01:00:54,622
Their learning rate they
use a similar learning rate

1175
01:00:54,622 --> 01:00:57,280
type of schedule where you
decay your learning rate

1176
01:00:57,280 --> 01:01:00,280
when your validation error plateaus.

1177
01:01:02,561 --> 01:01:05,351
Mini batch size 256, a
little bit of weight decay

1178
01:01:05,351 --> 01:01:06,684
and no drop out.

1179
01:01:08,455 --> 01:01:10,990
And so experimentally they
were able to show that they

1180
01:01:10,990 --> 01:01:12,710
were able to train these
very deep networks,

1181
01:01:12,710 --> 01:01:14,391
without degrading.

1182
01:01:14,391 --> 01:01:17,582
They were able to have
basically good gradient flow

1183
01:01:17,582 --> 01:01:19,870
coming all the way back
down through the network.

1184
01:01:19,870 --> 01:01:23,435
They tried up to 152 layers on ImageNet,

1185
01:01:23,435 --> 01:01:27,442
1200 on Cifar, which is a,
you have played with it,

1186
01:01:27,442 --> 01:01:31,525
but a smaller data set
and they also saw that now

1187
01:01:32,804 --> 01:01:35,556
you're deeper networks are
able to achieve lower training

1188
01:01:35,556 --> 01:01:37,113
errors as expected.

1189
01:01:37,113 --> 01:01:40,228
So you don't have the same strange plots

1190
01:01:40,228 --> 01:01:43,841
that we saw earlier where the behavior

1191
01:01:43,841 --> 01:01:45,353
was in the wrong direction.

1192
01:01:45,353 --> 01:01:48,260
And so from here they were
able to sweep first place

1193
01:01:48,260 --> 01:01:50,344
at all of the ILSVRC competitions,

1194
01:01:50,344 --> 01:01:53,570
and all of the COCO competitions in 2015

1195
01:01:53,570 --> 01:01:55,653
by a significant margins.

1196
01:01:56,962 --> 01:02:01,574
Their total top five error
was 3.6 % for a classification

1197
01:02:01,574 --> 01:02:05,626
and this is actually better
than human performance

1198
01:02:05,626 --> 01:02:07,459
in the ImageNet paper.

1199
01:02:09,712 --> 01:02:12,864
There was also a human
metric that came from

1200
01:02:12,864 --> 01:02:17,031
actually [mumbles] our
lab Andre Kapathy spent

1201
01:02:18,023 --> 01:02:21,940
like a week training
himself and then basically

1202
01:02:23,127 --> 01:02:25,540
did all of, did this task himself

1203
01:02:25,540 --> 01:02:29,341
and was I think somewhere around 5-ish %,

1204
01:02:29,341 --> 01:02:31,584
and so I was basically able to do

1205
01:02:31,584 --> 01:02:35,001
better than the then that human at least.

1206
01:02:36,985 --> 01:02:41,072
Okay, so these are kind
of the main networks

1207
01:02:41,072 --> 01:02:42,879
that have been used recently.

1208
01:02:42,879 --> 01:02:45,211
We had AlexNet starting off with first,

1209
01:02:45,211 --> 01:02:48,814
VGG and GoogleNet are still very popular,

1210
01:02:48,814 --> 01:02:51,967
but ResNet is the most
recent best performing model

1211
01:02:51,967 --> 01:02:56,100
that if you're looking for
something training a new network

1212
01:02:56,100 --> 01:02:57,695
ResNet is available, you should try

1213
01:02:57,695 --> 01:02:59,028
working with it.

1214
01:03:00,964 --> 01:03:04,844
So just quickly looking at
some of this getting a better

1215
01:03:04,844 --> 01:03:07,213
sense of the complexity involved.

1216
01:03:07,213 --> 01:03:09,098
So here we have some
plots that are sorted by

1217
01:03:09,098 --> 01:03:12,848
performance so this is
top one accuracy here,

1218
01:03:14,003 --> 01:03:16,085
and higher is better.

1219
01:03:16,085 --> 01:03:17,887
And so you'll see a lot
of these models that we

1220
01:03:17,887 --> 01:03:19,817
talked about, as well as
some different versions

1221
01:03:19,817 --> 01:03:22,350
of them so, this
GoogleNet inception thing,

1222
01:03:22,350 --> 01:03:26,266
I think there's like V2,
V3 and the best one here

1223
01:03:26,266 --> 01:03:29,135
is V4, which is actually
a ResNet plus inception

1224
01:03:29,135 --> 01:03:32,199
combination, so these are just kind of

1225
01:03:32,199 --> 01:03:34,820
more incremental, smaller
changes that they've

1226
01:03:34,820 --> 01:03:37,555
built on top of them,
and so that's the best

1227
01:03:37,555 --> 01:03:39,969
performing model here.

1228
01:03:39,969 --> 01:03:42,756
And if we look on the
right, these plots of their

1229
01:03:42,756 --> 01:03:46,256
computational complexity here it's sorted.

1230
01:03:48,496 --> 01:03:51,609
The Y axis is your top one accuracy

1231
01:03:51,609 --> 01:03:53,123
so higher is better.

1232
01:03:53,123 --> 01:03:57,223
The X axis is your operations
and so the more to the right,

1233
01:03:57,223 --> 01:03:59,749
the more ops you're doing,
the more computationally

1234
01:03:59,749 --> 01:04:02,456
expensive and then the bigger the circle,

1235
01:04:02,456 --> 01:04:03,884
your circle is your memory usage,

1236
01:04:03,884 --> 01:04:06,097
so the gray circles are referenced here,

1237
01:04:06,097 --> 01:04:08,061
but the bigger the circle
the more memory usage

1238
01:04:08,061 --> 01:04:11,824
and so here we can see
that VGG these green ones

1239
01:04:11,824 --> 01:04:14,152
are kind of the least efficient.

1240
01:04:14,152 --> 01:04:15,909
They have the biggest memory,

1241
01:04:15,909 --> 01:04:17,016
the most operations,

1242
01:04:17,016 --> 01:04:19,433
but they they do pretty well.

1243
01:04:20,648 --> 01:04:23,221
GoogleNet is the most efficient here.

1244
01:04:23,221 --> 01:04:25,757
It's way down on the operation side,

1245
01:04:25,757 --> 01:04:30,085
as well as a small little
circle for memory usage.

1246
01:04:30,085 --> 01:04:34,059
AlexNet, our earlier
model, has lowest accuracy.

1247
01:04:34,059 --> 01:04:35,985
It's relatively smaller compute, because

1248
01:04:35,985 --> 01:04:38,804
it's a smaller network, but
it's also not particularly

1249
01:04:38,804 --> 01:04:40,221
memory efficient.

1250
01:04:42,119 --> 01:04:47,026
And then ResNet here, we
have moderate efficiency.

1251
01:04:47,026 --> 01:04:49,310
It's kind of in the middle,
both in terms of memory

1252
01:04:49,310 --> 01:04:53,310
and operations, and it
has the highest accuracy.

1253
01:04:56,839 --> 01:04:58,838
And so here also are
some additional plots.

1254
01:04:58,838 --> 01:05:01,618
You can look at these
more on your own time,

1255
01:05:01,618 --> 01:05:04,600
but this plot on the left is
showing the forward pass time

1256
01:05:04,600 --> 01:05:07,825
and so this is in milliseconds
and you can up at the top

1257
01:05:07,825 --> 01:05:11,253
VGG forward passes about 200
milliseconds you can get about

1258
01:05:11,253 --> 01:05:13,466
five frames per second with this,

1259
01:05:13,466 --> 01:05:15,678
and this is sorted in order.

1260
01:05:15,678 --> 01:05:18,459
There's also this plot on
the right looking at power

1261
01:05:18,459 --> 01:05:22,584
consumption and if you look
more at this paper here,

1262
01:05:22,584 --> 01:05:25,693
there's further analysis of
these kinds of computational

1263
01:05:25,693 --> 01:05:26,693
comparisons.

1264
01:05:31,414 --> 01:05:34,318
So these were the main
architectures that you should

1265
01:05:34,318 --> 01:05:36,776
really know in-depth and be familiar with,

1266
01:05:36,776 --> 01:05:39,560
and be thinking about actively using.

1267
01:05:39,560 --> 01:05:41,278
But now I'm going just
to go briefly through

1268
01:05:41,278 --> 01:05:43,156
some other architectures
that are just good

1269
01:05:43,156 --> 01:05:46,323
to know either historical inspirations

1270
01:05:47,290 --> 01:05:50,040
or more recent areas of research.

1271
01:05:51,526 --> 01:05:53,569
So the first one Network in Network,

1272
01:05:53,569 --> 01:05:57,152
this is from 2014, and
the idea behind this

1273
01:06:01,339 --> 01:06:06,334
is that we have these
vanilla convolutional layers

1274
01:06:06,334 --> 01:06:10,261
but we also have these,
this introduces the idea of

1275
01:06:10,261 --> 01:06:13,493
MLP conv layers they call
it, which are micro networks

1276
01:06:13,493 --> 01:06:15,497
or basically network within networth, the

1277
01:06:15,497 --> 01:06:16,928
name of the paper.

1278
01:06:16,928 --> 01:06:21,088
Where within each conv
layer trying to stack an MLP

1279
01:06:21,088 --> 01:06:23,962
with a couple of fully
connected layers on top of

1280
01:06:23,962 --> 01:06:26,293
just the standard conv
and be able to compute

1281
01:06:26,293 --> 01:06:28,989
more abstract features for these local

1282
01:06:28,989 --> 01:06:29,977
patches right.

1283
01:06:29,977 --> 01:06:32,017
So instead of sliding
just a conv filter around,

1284
01:06:32,017 --> 01:06:36,153
it's sliding a slightly
more complex hierarchical

1285
01:06:36,153 --> 01:06:40,780
set of filters around
and using that to get the

1286
01:06:40,780 --> 01:06:42,785
activation maps.

1287
01:06:42,785 --> 01:06:46,092
And so, it uses these fully connected,

1288
01:06:46,092 --> 01:06:48,751
or basically one by one
conv kind of layers.

1289
01:06:48,751 --> 01:06:51,001
It's going to stack them all up like the

1290
01:06:51,001 --> 01:06:53,701
bottom diagram here where
we just have these networks

1291
01:06:53,701 --> 01:06:58,006
within networks stacked
in each of the layers.

1292
01:06:58,006 --> 01:07:01,475
And the main reason to know this is just

1293
01:07:01,475 --> 01:07:04,825
it was kind of a precursor
to GoogleNet and ResNet

1294
01:07:04,825 --> 01:07:08,584
in 2014 with this idea
of bottleneck layers

1295
01:07:08,584 --> 01:07:10,912
that you saw used very heavily in there.

1296
01:07:10,912 --> 01:07:13,828
And it also had a little bit
of philosophical inspiration

1297
01:07:13,828 --> 01:07:17,388
for GoogleNet for this idea
of a local network typology

1298
01:07:17,388 --> 01:07:19,963
network in network that they also used,

1299
01:07:19,963 --> 01:07:22,880
with a different kind of structure.

1300
01:07:25,048 --> 01:07:29,422
Now I'm going to talk
about a series of works,

1301
01:07:29,422 --> 01:07:32,125
on, or works since ResNet
that are mostly geared

1302
01:07:32,125 --> 01:07:34,655
towards improving resNet
and so this is more recent

1303
01:07:34,655 --> 01:07:37,569
research has been done since then.

1304
01:07:37,569 --> 01:07:38,923
I'm going to go over these pretty fast,

1305
01:07:38,923 --> 01:07:40,721
and so just at a very high level.

1306
01:07:40,721 --> 01:07:42,147
If you're interested in
any of these you should

1307
01:07:42,147 --> 01:07:45,564
look at the papers, to have more details.

1308
01:07:46,565 --> 01:07:50,686
So the authors of ResNet
a little bit later on

1309
01:07:50,686 --> 01:07:54,853
in 2016 also had this paper
where they improved the

1310
01:07:55,876 --> 01:07:57,552
ResNet block design.

1311
01:07:57,552 --> 01:08:01,327
And so they basically
adjusted what were the layers

1312
01:08:01,327 --> 01:08:03,825
that were in the ResNet block path,

1313
01:08:03,825 --> 01:08:07,299
and showed this new
structure was able to have

1314
01:08:07,299 --> 01:08:11,024
a more direct path in order
for propagating information

1315
01:08:11,024 --> 01:08:14,711
throughout the network,
and you want to have a good

1316
01:08:14,711 --> 01:08:16,883
path to propagate
information all the way up,

1317
01:08:16,883 --> 01:08:19,671
and then back up all the way down again.

1318
01:08:19,671 --> 01:08:22,244
And so they showed that this
new block was better for that

1319
01:08:22,245 --> 01:08:26,129
and was able to give better performance.

1320
01:08:26,129 --> 01:08:29,769
There's also a Wide Residual
networks which this paper

1321
01:08:29,769 --> 01:08:34,401
argued that while ResNets
made networks much deeper

1322
01:08:34,401 --> 01:08:36,687
as well as added these
residual connections

1323
01:08:36,687 --> 01:08:40,206
and their argument was
that residuals are really

1324
01:08:40,206 --> 01:08:41,038
the important factor.

1325
01:08:41,038 --> 01:08:42,334
Having this residual construction,

1326
01:08:42,335 --> 01:08:46,100
and not necessarily having
extremely deep networks.

1327
01:08:46,100 --> 01:08:50,277
And so what they did was they
used wider residual blocks,

1328
01:08:50,277 --> 01:08:52,702
and so what this means is
just more filters in every

1329
01:08:52,702 --> 01:08:53,604
conv layer.

1330
01:08:53,604 --> 01:08:56,806
So before we might have
F filters per layer

1331
01:08:56,807 --> 01:09:00,118
and they use these factors
of K and said well,

1332
01:09:00,118 --> 01:09:03,472
every layer it's going to be
F times K filters instead.

1333
01:09:03,473 --> 01:09:07,432
And so, using these
wider layers they showed

1334
01:09:07,432 --> 01:09:09,812
that their 50 layer wide
ResNet was able to out-perform

1335
01:09:09,812 --> 01:09:12,312
the 152 layer original ResNet,

1336
01:09:14,564 --> 01:09:17,099
and it also had the
additional advantages of

1337
01:09:17,099 --> 01:09:21,513
increasing with this,
even with the same amount

1338
01:09:21,513 --> 01:09:23,845
of parameters, tit's more
computationally efficient

1339
01:09:23,845 --> 01:09:26,587
because you can parallelize
these with operations

1340
01:09:26,587 --> 01:09:27,732
more easily.

1341
01:09:27,733 --> 01:09:31,143
Right just convolutions with more neurons

1342
01:09:31,143 --> 01:09:33,560
just spread across more kernels

1343
01:09:33,560 --> 01:09:36,430
as opposed to depth
that's more sequential,

1344
01:09:36,430 --> 01:09:39,416
so it's more computationally
efficient to increase

1345
01:09:39,416 --> 01:09:40,356
your width.

1346
01:09:40,356 --> 01:09:42,104
So here you can see
this work is starting to

1347
01:09:42,104 --> 01:09:44,402
trying to understand the
contributions of width

1348
01:09:44,403 --> 01:09:47,550
and depth and residual connections,

1349
01:09:47,550 --> 01:09:50,627
and making some arguments
for one way versus the other.

1350
01:09:50,627 --> 01:09:54,127
And this other paper around the same time,

1351
01:09:55,874 --> 01:09:58,935
I think maybe a little
bit later, is ResNeXt,

1352
01:09:58,935 --> 01:10:02,601
and so this is again,
the creators of ResNet

1353
01:10:02,601 --> 01:10:05,193
continuing to work on
pushing the architecture.

1354
01:10:05,193 --> 01:10:09,110
And here they also had
this idea of okay, let's

1355
01:10:10,509 --> 01:10:13,742
indeed tackle this width
thing more but instead of just

1356
01:10:13,742 --> 01:10:15,829
increasing the width
of this residual block

1357
01:10:15,829 --> 01:10:19,386
through more filters they have structure.

1358
01:10:19,386 --> 01:10:23,335
And so within each residual
block, multiple parallel

1359
01:10:23,335 --> 01:10:25,344
pathways and they're going to call

1360
01:10:25,344 --> 01:10:27,225
the total number of these
pathways the cardinality.

1361
01:10:27,225 --> 01:10:31,308
And so it's basically
taking the one ResNet block

1362
01:10:33,386 --> 01:10:36,341
with the bottlenecks and having
it be relatively thinner,

1363
01:10:36,341 --> 01:10:39,205
but having multiple of
these done in parallel.

1364
01:10:39,205 --> 01:10:43,373
And so here you can also
see that this both have some

1365
01:10:43,373 --> 01:10:45,262
relation to this idea of wide networks,

1366
01:10:45,262 --> 01:10:50,097
as well as to has some connection
to the inception module

1367
01:10:50,097 --> 01:10:52,461
as well right where we
have these parallel,

1368
01:10:52,461 --> 01:10:54,833
these layers operating in parallel.

1369
01:10:54,833 --> 01:10:59,000
And so now this ResNeXt has
some flavor of that as well.

1370
01:11:01,648 --> 01:11:05,653
So another approach
towards improving ResNets

1371
01:11:05,653 --> 01:11:09,445
was this idea called Stochastic
Depth and in this work

1372
01:11:09,445 --> 01:11:12,557
the motivation is well let's look more

1373
01:11:12,557 --> 01:11:14,688
at this depth problem.

1374
01:11:14,688 --> 01:11:18,855
Once you get deeper and
deeper the typical problems

1375
01:11:20,385 --> 01:11:22,347
that you're going to have
vanishing gradients right.

1376
01:11:22,347 --> 01:11:26,874
You're not able to, your
gradients will get smaller

1377
01:11:26,874 --> 01:11:29,077
and eventually vanish as
you're trying to back propagate

1378
01:11:29,077 --> 01:11:32,881
them over very long layers,
or a large number of layers.

1379
01:11:32,881 --> 01:11:36,003
And so what their motivation
is well let's try to have

1380
01:11:36,003 --> 01:11:40,418
short networks during training
and they use this idea

1381
01:11:40,418 --> 01:11:43,855
of dropping out a subset of
the layers during training.

1382
01:11:43,855 --> 01:11:46,474
And so for a subset of the
layers they just drop out

1383
01:11:46,474 --> 01:11:49,246
the weights and they just set
it to identity connection,

1384
01:11:49,246 --> 01:11:53,379
and now what you get is you
have these shorter networks

1385
01:11:53,379 --> 01:11:55,425
during training, you can pass back your

1386
01:11:55,425 --> 01:11:56,936
gradients better.

1387
01:11:56,936 --> 01:12:00,343
It's also a little more
efficient, and then it's

1388
01:12:00,343 --> 01:12:02,345
kind of like the drop out right.

1389
01:12:02,345 --> 01:12:04,884
It has this sort of flavor
that you've seen before.

1390
01:12:04,884 --> 01:12:07,251
And then at test time you want
to use the full deep network

1391
01:12:07,251 --> 01:12:08,918
that you've trained.

1392
01:12:11,256 --> 01:12:13,591
So these are some of the
works that looking at the

1393
01:12:13,591 --> 01:12:15,640
resident architecture, trying
to understand different

1394
01:12:15,640 --> 01:12:19,848
aspects of it and trying
to improve ResNet training.

1395
01:12:19,848 --> 01:12:23,241
And so there's also some
works now that are going

1396
01:12:23,241 --> 01:12:25,856
beyond ResNet that are
saying well what are some non

1397
01:12:25,856 --> 01:12:30,116
ResNet architectures that
maybe can also work better,

1398
01:12:30,116 --> 01:12:33,063
or comparable or better to ResNets.

1399
01:12:33,063 --> 01:12:36,579
And so one idea is
FractalNet, which came out

1400
01:12:36,579 --> 01:12:39,282
pretty recently, and the
argument in FractalNet

1401
01:12:39,282 --> 01:12:42,312
is that while residual
representations maybe

1402
01:12:42,312 --> 01:12:44,239
are not actually necessary,
so this goes back

1403
01:12:44,239 --> 01:12:46,083
to what we were talking about earlier.

1404
01:12:46,083 --> 01:12:48,586
What's the motivation of
residual networks and it seems

1405
01:12:48,586 --> 01:12:50,756
to make sense and there's, you know,

1406
01:12:50,756 --> 01:12:53,455
good reasons for why this
should help but in this paper

1407
01:12:53,455 --> 01:12:56,600
they're saying that well here
is a different architecture

1408
01:12:56,600 --> 01:12:59,217
that we're introducing, there's
no residual representations.

1409
01:12:59,217 --> 01:13:01,884
We think that the key is
more about transitioning

1410
01:13:01,884 --> 01:13:04,708
effectively from shallow to deep networks,

1411
01:13:04,708 --> 01:13:07,203
and so they have this fractal architecture

1412
01:13:07,203 --> 01:13:09,901
which has if you look on the right here,

1413
01:13:09,901 --> 01:13:14,068
these layers where they compose
it in this fractal fashion.

1414
01:13:15,579 --> 01:13:18,199
And so there's both
shallow and deep pathways

1415
01:13:18,199 --> 01:13:19,449
to your output.

1416
01:13:20,855 --> 01:13:23,269
And so they have these
different length pathways,

1417
01:13:23,269 --> 01:13:26,046
they train them with
dropping out sub paths,

1418
01:13:26,046 --> 01:13:30,378
and so again it has this
dropout kind of flavor,

1419
01:13:30,378 --> 01:13:33,151
and then at test time they'll
use the entire fractal network

1420
01:13:33,151 --> 01:13:35,846
and they show that this was able to

1421
01:13:35,846 --> 01:13:38,013
get very good performance.

1422
01:13:39,857 --> 01:13:42,953
There's another idea
called Densely Connected

1423
01:13:42,953 --> 01:13:45,696
convolutional Networks,
DenseNet, and this idea

1424
01:13:45,696 --> 01:13:48,397
is now we have these
blocks that are called

1425
01:13:48,397 --> 01:13:49,377
dense blocks.

1426
01:13:49,377 --> 01:13:51,588
And within each block
each layer is going to be

1427
01:13:51,588 --> 01:13:55,238
connected to every other layer after it,

1428
01:13:55,238 --> 01:13:56,750
in this feed forward fashion.

1429
01:13:56,750 --> 01:13:58,312
So within this block,
your input to the block

1430
01:13:58,312 --> 01:14:01,172
is also the input to
every other conv layer,

1431
01:14:01,172 --> 01:14:04,764
and as you compute each conv output,

1432
01:14:04,764 --> 01:14:06,609
those outputs are now connected to every

1433
01:14:06,609 --> 01:14:09,589
layer after and then,
these are all concatenated

1434
01:14:09,589 --> 01:14:12,500
as input to the conv
layer, and they do some

1435
01:14:12,500 --> 01:14:16,262
they have some other
processes for reducing

1436
01:14:16,262 --> 01:14:19,453
the dimensions and keeping efficient.

1437
01:14:19,453 --> 01:14:23,329
And so their main takeaway from this,

1438
01:14:23,329 --> 01:14:26,843
is that they argue that
this is alleviating

1439
01:14:26,843 --> 01:14:29,790
a vanishing gradient problem
because you have all of these

1440
01:14:29,790 --> 01:14:31,673
very dense connections.

1441
01:14:31,673 --> 01:14:35,722
It strengthens feature propagation
and then also encourages

1442
01:14:35,722 --> 01:14:38,134
future use right because
there are so many of these

1443
01:14:38,134 --> 01:14:41,807
connections each feature
map that you're learning

1444
01:14:41,807 --> 01:14:44,630
is input in multiple
later layers and being

1445
01:14:44,630 --> 01:14:46,297
used multiple times.

1446
01:14:48,716 --> 01:14:50,813
So these are just a
couple of ideas that are

1447
01:14:50,813 --> 01:14:55,025
you know alternatives or
what can we do that's not

1448
01:14:55,025 --> 01:14:57,836
ResNets and yet is still performing either

1449
01:14:57,836 --> 01:15:00,784
comparably or better to
ResNets and so this is

1450
01:15:00,784 --> 01:15:03,816
another very active area
of current research.

1451
01:15:03,816 --> 01:15:05,501
You can see that a lot of this is looking

1452
01:15:05,501 --> 01:15:09,057
at the way how different layers
are connected to each other

1453
01:15:09,057 --> 01:15:12,640
and how depth is managed
in these networks.

1454
01:15:14,338 --> 01:15:15,808
And so one last thing
that I wanted to mention

1455
01:15:15,808 --> 01:15:18,801
quickly, is just efficient networks.

1456
01:15:18,801 --> 01:15:21,831
So this idea of efficiency
and you saw that GoogleNet

1457
01:15:21,831 --> 01:15:24,216
was a work that was
looking into this direction

1458
01:15:24,216 --> 01:15:27,240
of how can we have efficient
networks which are important

1459
01:15:27,240 --> 01:15:30,260
for you know a lot of
practical usage both training

1460
01:15:30,260 --> 01:15:34,804
as well as especially
deployment and so this is

1461
01:15:34,804 --> 01:15:38,737
another recent network
that's called SqueezeNet

1462
01:15:38,737 --> 01:15:40,744
which is looking at
very efficient networks.

1463
01:15:40,744 --> 01:15:42,428
They have these things
called fire modules,

1464
01:15:42,428 --> 01:15:45,095
which consists of a
squeeze layer with a lot of

1465
01:15:45,095 --> 01:15:47,674
one by one filters and
then this feeds then into

1466
01:15:47,674 --> 01:15:50,455
an expand layer with one by
one and three by three filters,

1467
01:15:50,455 --> 01:15:53,490
and they're showing that with
this kind of architecture

1468
01:15:53,490 --> 01:15:57,820
they're able to get AlexNet
level accuracy on ImageNet,

1469
01:15:57,820 --> 01:16:00,030
but with 50 times fewer parameters,

1470
01:16:00,030 --> 01:16:03,140
and then you can further do
network compression on this

1471
01:16:03,140 --> 01:16:06,903
to get up to 500 times
smaller than AlexNet

1472
01:16:06,903 --> 01:16:10,905
and just have the whole
network just be 0.5 megs.

1473
01:16:10,905 --> 01:16:13,691
And so this is a direction
of how do we have

1474
01:16:13,691 --> 01:16:15,572
efficient networks model compression

1475
01:16:15,572 --> 01:16:17,955
that we'll cover more in a lecture later,

1476
01:16:17,955 --> 01:16:20,872
but just giving you a hint of that.

1477
01:16:22,666 --> 01:16:26,351
OK so today in summary we've
talked about different kinds

1478
01:16:26,351 --> 01:16:27,619
of CNN Architectures.

1479
01:16:27,619 --> 01:16:30,568
We looked in-depth at four
of the main architectures

1480
01:16:30,568 --> 01:16:32,365
that you'll see in wide usage.

1481
01:16:32,365 --> 01:16:36,363
AlexNet, one of the early,
very popular networks.

1482
01:16:36,363 --> 01:16:39,642
VGG and GoogleNet which
are still widely used.

1483
01:16:39,642 --> 01:16:43,695
But ResNet is kind of
taking over as the thing

1484
01:16:43,695 --> 01:16:46,716
that you should be
looking most when you can.

1485
01:16:46,716 --> 01:16:47,943
We also looked at these other networks

1486
01:16:47,943 --> 01:16:50,147
in a little bit more depth at a brief

1487
01:16:50,147 --> 01:16:51,397
level overview.

1488
01:16:52,731 --> 01:16:55,315
And so the takeaway that these
models that are available

1489
01:16:55,315 --> 01:16:57,974
they're in a lot of
[mumbles] so you can use them

1490
01:16:57,974 --> 01:16:59,038
when you need them.

1491
01:16:59,038 --> 01:17:01,085
There's a trend toward
extremely deep networks,

1492
01:17:01,085 --> 01:17:04,804
but there's also significant
research now around

1493
01:17:04,804 --> 01:17:07,637
the design of how do we connect layers,

1494
01:17:07,637 --> 01:17:10,785
skip connections, what
is connected to what,

1495
01:17:10,785 --> 01:17:14,343
and also using these to
design your architecture

1496
01:17:14,343 --> 01:17:16,229
to improve gradient flow.

1497
01:17:16,229 --> 01:17:19,013
There's an even more recent
trend towards examining

1498
01:17:19,013 --> 01:17:22,246
what's the necessity
of depth versus width,

1499
01:17:22,246 --> 01:17:23,558
residual connections.

1500
01:17:23,558 --> 01:17:25,358
Trade offs, what's
actually helping matters,

1501
01:17:25,358 --> 01:17:28,356
and so there's a lot of these recent works

1502
01:17:28,356 --> 01:17:30,153
in this direction that you can look into

1503
01:17:30,153 --> 01:17:32,190
some of the ones I pointed
out if you are interested.

1504
01:17:32,190 --> 01:17:34,407
And next time we'll talk about
Recurrent neural networks.

1505
01:17:34,407 --> 00:00:00,000
Thanks.

